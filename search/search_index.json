{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Main Page","text":""},{"location":"ML/","title":"Machine Learning","text":"<p>My online notebook for Machine Learning notes</p> <p>This book contains short notes for the key machine learning algorithms and topics. In time, I will also update it to include references to other topics that I find interesting.</p> <p>Oops! You've caught us in the middle of our creative process.</p> <p>I am working around the clock to bring you an extraordinary experience in the world of AI.</p>"},{"location":"ML/Appendix/Programming/Python/","title":"Python Programming","text":""},{"location":"ML/Appendix/Programming/Python/#libraries","title":"Libraries","text":"<p>The following important libraries have been explored in addition to the text below:</p> <ol> <li>Numpy</li> <li>MatPlotLib</li> <li>Pandas</li> <li>SciPy</li> <li>Web Libraries</li> </ol>"},{"location":"ML/Appendix/Programming/Python/#data-types","title":"Data Types","text":""},{"location":"ML/Appendix/Programming/Python/#tuples","title":"Tuples","text":"<p>Tuples are immutable ordered objects that are created using (). For defining a new tuple, one could simply write <code>x = (1, 2, 3)</code>. This would create a new tuple with the values defined. In order to extract these values into separate variables, one could write <code>a, b, c = x</code> and then the values would be mapped to the corresponding variables. Individual elements can also be accessed, just like the elements of lists are accessed, i.e. <code>print(x[2])</code> would print 3, as one would expect from a list.</p>"},{"location":"ML/Appendix/Programming/Python/#dictionaries","title":"Dictionaries","text":""},{"location":"ML/Appendix/Programming/Python/#validity","title":"Validity","text":"<p>These are data types that store data in form of key-value pairs. The only thing worth noting here is that keys need to be immutable variables, i.e. they cannot be lists of other dictionaries because they are mutable by their very definition.</p>"},{"location":"ML/Appendix/Programming/Python/#check-for-existence","title":"Check for existence","text":"<p>In order to quickly check whether or not a key exists in a particular dictionary, use the syntax <code>\"key\" in dict</code>  which would return <code>True</code> if the key is present in the dictionary object <code>dict</code>.</p>"},{"location":"ML/Appendix/Programming/Python/#deletion","title":"Deletion","text":"<p>The obvious function for most deletions, <code>del</code> is used for deleting a key-value pair in any dictionary with the syntax being <code>del(dict[\"key\"])</code>.</p>"},{"location":"ML/Appendix/Programming/Python/#strings","title":"Strings","text":""},{"location":"ML/Appendix/Programming/Python/#manipulations-in-strings","title":"Manipulations in Strings","text":"<ol> <li>Search for substring: <code>contains()</code> method of the <code>str</code> attribute of every string out there, is used for getting back a Boolean result for a string column/atom with the value True if the substring exists otherwise False.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/#manipulations","title":"Manipulations","text":""},{"location":"ML/Appendix/Programming/Python/#iteration","title":"Iteration","text":"<ul> <li>Iterable: Any object that has an associated <code>iter()</code> method is called an iterable in Python terminology. </li> <li> <p>Iterator: Any object that has an associated <code>next()</code> method that presents the next value when called. An iterator object can be created using the <code>iter()</code> method as follows</p> <pre><code>x = 'Test'\nit_obj = iter('Test')\n\nnext(it_obj)\n# Prints 'T'\n\nnext(it_obj)\n# Prints 'e'\n# ... so forth and so on\n</code></pre> <p>All objects of the iterator can be printed in a single call using * operator as <code>print(*it_obj)</code>. This would print <code>T e s t</code> and calling <code>next()</code> again on this object would throw the end of iteration error.</p> </li> </ul>"},{"location":"ML/Appendix/Programming/Python/#lists","title":"Lists","text":"<p>We can iterate over lists using the <code>enumerate</code> function like <code>for i, val in enumerate(my_list)</code> would then return the index and values in <code>i</code> and <code>val</code> respectively on every iteration of the loop. We can change the first index of the enumeration by using <code>start</code> parameter of <code>enumerate</code> function like <code>for i, val in enumerate(my_list, start = 10)</code>, which would, in this case, start the indexing at 10 instead of 0.</p>"},{"location":"ML/Appendix/Programming/Python/#dictionaries_1","title":"Dictionaries","text":""},{"location":"ML/Appendix/Programming/Python/#using-for-loop","title":"Using for loop","text":"<p>In case of dictionaries, the method <code>items()</code> must be called in order to properly iterate over items. The syntax for this, is as follows:</p> <pre><code>for key, value in my_dict.items():\n    do_something(key)\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/#enumeration","title":"Enumeration","text":"<p>A common lookup for <code>for</code> loops is the iteration dictionaries using both keys and values. The simple syntax to do this is <code>for index, value in enumerate(my_list)</code> would then product both indices and values for lists. </p>"},{"location":"ML/Appendix/Programming/Python/#zipping","title":"Zipping","text":"<p>Zipping objects using the <code>zip()</code> function creates a zip object, which can be used as follows:</p> <p><pre><code>name = [\"Apple\", \"Xiomi\", \"LG\"]\nbrand_id = [121, 289, 323]\n\nz = zip(name, brand_id)\n</code></pre> A zip object is an iterator of tuples, which can be turned into a list object, using the list function. This list object would be a list of tuples containing pairs in the format (name, brand_id).</p> <p>Objects of zip type can be unzipped into two separate variables using the zip() function with a combination of * as shown in <code>name_new, brand_id_new = zip(*z)</code>.</p>"},{"location":"ML/Appendix/Programming/Python/#comprehensions","title":"Comprehensions","text":""},{"location":"ML/Appendix/Programming/Python/#simple-list-comprehensions","title":"Simple List Comprehensions","text":"<p>A list comprehension is used to build lists without using for loops and comprises three basic elements, listed below:</p> <ol> <li>iterable</li> <li>iterator variable</li> <li>output expression</li> </ol> <p><pre><code>new_list = [k + 2 for k in old_list]\n</code></pre> The code above would iterate over all elements in <code>old_list</code> and add 2 in them to create the new list called <code>new_list</code>.</p>"},{"location":"ML/Appendix/Programming/Python/#conditional-lcs","title":"Conditional LCs","text":"<p>Conditional statements can also be added to LCs as shown in <code>new_list = [k + 2 for k in old_list if k % 2 == 0]</code>. This would return values only if the number being operated upon is even. </p> <p>There is one obvious problem in this technique of conditional LC; the size of list returned might not be the same as the size of input list.</p> <p>In order to fix this, a conditional LC with two conditions must be used. The only difference is that conditional statement will now be written before the <code>for</code> loop as <code>new_list = [k + 2 if k % 2 == 0 else 0 for k in old_list]</code>.</p>"},{"location":"ML/Appendix/Programming/Python/#dictionary-comprehensions","title":"Dictionary Comprehensions","text":"<p>It is pretty much similar to the list comprehensions mentioned above except that curly brackets are used instead of square brackets. </p>"},{"location":"ML/Appendix/Programming/Python/#generator","title":"Generator","text":"<p>A generator object can be created by using parenthesis instead of the usual square brackets as in <code>(k + 2 for k in old_list)</code>. It returns an iterable object of the type generator. </p> <p>It works on the concept of Lazy Evaluations therefore saving tons of memory required for evaluation until the value is actually required. </p> <p>Generator Functions have been defined in the \"Functions\" heading</p>"},{"location":"ML/Appendix/Programming/Python/#functions","title":"Functions","text":"<p>New functions can be defined in python on using the following structure:</p> <pre><code>def my_function(parameters):\n    \"\"\" Docstring for the function \"\"\"\n    code_to_execute\n    return result\n\nmy_function(arguments)\n</code></pre> <p>Here the name of the function is <code>my_function</code> and it takes in the parameters and returns a value. This value can then be stored for later use whenever necessary. </p> <p>The Docstring serves as a documentation and should be written enclosed in triple double-quotes helping future interpretation and modifications to our functions.</p>"},{"location":"ML/Appendix/Programming/Python/#functions-with-variables-arguments","title":"Functions with variables arguments","text":"<ol> <li> <p>Variable number of parameters</p> <pre><code>def my_function(*parameters):\n    \"\"\" Docstring for the function \"\"\"\n    code_to_execute\n    return result\n\nmy_function(argument1, argument2)\n</code></pre> <p>The function defined above would take as many inputs as provided, and then convert them into a tuple therefore allowing you to add as many arguments as necessary in one go.</p> </li> <li> <p>Variable type of parameters</p> <pre><code>def my_function(**parameters):\n    \"\"\" Docstring for the function \"\"\"\n    for k,v in parameters.items():\n        do_something(k, v)\n    return result\n\nmy_function(key1 = value1, key2 = value2)\n</code></pre> <p>The function above allows us to pass multiple arguments in the form of key-value pairs therefore allowing us to access the values with names that they came with depending on the use case. </p> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/#scope","title":"Scope","text":"<p>The scope heirarchy is as follows:</p> <ol> <li>Local Scope</li> <li>Enclosing Scope(s) (in order)</li> <li>Global Scope</li> <li>Builtin Scope</li> </ol> <p>If a variable in the global scope needs to be altered from within a function, it can be done using the keyword <code>global</code>. Therefore</p> <pre><code>x = 5\ndef my_function(parameters):\n    \"\"\" Docstring for the function \"\"\"\n    global x\n    x = 10\n\n\nmy_function(arguments)\n</code></pre> <p>the function above would alter the global value of x because we have defined that currently we want to fiddle with the <code>global</code> value of x using the <code>global</code> keyword.</p>"},{"location":"ML/Appendix/Programming/Python/#lambda-functions","title":"Lambda Functions","text":"<p>Generation of functions when required can be done in quicker way using the keyword <code>lambda</code> as <code>x = lambda x, y: x*y</code> would create a function <code>x</code> that can then be called like a normal function to find products of <code>x</code> and <code>y</code>.</p>"},{"location":"ML/Appendix/Programming/Python/#generator-functions","title":"Generator Functions","text":"<p>These can be used to create functions that are executed upon runtime but perform rather complicated tasks with ease. </p> <pre><code>def my_function(parameters):\n    \"\"\" A function that would act like a list being generated in realtime \"\"\"\n    for i in parameters: # Assuming parameters is a list\n        yield(i+1)\n\nx = my_function([1, 2, 4])\n</code></pre> <p>This would result in a dynamic generator object being created and stored in the variable x. This x can then be looped over as shown in the code below. This allows us to create a virtual dynamic function that is called and executed step by step on every iteration of the <code>for</code> loop that is called over <code>x</code>.</p> <pre><code>for item in x:\n    print(x)\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/#unexpected-behaviors","title":"Unexpected Behaviors","text":""},{"location":"ML/Appendix/Programming/Python/#exception-handling","title":"Exception Handling","text":"<p>We can catch different types of exceptions that occur during runtime for a given function using the code below. </p> <pre><code>def add_two(parameter):\n    \"\"\" Adds 2 to any number provided \"\"\"\n    try: \n        return parameter + 2\n    except:\n        print(\"parameter must be an integer or float\")\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/#custom-errors","title":"Custom Errors","text":"<p>We can manually raise different types of error for a given function using the code below. </p> <pre><code>def add_two_to_int(parameter):\n    \"\"\" Adds 2 to any number provided \"\"\"\n    if round(parameter) != parameter:\n        raise ValueError(\"argument provided must be a positive integer\") \n    try: \n        return parameter + 2\n    except:\n        print(\"parameter must be an integer or float\")\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/#data-generation","title":"Data Generation","text":"<p>There are multiple ways to generate data in Python and some of them are as follows:</p> <ol> <li>Controlled by Number of Points: <code>np.linspace(0, 10, 10)</code> takes in three arguments, the starting point, the ending point, and the number of points in between the two. </li> </ol>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/","title":"Numpy","text":"<p>Numpy is a library used specifically for advanced and faster data manipulation in Python. It allows us to effectively manage and manipulate our datasets with minimal programming. In this document, we will have a look at what are the most commonly used features of Numpy and how can we exploit them to optimize our Python programming. </p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#import-data","title":"Import Data","text":"<ol> <li>Read a csv: A csv file containing only numerical data can be imported using <code>np.loadtext('filepath.csv', delimiter = \",\")</code>. The default delimiter is whitespace so explicit definition is required.</li> <li>No Headers: If the first row contains header, the <code>skiprows = 1</code> argument must be used to skip the first row.</li> <li>Selective column import: If only a certain number of columns are required to be imported, then we can use <code>usecols = [0,1,4]</code> attribute with column indices to import only a few of all columns possible. </li> <li>Import columns with different datatypes: Although not recommended, Numpy has the ability to import dataframe like structures which contain different datatypes in different columns. This can be done using <code>np.genfromtxt()</code> function. Refer documentation for more.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#arrays","title":"Arrays","text":""},{"location":"ML/Appendix/Programming/Python/01-Numpy/#2-d-arrays","title":"2-D Arrays","text":"<p>We can create 2-D Numpy arrays as <code>a = np.array([[1,2,3], [4,5,6]])</code> and this would lead to a 2 dimensional array with 2 rows and 3 columns. </p> <p>On this object, the attribute <code>shape</code> represents the dimensions. It can be used as <code>a.shape</code> to return <code>(2,3)</code> meaning 2 rows and 3 columns exist in this 2D array. </p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#iteration","title":"Iteration","text":"<p>If iteration is required over every single element in a 2-D Array using a for loop, then the method <code>np.nditer</code> must be used in the following syntax:</p> <pre><code>for val in np.nditer(my_np_array):\n    do_something(val)\n</code></pre> <p>The iteration in the case above will happen in a row wise fashion. First observation will be iterated over (and all features of this row will be called),  and then the second row and so forth.</p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#mathematics-on-vectors","title":"Mathematics on Vectors","text":""},{"location":"ML/Appendix/Programming/Python/01-Numpy/#random-operations","title":"Random Operations","text":"<ol> <li>In order to generate a simple random number, <code>np.random.rand()</code> can be used. This would result in a random number between 0 and 1.</li> <li>We can set a seed using the function <code>np.random.seed(seedValue)</code> which would then introduce reproducability between our function calls.</li> <li>Random integers from a select range of integers can be generated using <code>np .random.randint(start_integer, end_integer)</code> which in this case would result in random integers between start_integer and end_integer - 1 (because the end of range is not included in Python). </li> <li></li> </ol>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#dot-products","title":"Dot Products","text":"<p>The dot products can be defined for two vectors or matrices in the following ways:</p> <ol> <li> <p></p> <p>This is the summation of element wise multiplication of the two vectors. The notation  denotes that the vectors are column vectors and the result of the equation above would be a 1x1 vector which is a scalar quantity. </p> <p>This definition can be emulated in Python (using Numpy) in various ways:</p> <ol> <li> <p>Without using Numpy functions)</p> <pre><code># Create the necessary variables\ndotProd = 0\na = np.array([1,2,3])\nb = np.array([2,3,4])\n\n# Use a for-loop to calculate the dot product\nfor e,f in zip(a,b):\n    dotProd += e*f\n\n\n# The value of dot now becomes 20 as one would expect\n</code></pre> </li> <li> <p>Using <code>np.sum</code> function</p> <pre><code>dotProd = np.sum(a*b)\n\n# The value of dotProd will be the same as the generic \n# code we wrote above because the a*b notation creates \n# a vector of products of individual elements and then\n# we just sum them to emulate the equation above\n</code></pre> </li> <li> <p>Using the <code>sum</code> function over the <code>np.array</code> object instances</p> <pre><code>dotProd = (a*b).sum()\n\n# Notice the use of object's sum function instead of\n# using the sum function of the class as in Method 2\n</code></pre> </li> <li> <p>Using the <code>np.dot</code> function</p> <pre><code>dotProd = np.dot(a,b)\n</code></pre> </li> <li> <p>Using the <code>dot</code> function over the <code>np.array</code> object instances</p> <pre><code>dotProd = a.dot(b)\n\n# Notice the use of object's dot function instead of\n# using the dot function of the class as in Method 4\n</code></pre> </li> </ol> <p><code>for</code> loops should be avoided whenever possible. The intrinsic functions of Numpy are magnitudes faster in operation.</p> </li> <li> <p></p> <p>This notation is not very convenient for vector multiplication unless a the angle on the right hand side is known to us. Although, it is a much more common practice to use this equation for finding out the angle between two vectors using </p> <p></p> <p>Let's use this equation to find the angle between the vectors above step by step:</p> <ol> <li> <p>Find the magnitudes of the vectors.</p> <pre><code># We can do this in two ways:\n\n# Option 1: Without using the built-in Numpy function \n# for this task\nmagA = np.sqrt(a.dot(a))\n\n# Option 2: Using the Linear Algebra module of the \n# Numpy package to do this task\nmagA = np.linalg.norm(a)\n\n# Using the equation in the starting of this chapter\n</code></pre> </li> <li> <p>Calculate the cosine of the angle between the two vectors. We know from the equation shown above that the angle between the two vectors can easily be calculated if we have the magnitudes of the two vectors and their cross product. </p> <pre><code>costheta = a.dot(b) / ( np.linalg.norm(a) * np.linalg.norm(b) )\n</code></pre> </li> <li> <p>Once we have done this, the actual angle can easily be calculated by using the <code>np.arccos</code> function of the Numpy Library</p> <pre><code>theta = np.arccos(costheta)\n</code></pre> <p>The value that we obtain for <code>theta</code> from the operation above is in radians.</p> </li> </ol> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#outer-products","title":"Outer Products","text":"<p>This function takes in two vectors <code>a</code> and <code>b</code> and then returns their outer product. <pre><code>AOuter = np.outer(a, b)\n</code></pre></p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#matrix","title":"Matrix","text":"<p>A matrix is an inherent data type in Numpy but it can also be an array of arrays if we don't want it t be a matrix. The official NP documentation discourages the use of matrices and encourages users to use array of arrays notation instead. This requires all arrays to be of the same length however, obviously. it can be defined as <code>M = np.array([[1, 2], [3, 4]])</code> which would then define an array of array kind of matrix immediately.</p> <p>To access a particular element, <code>[i, j]</code> notation may be used, just like data frames.</p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#create-a-matrix","title":"Create a matrix","text":"<p>We can create an empty matrix by using the <code>np.zeros((5, 5))</code> which would return a <code>5 by 5</code> matrix of zeros. Similarly <code>np.ones</code> can be used to create a matrix of ones. </p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#mathematics-on-matrices","title":"Mathematics on Matrices","text":""},{"location":"ML/Appendix/Programming/Python/01-Numpy/#multiplication-of-matrices","title":"Multiplication of Matrices","text":"<p>The definition of matrix is given in the prerequisites section of this book. </p> <ol> <li> <p>A simple operation like matrix multiplication can be easily done by using the <code>.dot()</code> function of Numpy stack. therefore for two matrices <code>A</code> and <code>B</code>, their multiplicative result would be given by <code>C = A.dot(B)</code>. This would result in the matrix multiplication of the two matrices A and B. </p> </li> <li> <p>In order to do an element wise multiplication in matrices we can simply say <code>A*B</code> and this would result in each element of one matrix to be multiplied by the corresponding element in the other matrix. </p> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#other-common-mathematical-operations-on-a-matrix","title":"Other common mathematical operations on a Matrix","text":"<pre><code>A = np.array([[1, 2], [3, 4]])\nAinv    = np.linalg.inv(A)  // Gives us the inverse of A\nAdet    = np.linalg.det(A)  // Gives us the determinant of A\nAdiag   = np.diag(A)            // Gives us the diagonal elements of A in a vector\nAtrace = np.trace(A)        // Gives us the sum of diagonal elements of A\n</code></pre> <p>If you pass a 2D Array to <code>np.diag</code>, then it returns the diagonal elements, if you a pass a 1D array however, it returns a 2D Array with all off diagonal elements as <code>0</code> and the elements of the array as diagonal elements.</p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#solving-a-linear-system","title":"Solving a Linear System","text":"<p>The problems in a linear system are often of the form . The solution for <code>x</code>, is easily given by . We are assuming that <code>A</code> is a square matrix and is invertible. The system has <code>D</code> equations and <code>D</code> unknowns to solve for. This can be simply done by using the equation above and the basic Numpy methods we have used thus far:</p> <pre><code>x = np.linalg.inv(A).dot(b)     // Method 1\nx = np.linalg.solve(a, b)       // Method 2 (Recommended)\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#operator-overloading-in-np","title":"Operator overloading in 'np'","text":""},{"location":"ML/Appendix/Programming/Python/01-Numpy/#mathematical-operators","title":"Mathematical Operators","text":"<p><pre><code>a = [1, 2, 3]\nprint(a+a) \n</code></pre> would return <code>[1,2,3,1,2,3]</code> but if you perform the operation with numpy as follows:</p> <p><pre><code>a = np.array([1,2,3])\nprint(a+a)\n</code></pre> would return the element wise sum of the array, i.e. <code>[2,4,6]</code></p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#boolean-operators","title":"Boolean Operators","text":"<p>In case of Boolean Operators over Numpy arrays, the preferred method of operation is using the Numpy function, <code>logical_and()</code>, <code>logical_or()</code> and <code>logical_not()</code>. These are Numpy array equivalents of <code>and</code>, <code>or</code> and <code>not</code> found in base Python.</p>"},{"location":"ML/Appendix/Programming/Python/01-Numpy/#faqs","title":"FAQs","text":""},{"location":"ML/Appendix/Programming/Python/01-Numpy/#1-what-is-the-difference-between-a-list-and-an-np-array","title":"1. What is the difference between a List and an NP Array?","text":"<p>There are several differences between an NP Array and a Python List:</p> <ol> <li>There is no append method on a NP Array while the method works well on Python Lists.</li> <li>Lists can be added with a + operator.</li> <li>If <code>L1 = [1, 2]</code> and <code>L2 = [3, 4]</code>, adding two lists would gives us the concatenation of those lists (<code>L3 = L1 + L2</code> would give us the value of <code>L3</code> as <code>[1, 2, 3, 4]</code>) but adding two Numpy Array would give us the element wise sum for the two Arrays. For example for a Numpy array <code>A = np.array([1, 2])</code>, doing <code>A + A</code> would give us the value of <code>A</code> to be <code>array([2, 4])</code>. </li> <li>Numpy lists can be multiplied and added to elements, while the same is not possible with Python Lists. Doing <code>2 * L1</code> would repeat all elements in <code>L1</code> but doing <code>2 * A</code> would multiply each element of the Numpy Array with the constant. </li> <li>Almost all mathematical operations are applied element-wise when you are working with Numpy arrays but won't do so with Lists. </li> <li>It's almost always better to use NP arrays for doing mathematical operations and creating mathematical objects. </li> </ol>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/","title":"MatPlotLib","text":"<p>This is the visualization library available for plotting graphs in python. It can be imported in any python script as <code>import matplotlib.pyplot as plt</code>. This allows us to use the shorthand notation <code>plt</code> rather than having to use the complete name for the plot function.</p>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#plots","title":"Plots","text":""},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#line-based-plot","title":"Line based plot","text":"<pre><code># For graphs with lines\nplt.plot(x, y)\nplt.show()\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#scatter-plots","title":"Scatter Plots","text":"<pre><code># For scatter plots\nplt.scatter(x, y)\nplt.show()\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#histograms","title":"Histograms","text":"<pre><code>plt.hist(x)\nplt.show()\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#box-plots","title":"Box Plots","text":"<p>In the example mentioned below, we pass the columns to be plotted, into the <code>column</code> argument and the column that we want to compare boxplots across, into the <code>by</code> argument.</p> <pre><code>df.boxplot(column = 'column_name', by = \"continent\")\n</code></pre> <p>The lines that extend from a boxplot are called whiskers. They represent the maximums and minimums of our data, excluding the outliers. </p>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#customizations","title":"Customizations","text":"<ol> <li>Bins: The number of default <code>bins</code> for a histogram is 10, and it can be altered by passing a different value <code>plt.hist(x, bins =3)</code></li> <li>Range: Setting the minimum and maximum value for a histogram is done by using the <code>range</code> argument and passing it a tuple <code>plt.hist(x, range = (0, 10))</code> </li> <li>Normalization: The data can be normalized before the histogram is plotted using <code>normed</code> argument as <code>plt.hist(x, normed = True)</code>.</li> <li>CDF: A Cumulative Distribution Function can be calculated before plotting by using the Boolean argument <code>cumulative</code> in addition to <code>normed</code> while plotting <code>plt.hist(x, cumulative = True, normed = True)</code>.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#plotting-a-dataframe","title":"Plotting a DataFrame","text":"<p>A pandas DataFrame can be plotted using the <code>plt.plot(pd_df)</code> function. This call would plot all the numeric values in the dataframe across the index. </p>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#customizations_1","title":"Customizations","text":"<ol> <li>Labels:The customizations for a plot <code>plt.plot(x,y)</code> for X and Y labels can be done with <code>plt.xlabel('X')</code> and <code>plt.ylabel('Y')</code>. </li> <li>Title: A title can be added with <code>plt.title('Plot Title')</code>.</li> <li>Altering Axis: The axis' can be changed by passing an one dimensional array to the <code>yticks</code> function as in <code>plt.yticks([0,2,4,6])</code>. This would force the Y axis to have these numbers of the intervals. Optionally, a second list of the same length can also be passed to <code>yticks</code> for custom labels, while still using the first row as the original axis numbers. It can be done as  <code>plt.yticks([0,2,4,6],[\"Zero\", \"Two\", \"Four\", \"Six\"])</code>.</li> <li>Adding Text: Text can be added to any point in the plot using <code>plt.text(x, y, \"Text\")</code> syntax.</li> <li>Logarithms: If the values are interfering with each other due to dominant behavior of a particular feature, then we can use <code>plt.yscale('log')</code> to neutralize the effect of that feature before showing the plot. </li> <li>Colors: Colors can be added for features while plotting them using <code>pd_df[\"column_1\"].plot(color = \"r\")</code>.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#statistical-significance","title":"Statistical Significance","text":""},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#types-of-plots","title":"Types of Plots","text":"<ol> <li>Bar Plots: It's a good idea to use bar plots for discrete data counts</li> <li>Histograms: Histograms are good for frequency analysis on continuous data columns</li> <li>Box Plots: It's a good idea to use box plots to visualize all basic summary statistics for a given column. </li> <li>Scatter Plots: They're used to observe visually, the relationships between two or more numeric columns.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/02-MatPlotLib/#exporting-plots","title":"Exporting plots","text":"<p>We can easily export plots using <code>plt.savefig('filename.jpg')</code> before calling <code>show()</code> and save the plots.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/","title":"Pandas","text":"<p>The high level data manipulation tool used by data scientists. It can be imported with the syntax <code>import pandas as pd</code>.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#creating-a-pandas-dataframe","title":"Creating a Pandas Dataframe","text":""},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-dictionaries","title":"From Dictionaries","text":"<p>There are multiple ways to create a Pandas dataframe, the commonly used one is to create it from a dictionary by setting a key for every column label and a the value to be a list of observations of that label for each column. Then simply calling <code>pd.DataFrame(dict)</code> would create the data frame. </p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#assigning-row-labels","title":"Assigning row labels","text":"<p>This can be done by using the syntax <code>df.index = ['label1', 'label2', ... 'labeln']</code> for n observations that exist in the dataframe.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-lists","title":"From Lists","text":"<p>If we are to create a DataFrame from two conforming lists that are defined as follows, we would need to use the <code>list</code> and <code>zip</code> functions.</p> <pre><code>labels = ['n1', 'n2']\nx = [1, 2, 3]\ny = [4, 5, 6]\n</code></pre> <ol> <li>We would first need to create a list of lists like <code>list_columns = [x, y]</code></li> <li>Then we would need create a list after element-wise zipping of the columns with their labels, as shown in <code>z = list(zip(labels, list_columns))</code></li> <li>This will now need to be converted into a dictionary which would have column names (<code>labels</code>) and columns (<code>z</code>) using <code>data = dict(z)</code></li> <li>After this, we use the method defined above for dictionaries, i.e. <code>pd.DataFrame(data)</code> to create a DataFrame.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#broadcasting","title":"Broadcasting","text":"<p>It is the concept of recycling from R, that is called broadcasting in Python. The idea is, that a particular value can be recycled and used to fill all the other observations, if unsuitable number of details have been provided.</p> <p><pre><code>x = [1, 2, 3]\ny = {'n': x, 'is_int': 'Yes'}\nz = pd.DataFrame(y)\n</code></pre> would return a DataFrame with two columns, the column containing <code>Yes</code> thrice. </p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#reading-and-importing-data","title":"Reading and Importing Data","text":""},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-csvs","title":"From CSVs","text":"<p>It is relatively straightforward to be reading data from CSVs. One can use <code>pd.read_csv('path_to_csv.csv')</code> in order to read from a file. </p> <ol> <li>No column labels: If the data does not have column labels, <code>pd.read_csv('path_to_csv.csv', header = None)</code> will allow it to read data without it.</li> <li>External column names: External column names can be added to the data frame using the names argument, <code>pd.read_csv('path_to_csv.csv', header = None, names = list_of_names)</code></li> <li> <p>Null value declaration: If our data uses any other convention than <code>NaN</code> for declaring null values in it, we can explicitly define it, by setting the <code>na_values</code> attritbute to that character <code>pd.read_csv('path_to_csv.csv', na_values =  ['-1'])</code></p> <p>This can also be done if there are more than one kinds of <code>NaN</code> values present in the dataset using a list of values as shown in <code>pd.read_csv('path.csv', na_values =  ['-1', '999'])</code> or using a dictionary as shown <code>pd.read_csv('path.csv', na_values =  {col1: '-1', col2: '999'})</code> if there are separate <code>NaN</code> characters in separate columns. 4. Assigning row labels: In case the first column of the csv contains row labels for the data, then use <code>pd.read_csv('path_to_csv.csv', index_col=0)</code> for using the (row) labels for your dataframe. 5. Date values: If year, month and date are in separate columns and need to be converged into one column, it can be done using <code>parse_dates</code> argument of the <code>read_csv()</code> function, e.g. <code>pd.read_csv('path_to_csv.csv', parse_dates = [[1, 2, 3]])</code> where columns with indices would contain year, month and day data. </p> <p>Alternatively, we can parse the date-time values using <code>parse_dates = True</code> which would then convert all the dates that are ISO 8601 compatible (yyyy-mm-dd hh:mm:ss), into appropriate date structure. </p> </li> <li> <p>Handling comments: If the file contains comments within the data, they can be distinguished using the delimiter passed to the <code>comment</code> argument as shown in <code>pd.read_csv('path_to_csv.csv', comment='#')</code></p> </li> <li>Delimiter: The delimiter in while reading a csv to a Pandas DataFrame object can be set using <code>sep</code> argument</li> <li>Skipping rows: Rows can be skipped while reading a csv file by using <code>skiprows</code> argument in combination with <code>header</code> argument.</li> <li>Skipping footer: Rows at the end of the file can be skipped using the <code>skipfooter = n</code> argument. This would skip the last <code>n</code> rows of the file.      &gt; NOTE: The <code>skipfooter</code> argument doesn't work with the default C Engine so we need to specify the <code>engine = python</code> when setting this parameter.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#chunkwise-loading","title":"Chunkwise loading","text":"<p>In case of large datasets, data can be loaded and processed in chunks. It can be done with the help of <code>for</code> loop as in <code>for chunk in pd.read_csv('path_to_csv.csv', chunksize = 1000)</code>.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#globbing","title":"Globbing","text":"<p>The process of looking for file names with specific patterns, and loading them is called globbing. </p> <pre><code>import glob\n\npattern = '*.csv'\ncsv_files = glob.glob(pattern)\n</code></pre> <p>The code above, would return a list of files names, called <code>csv_files</code>. Then we can loop over this list to load all data frames. Concatenation can be used for merging all the datasets into one single dataset if required. </p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-urls","title":"From URLs","text":"<p>Importing a csv from a web URL can be done with the UrlLib package as follows</p> <pre><code>from urllib.request import urlretrieve\n\nurlretrieve('http://onlinefilepath', 'local_file_path.csv')\n</code></pre> <p>And then proceed with <code>read_csv()</code> function as usual.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-excel","title":"From Excel","text":"<ol> <li>Reading a file: A simple read operation over an Excel Spreadsheet can be executed by using <code>x = pd.ExcelFile('filepath.xlsx')</code>. </li> <li>Listing sheets: There can be multiple sheets involved in any particular file and they can be listed by using the <code>sheet_names</code> attribute as <code>print(x.sheet_names)</code></li> <li>Reading a particular sheet: It is done by passing the sheet name to the <code>parse()</code> method as shown in <code>df_sheet = x.parse('sheet1')</code></li> <li>Custom Headers: One can define custom headers while parsing from an excel sheet by using the <code>names()</code> argument.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-hdf5-hierarchical-data-format-version5","title":"From HDF5 (Hierarchical Data Format version5)","text":"<p>This is data format commonly used for storing large quantities of numerical data in Python. This is done using the following code segment</p> <pre><code>import h5py\ndata = h5py.File(\"path_to_file.hdf5\", \"r\")\n</code></pre> <p>You can explore the <code>data</code> object so obtained by using code similar to that required to explore a dictionary. </p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-pickles","title":"From Pickles","text":"<pre><code>with open('file_path.pkl', 'rb') as file:\n    data = pickle.load(file)\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#from-sql","title":"From SQL","text":"<ol> <li> <p>Creating a Database engine</p> <p><pre><code>from sqlalchemy import create_engine\nengine = create_engine(\"path to sql db connector\") \n</code></pre> 2. List Tables: This can be done using <code>engine.table_names()</code> method of engine object. 3. Connecting to the engine: This is done using the <code>connect()</code> method available with every sqlalchemy <code>engine</code> object. 4. Querying: There are two ways to query an SQL database and they are as follows:  1. The first one takes all of the above methods and works as follows:</p> <pre><code>```python\ncon = engine.connect()\nresults = con.execute(\"SELECT * FROM table_name\")\ndf = pd.DataFrame(results.fetchall())\ndf.columns = results.keys()\ncon.close()\n```\n\nThis syntax is very similar to the **PHP** syntax for this operation. We create a connection, then execute the query and get returned a results binary object. We then use `fetchall()` method to convert it to a flat structure and store it in a pandas dataframe. Finally, we add the column names to the dataframe that we created and close the connection.\n</code></pre> <ol> <li> <p>The second method is much more concise and works just fine. It harnesses the power of Pandas library and works as follows:</p> <pre><code>df = pd.read_sql_query(\"SELECT * FROM table_name\", engine)\n</code></pre> <p>This single line of code then executes the command and returns the results in form of a DataFrame.</p> </li> </ol> </li> <li> <p>Fetch fewer rows: Sometimes the SQL query that we execute might return humongous results, then we can use the <code>fetchmany()</code> function with the <code>size</code> argument over the <code>results</code> object in order to fetch a certain number of rows instead of all.</p> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#exporting-data","title":"Exporting Data","text":"<ol> <li>csv: The method <code>to_csv()</code> for every DataFrame object allows us to export it to any file that we desire. It works as <code>pd_df.to_csv('filename.csv')</code>.</li> <li>Excel: The method <code>to_excel()</code> for every DataFrame object allows us to export it to an excel spreadsheet file that we desire. It works as <code>pd_df.to_csv('filename.xlsx')</code>.</li> <li>Numpy array: Any Pandas DataFrame can be converted into a Numpy array object using <code>values</code> attribute of every pandas dataframe object. </li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#selecting-and-index-data","title":"Selecting and Index Data","text":""},{"location":"ML/Appendix/Programming/Python/03-Pandas/#column-selection","title":"Column Selection","text":"<p>A column in a dataframe <code>df</code> may easily be selected using the syntax <code>df[\"columnName\"]</code>. </p> <p>NOTE: The returned object from the code above is NOT a dataframe but an object of the type series. This may lead to unexpected results and therefore this method is not recommended.</p> <p>There are two fixes for the problem mentioned above:</p> <ol> <li>The fix for the problem mentioned above is to use double square brackets like <code>df[[\"columnName\"]]</code> for selecting the column. This would return a dataframe instead of a series as was the case in the method above. </li> <li>Use the <code>values</code> attribute for any Series object in order to retrieve the numerical values involved in form of a Numpy array.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#slicing-for-row-selection","title":"Slicing for Row Selection","text":"<p>It is uncommon to use regular square brackets for row selection but it can be done using <code>df[1:5]</code> which would return the rows with indices 1 through 4 (because as always, the last index would not be included).</p> <p>If alternative slicing methods are required then it can be achieved as <code>pd_df[::3,:]</code> would select every third row of the DataFrame.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#loc-and-iloc","title":"Loc and iLoc","text":"<p>These are the two most commonly used methods (of Pandas Data Frame objects) for selecting or subsetting data. The loc technique operates on labels and the iloc technique relies on integer positions.</p> <ol> <li> <p>loc: This method allows us to select certain rows based on labels as follows <code>df.loc[[\"row_label1\", \"row_label2\"]]</code> would select the rows with these two labels. </p> <p>One trick for range of slicing is to use <code>df.loc[[\"row_label2\", \"row_label1\":-1]]</code> for reverse slicing. It would select rows from <code>row_label1</code> to <code>row_label2</code> but in reverse order.</p> <p>Note: The use of <code>[[ ]]</code> is still necessary for making sure that the returned object is indeed a Pandas DataFrame in order to avoid any inconsistencies.</p> <p>WARNING: Unlike conventional slicing (with numbers) slicing with <code>loc</code> using <code>'column_name_1':'column_name_2'</code> would include <code>column_name_2</code> in the resulting object. This is different from the index based slicing as that ignores the last index.</p> <p>It can be further extended to include only specific columns using a comma, as in <code>df.loc[[\"row_label1\", \"row_label2\"], [\"column_label1\", \"column_label2\"]]</code>. This query would only return the columns with labels column_label1 and column_label2.</p> </li> <li> <p>iloc: Everything remains the same except that indices are used instead of labels.</p> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#filtering","title":"Filtering","text":"<ol> <li>any and all: <code>any()</code> or <code>all()</code> methods are helpful in filtering the columns that have certain properties. They're usually used in combination with <code>isnull()</code> or <code>notnull()</code> methods.</li> <li>Drop na: The <code>dropna()</code> method can be used on data frames to filter out rows with any or all na values based on the argument <code>how='any'</code> or <code>how='all'</code>.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#iterations","title":"Iterations","text":""},{"location":"ML/Appendix/Programming/Python/03-Pandas/#columns","title":"Columns","text":"<p>A basic <code>for</code> loop would result in iteration over column names. For instance, </p> <pre><code>for i in pd_df:\n    print(i)\n</code></pre> <p>would simply print the columns names that exist in the pandas dataframe <code>pd_df</code>.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#rows","title":"Rows","text":"<p>The rows need to be iterated over using the method <code>iterrows</code> of the pandas dataframe object that we are trying to access.</p> <pre><code>for lab, row in pd_df.iterrows():\n    print(lab)\n    print(row)\n</code></pre> <p>would then print, first the label, and then the contents of each row as a Series object.</p>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#manipulating-dataframes","title":"Manipulating Dataframes","text":"<p>NOT COMPLETE: would need to visit more websites and research materials to complete manipulatio</p> <ol> <li> <p>Adding a new column</p> <ol> <li> <p>Single Value(loc): The operator <code>loc</code> can be used to add a new column to an existing dataframe. </p> <p><pre><code>pd_df.loc[\"new_column\"] = 2\n</code></pre> should create a new column names <code>new_column</code> in the <code>pd_df</code> dataframe with the value <code>2</code> on all rows.     2. Mutation(apply): <code>pd_df[\"new_column\"] = pd_df[\"old_column\"].apply(len)</code> would add a new column with the length of values present in currently existing <code>old_column</code>.</p> </li> </ol> </li> <li> <p>Tidying Data</p> <ol> <li> <p>Melting Dataframes: If columns contain values instead of variables, then we would need to use the <code>melt()</code> function. It can be used as </p> <pre><code>pd.melt(    frame = df, \n            id_vars = 'identifier_column', \n            value_vars = ['column1', 'column2'],\n            var_name = 'new_key_column_name',\n            value_name = 'new_value_column_name')\n</code></pre> <p>where <code>id_vars</code> is the column/set of columns that contain the ID, and <code>value_vars</code> are the columns that need to be merged.</p> </li> <li> <p>Pivoting Data: It's the opposite process of melting data. It can be used to change a column into multiple columns as follows:</p> <pre><code>pd.pivot(   frame = df, \n            index = 'index_column', \n            columns = 'key_column',\n            values = 'value_column')    \n</code></pre> <p>This would work just fine if we're dealing with perfect data, i.e. there are no duplicates. If there are duplicates though, then we would need to use the <code>pivot_table()</code> method in order to deal with them. It is done with one additional parameter, and with , as shown below</p> <pre><code>df.pivot_table( index = 'index_column', \n                    columns = 'key_column',\n                    values = 'value_column',\n                    aggfunction = np.mean)  \n</code></pre> <p>where we are telling Pandas to mean the duplicate numeric values using the <code>aggfunction</code> attribute. </p> <p><code>reset_index()</code> method is used on the data frames that have been pivoted in order to flatten them2</p> </li> <li> <p>Concatenating Data: A list of dataframes can be passed to the <code>pd.concat()</code> function for concatenating data.</p> <p>The <code>axis = 1</code> argument can be used for column wise concatenation.</p> <p>Note: We need to reset the index labels by passing the <code>ignore_index=True</code> argument to the <code>pd.concat()</code> function so that there are no duplicate indices in order to avoid using <code>reset_index()</code> method later.</p> </li> </ol> </li> <li> <p>Merging Data</p> <p>We can use the Pandas equivalent of join to merge two dataframes as follows</p> <pre><code>pd.merge(       left = left_df, right = right_df,\n                on = None, \n                left_on = 'left_df_column_name', \n                right_on = 'right_df_column_name')\n</code></pre> <p>If the column name is same on both left and right dataframes, then only the <code>on</code> parameter can be specified in the function above and the other factors will be redundant. </p> <p>There are multiple kinds of merges that can happen in Pandas:</p> <ol> <li>One to One: Both the keys take a value only 1 time on both sides</li> <li>Many to One/ One to Many: This merge happens when there are more than one duplicates on either of the tables. In this case, the value from the other key will be duplicated to fill in the missing repition.</li> </ol> </li> <li> <p>Data Type Cleaning</p> <p>We can observe the datatypes of various columns by viewing the <code>dtypes</code> attribute of the dataframe that we want to check these details for.</p> <ol> <li>Converting Data Types: The data types can be converted using the <code>astype()</code> method of any column. </li> <li>Convert to categorical column: We would often want to convert the column type to a categorical variable, we can pass the <code>'category'</code> argument to the <code>astype()</code> method of any column to convert it into a categorical column.</li> <li>Convert to Numeric: If there is a column that should be of numeric type but is not, because of mistreated data, or erroneous characters in the data, we can use the <code>pd.to_numeric(df['column_name'])</code> function and pass it the additional argument <code>errors = 'coerce'</code> in order to convert all that erroneous data to <code>NaN</code> with ease.</li> <li>Drop NA: If there are really few data points that have missing values in them, we can lose them with the <code>dropna()</code> method. </li> <li>Recode Missing Values: We can customize the missing values using the <code>fillna('missing_value_placeholder')</code> method of every data frame object and the columns. </li> <li>String Cleaning: The <code>re</code> library for regular expressions gives us a neat way to do string manipulations. We can formulate regular expression formalue like <code>x = re.compile('\\d{3}-\\d{3}-\\d{4}')</code>. This would create a new regex object called <code>x</code> which has a method <code>match()</code>. We can pass any string to this <code>match()</code> method to match it with our regular expression and it returns a boolean <code>True</code> if the string matches. </li> <li>Duplicate Data: There may be mulitple rows where redundant partial or complete row information is stored and these may be sorted out by using the <code>drop_duplicates()</code> methods of the data frame object.</li> </ol> </li> <li> <p>Vectorized Operations: Whenever possible, it is recommended to use vectorized computations rather than going for custom solutions. </p> <ol> <li>Operating on Strings: There are vectorized methods in the <code>str</code> attribute of every dataframe column that contains strings. These functions enable us to do quick vectorized transformations on the df. </li> <li>Map Method: There are often times when the <code>str</code> or other vectorized attributes will not be present. In such cases the <code>map()</code> method can be used for mapping operation succinctly. </li> </ol> </li> <li> <p>Assigning Index: We can designate a column, or any other Numpy array of the same length to be the index by assigning it to the <code>df.index</code> attribute. </p> <ol> <li>Index Name: Index by default, won't have name associated with it, but one can assign a name to the index by assigning it to the attribute <code>df.index.name</code>. The similar operation can be carried for assigning an index name to the column names using the <code>df.columns.name</code> attribute.</li> <li>Using Tuples as Index: Often we would need to set two or more columns as index (much like composite keys in SQL). This can be done using Tuples. They list of columns that we need to be set as the composite index of a dataframe can be passed to the <code>set_index([\"composite_key_column_1\", \"composite_key_column_2\"])</code> to achieve this. It is called the MultiIndex.</li> <li> <p>Sorting Index: If we are using a Multiindex as shown above, we can also use the <code>sort_index()</code> method to sort the index and display it in a more organized manner. </p> <p>This allows for fancy indexing, i.e. calling <code>df.loc[([\"index_1_low\" : \"index_1_high\"], \"index_2\"), :]</code> would select all the columns for rows that belong in the range provided for <code>index_1</code> and all sub rows belonging to <code>index_2</code>. </p> <p>The <code>slice()</code> function must be used for slicing both indexes.     4.  Stacking and Unstacking Index: We might want to remove some of the indexes from the multi level indexes to be columns. To do this, we use the method <code>unstack()</code> with the <code>level=\"index_name_to_remove\"</code>. This will give us a hierarchical data frame and this effect can be reversed using the <code>stack()</code> method in the same format.     5. Swapping Index Levels: The index levels can be swapped using the method <code>swaplevel(0, 1)</code> on any dataframe. This would essentially exchange the hierarchical arrangement of indices and running <code>sort_index()</code> right after it would do the rest.</p> </li> </ol> </li> <li> <p>Aggregation/Reduction: The <code>groupby()</code> method is the Python equivalent of R's <code>aggregate()</code> method. It allows us to create virtual groups within the dataframe. It is usually chained together with other aggregation functions like <code>sum()</code>, <code>count()</code>, <code>unique()</code> etc. to produce meaningful results. We can use a typical grouping operation as follows:</p> <pre><code>titanic.groupby(['pclass', 'sex'])['survived'].count()\n</code></pre> <p>There is also the option of finding out multiple aggregation details on the grouped dataframe: 1. Multiple Aggregations: We can use <code>titanic.groupby('pclass').agg(['mean', 'sum'])</code> to compute multiple aggregation values at once. 2. Custom Aggregations: We can pass custom functions as arguments to <code>agg()</code> method that would take <code>Series</code>  objects as inputs and produce results from them. When used, they would receive as inputs multiple <code>Series</code> objects (one for each group) and would produce grouped results like other functions. 3. Differnet Agg on Different Columns: We can pass a <code>dictionary</code> object to <code>agg()</code> method, as an argument, which would contain column names as keys and corresponding aggregation functions to apply as values. This allows us to compute different statistics for the same grouping of objects, upon different columns.</p> </li> <li> <p>Transformation: Transformation functions are used to transform one or more columns after they have been grouped and is usually chained after the <code>groupby()</code> method as <code>transform(transformation_function)</code>. This transformation method passes the Series to <code>transform_function()</code> which could be a user defined function or a builtin one, which then returns a transformed series of a conforming size. </p> </li> <li>Grouping and Filtering: We can use the dictionary object created by <code>groupby()</code> method to loop over and therefore filter only the rows of interest.</li> <li>Sorting: We can sort the values in any column by using the <code>sort_values(ascending = False)</code> method available for columns of all dataframe objects.</li> <li>Matrix Operations: Direct matrix operations will not work on Dataframes but <code>pandas.core.frame.DataFrame</code> object comes with an <code>as_matrix()</code> method available to each object for converting it readily into a Numpy 2D Array. This will only work for DataFrames with only numerical values though. There is a crucial difference between Numpy 2D Arrays and Pandas' DataFrames and that is, <code>X[0]</code> for a Numpy 2D Array returns the 0th row while accessing a DataFrame X[0] would return the 0th column of the DataFrame.</li> <li>Mathematical Operations: There are various mathematical operations available for our use.<ol> <li>pct_change(): This method can used to detect percentage change over a particular column or aggregation values.</li> <li>add(): This method can be used to add two Series with corresponding row indices as <code>a.add(b)</code>. This would add the series <code>a</code> and <code>b</code>. However, if there are non matching indices, i.e. an index in <code>a</code> does not have any corresponding index in <code>b</code>, then this could return an <code>NaN</code> value. We can change this by changing the default non existent value by passing the argument <code>fill_value</code> into the <code>add()</code> method. This method is chainable so more than one Series can be added in a single line. </li> </ol> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#exploring-data","title":"Exploring Data","text":"<ol> <li>Dimensions: The <code>shape</code> attribute of any DataFrame can be used to check the dimensions.</li> <li>Column Names: The <code>columns</code> attribute of a DataFrame returns the names of all the columns.</li> <li>Indices: The atrributes <code>columns</code> and <code>index</code> can be used to retrieve the columns' and rows' index of a DataFrame.</li> <li>Column Details: Much like the <code>str</code> function in R, <code>info()</code> method can be used over any pandas DataFrame object in order to retrieve meaningful insight on columns. It returns the name of the column and the number of Non Null values preent in the data column.</li> <li>Statistical Summary: Statistical summaries for pandas DataFrames can be quickly generated using the <code>describe()</code> method on any pandas DataFrame.</li> <li>Interquantile range (IQR): Quantile ranges are useful when exploring a dataset and it can easily be determined by using <code>quantile()</code> method of Pandas dataframes. For instance, <code>pd_df.quantile([0.25, 0.75])</code> would return two values for each column in the dataset and half the data for those columns would lie between those two values. </li> <li>Range: The range can be calculated using the <code>min()</code> and <code>max()</code> methods on any DataFrame.</li> <li>Median: The <code>median()</code> method can be used for finding out the median of any given dataset. </li> <li>Standard deviation: The method <code>std()</code> can be used for finding out the standard deviation for any given column.</li> <li>Unique objects: Unique categories in any categorical column can be found using the <code>unique()</code> method.</li> <li>Frequency Count: The frequency  of factors in a column containing factors by using the <code>value_counts()</code> method on that column. Optionally, we could specify the <code>dropna</code> argument to this method with a Boolean Value specifying whether or not to involve null values. </li> <li>Data Type: We can explore the data type for any column that we want to, by having a look at the values of the attribute <code>dtypes</code> for each column in data frame.</li> <li>Index of Max: The <code>idxmax()</code> and <code>idxmin()</code> methods allow us to find the row or column labels where the maximum or minimum values are located with the help of <code>axis = 'columns'</code> for the column labels, and these methods default to <code>min()</code> so we won't have to specify anything there. </li> <li>Indexes of Non NULL Values: One can get the indices of non null values by using the <code>notnull()</code> method available for Series objects in Pandas.</li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#time-series-with-pandas","title":"Time Series with Pandas","text":"<ol> <li> <p>Slicing on the basis of time: Interesting selections can be done on date time indices, using selection operators like <code>pd_df.loc['2017-01-22':'2018-01-02']</code> to select the values for that 1 month.</p> </li> <li> <p>Reindexing: Any time series can be reindexed using the index of another time series by using the syntax <code>time_s2.reindex(time_s1.index)</code></p> </li> <li>Forward Filling of Null Values: Null values in a time series can be forward filled using <code>time_series.reindex(new_index, method = \"ffill\")</code></li> <li> <p>Resampling: There are two kinds of sampling that can be done with time series in Pandas dataframes:</p> <ol> <li>Downlsampling: Reduce datetimes to lower frequency, i.e. from daily to weekly etc. </li> <li>Upsampling: Increasing the times to higher frequency, i.e. from daily to hourly etc.</li> </ol> <p>There are multiple parameters that we can pass to <code>resample()</code> method for allowing us to derive quick statistics from a time series. For instance, <code>pd_ts.resample('D').mean()</code> would allow us to have daily averages of all numeric columns of the time series dataframe.</p> Code Meaning <code>min</code>, <code>T</code> Minute <code>H</code> Hour <code>D</code> Day <code>B</code> Business Day <code>W</code> Week <code>M</code> Month <code>Q</code> Quarter <code>Y</code> Year <p>Numeric values can be used as prefixes to the parameters above in order to increase or decrease the sampling size. For instance, <code>2W</code> can be used for downsampling for 2 weeks in <code>pd_ts.resample('2W').mean()</code></p> <p>The <code>ffill()</code> and <code>bfill()</code> methods can be used for filling in, rolling values as in <code>pd_ts.resample('2H').ffill()</code>.</p> <p>Interpolation: It can be carried out as follows:</p> <pre><code>pd_ts.resample('A').first().interpolate('linear')\n</code></pre> <p>This would resample and fill in the gaps between any two data points with NaN values. Then the <code>interpolate()</code> method would interpolate the values. </p> </li> <li> <p>Datetime Methods: These methods allow us to slice and select specific date time attributes from the data. </p> <ol> <li>Select hours: For instance <code>pd_ts['date'].dt.hour</code> would extract and return the hour where 0 is 12AM and 23 is 12PM.</li> <li>Timezone: We can define the timezone for a particular column by using <code>pd_ts['date'].dt.tz_localize('US/Central')</code>. On the other hand, easy conversions can be made, using the method <code>tz_convert()</code> that is specifically used for converting dates and times in one timezone to another.</li> <li></li> </ol> </li> </ol>"},{"location":"ML/Appendix/Programming/Python/03-Pandas/#moving-averages","title":"Moving Averages","text":"<p>We can calculate moving averages, that allows us to focus on long term trends instead of being stuck in short term fluctuations. We do so by using the <code>rolling()</code> method as shown in <code>time_series.rolling(window=24).mean()</code> which would compute new values for each point but it will still be hourly data. This would instead set the value of that point as an average of trailing 24 datapoints, hence making it smoother. </p> <pre><code>\"\"\"\nfor key in keys['order']:\n  key_weight = keys['weights'][key] # Weight of the key\n  key_data = keys['data'][key]\n  if key_data['type'] == \"singular\":\n    print(\"Jacuzzi\")\n  else:\n    for section in key_data['order']: \n      section_weight = key_data['weights'][section]\n      section_data = key_data['data'][section]\n\n      for unit in section_data:\n        user_df[key+'_'+section+'_'+unit] = user_data_raw[section_data[unit]['user']].notnull().astype(int)\n\n\nprint(user_df.columns)\n\"\"\"\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/04-SciPy/","title":"SciPy","text":""},{"location":"ML/Appendix/Programming/Python/04-SciPy/#read-and-import-data","title":"Read and Import Data","text":""},{"location":"ML/Appendix/Programming/Python/04-SciPy/#matlab-files","title":"Matlab Files","text":"<p>We can read files from matlab using the <code>scipy.io.loadmat()</code> function as shown below</p> <pre><code>import scipy.io\n\nmatlab_data = scipy.io.loadmat('filepath.mat')\n</code></pre> <p>This would create a dictionary object called <code>matlab_data</code> that would contain key value pairs for all the different kinds of objects that have been imported from the matlab file. </p> <p>It is interesting to note here that Matlab files are usually permanently stored work environments of Matlab so multiple objects should be expected while importing a single matlab file. </p>"},{"location":"ML/Appendix/Programming/Python/05-urllib/","title":"Web Libraries","text":""},{"location":"ML/Appendix/Programming/Python/05-urllib/#importing-data","title":"Importing Data","text":""},{"location":"ML/Appendix/Programming/Python/05-urllib/#reading-a-csv-file","title":"Reading a CSV File","text":"<p>Importing a csv from a web URL can be done with the UrlLib package as follows</p> <pre><code>from urllib.request import urlretrieve\n\nurlretrieve('http://onlinefilepath', 'local_file_path.csv')\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/05-urllib/#reading-a-json-file","title":"Reading a JSON file","text":"<p><pre><code>import json\n\nwith open(\"file_path.json\") as json_file:\n    json_data = json.load(json_file)\n</code></pre> This would create a dictionary obejct <code>json_data</code> allowing us to loop over or manipulate the data retrieved. </p>"},{"location":"ML/Appendix/Programming/Python/05-urllib/#retrieving-from-apis","title":"Retrieving from APIs","text":"<pre><code>import requests\nr = requests.get('http://url')\njson_data = r.json()\n</code></pre>"},{"location":"ML/Appendix/Programming/Python/05-urllib/#scraping-the-web","title":"Scraping the web","text":""},{"location":"ML/Appendix/Programming/Python/05-urllib/#using-the-requests-package","title":"Using the 'requests' package","text":"<pre><code>import requests\nraw = requests.get('http://url')\ntext = raw.text\n</code></pre> <p>We can scrap any webpage with one line of code using the requests package and then simply addressing the <code>text</code> attribute of any raw result produced by the <code>requests.get()</code> function would give us the actual HTML result of the page. </p>"},{"location":"ML/Appendix/Programming/Python/05-urllib/#beautiful-soup","title":"Beautiful Soup","text":"<p>We will use the <code>text</code> object created in the previous code segment and then beautify it with the library to improve navigability. </p> <pre><code>from bs4 import BeautifulSoup\nsoup = BeautifulSoup(text)\n</code></pre> <p>Now that we have extracted the html data into a <code>soup</code> object, we can use atrributes like <code>title</code> and methods like <code>get_text()</code>, <code>find_all()</code> etc. on it to extract more meaningful information.</p>"},{"location":"ML/Appendix/Programming/Python/05-urllib/#retrieving-from-apis_1","title":"Retrieving from APIs","text":"<pre><code>import requests\nr = requests.get('http://url')\njson_data = r.json()\n</code></pre>"},{"location":"ML/Classification/","title":"Classification","text":"<p>Machine Learning classification models are a category of methods used to predict the category of a data point. These models are mainly used in the field of medical imaging, speech recognition, and many others. This page entails explanations with Python examples of popular Machine Learning Classification models.</p> <ol> <li>Logistic Regression</li> <li>K-Nearest Neighbours</li> <li>Support Vector Machines</li> <li>Naive Bayes</li> <li>Decision Tree Classification</li> <li>Random Forest Classification</li> <li>Stochastic Gradient Descent Classifier (SGD)</li> <li>Gaussian Process Classification (GPC)</li> <li>Gradient Boosting Classifier</li> <li>AdaBoost Classifier</li> <li>Bagging Classifier</li> <li>Extra Trees Classifier</li> <li>Passive Aggressive Classifier</li> <li>Ridge Classifier</li> </ol>"},{"location":"ML/Classification/#types-of-classification-algorithms","title":"Types of Classification Algorithms","text":"<ol> <li>Discriminative Learning Algorithms</li> </ol> <p>These algorithms try to learn the probability of an end result Y for a given feature set X. These algorithms try to determine how Y is directly a function of X. Mathematically these are shown as</p> <p></p> <p>Some of these algorithms try to learn a hypothesis that tries to predict the possible classes, mathematically represented as</p> <p></p> <ol> <li>Generative Learning Algorithms</li> </ol> <p>This type of Algorithms try to learn, the probability of a given set of features X for a particular class Y (mathematically represented as ) and also, the probability of occurrence of this class Y (the probability of occurrence of a given class is represented as  and is called class prior. The most popular example of such algorithms is the Naive Bayes Algorithm.</p>"},{"location":"ML/Classification/DecisionTree/","title":"Decision Tree Classifier","text":"<p>A Decision Tree is a simple but powerful machine learning technique that is mostly used for classification and regression tasks in machine learning. The decision tree algorithm tries to solve the problem by using tree representation. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label.</p> <p></p> <p>The topmost node in a Decision Tree is known as the root node, which learns to partition based on the attribute values. This process is performed recursively in a manner called recursive partitioning.</p>"},{"location":"ML/Classification/DecisionTree/#building-a-decision-tree","title":"Building a Decision Tree:","text":"<p>The philosophy of Decision trees is to find those descriptive features that can help us differentiate the classes\u2014in binary classification\u2014 or distinctive conditions in the class variable for the multi-class problems.</p> <p>A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogeneous).</p> <p>The following python code can be used to implement a Decision Tree Classifier:</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Load data\niris = load_iris()\n\n# Create decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train model\nmodel = clf.fit(iris.data, iris.target)\n</code></pre> <p>In the above Python script, we start off by importing the necessary libraries. Then, we load the Iris dataset and finally create the Decision Tree Classifier and train the model using the fit method.</p>"},{"location":"ML/Classification/DecisionTree/#advantages-of-decision-trees","title":"Advantages of decision trees:","text":"<ol> <li> <p>Easy to understand: The Decision Tree algorithm is easy to understand, interpret, and visualize since it follows a clear, logical model.</p> </li> <li> <p>Less data cleaning required: The algorithm requires less data cleaning compared to some other modeling techniques. It is not influenced by outliers and missing values to a fair degree.</p> </li> <li> <p>Data type is not a constraint: It can handle both numerical and categorical variables.</p> </li> <li> <p>Non-parametric method: Decision Tree is considered to be a non-parametric method, meaning that decision trees have no assumptions about the space distribution and the classifier structure.</p> </li> </ol>"},{"location":"ML/Classification/DecisionTree/#disadvantages-of-decision-trees","title":"Disadvantages of decision trees:","text":"<ol> <li> <p>Overfitting: Over-complex trees do not generalize the data well, resulting in overfitting. This can be solved with techniques like pruning or setting the minimum number of samples per leaf.</p> </li> <li> <p>Unstable to variations: Small variations in the data can result in a different decision tree. Hence they are usually used in an assembly (like Random Forest) for getting more robust predictions.</p> </li> <li> <p>Biased trees: If some classes dominate, Decision Trees will create biased trees. So, it is recommended to balance out the dataset before building the decision tree.</p> </li> </ol>"},{"location":"ML/Classification/DecisionTree/#appropriate-usage-scenarios","title":"Appropriate Usage Scenarios","text":"<p>Decision Trees are mostly used in classification problems. However, they are also fit for regression type tasks. They are particularly useful in the following scenarios:</p> <ol> <li> <p>Decision-making tasks: Given its transparent model, decision trees are useful for decision-making tasks like whether a borrower will default on a loan, whether to launch a new product, or what strategy to adopt in a game.</p> </li> <li> <p>Feature selection: Decision Trees algorithms like Random Forests and Gradient Boosting can be used to rank the importance of the different features.</p> </li> <li> <p>Medical diagnosis: Decision Tree algorithms are often used in the medical field for diagnosis of patients based on their symptoms or test results.</p> </li> </ol>"},{"location":"ML/Classification/DecisionTree/#conclusion","title":"Conclusion:","text":"<p>Decision Trees are easy to understand, interpret and visualize. Although they are sensitive to fluctuations in data and are prone to overfitting, they can be powerful tools for exploring and understanding your data, especially when combined with ensemble methods.</p>"},{"location":"ML/Classification/HiddenMarkovModels/","title":"HiddenMarkovModels","text":"<p>05</p>"},{"location":"ML/Classification/LogisticRegression/","title":"Logistic Regression","text":"<p>Logistic Regression is a type of algorithm used in Machine Learning for binary classification problems. The outcome of this algorithm is discrete (not continuous). The results can be interpreted as probabilities of success.</p> <p>Contrary to popular belief, the logistic regression model is a statistical model that uses a logistic function to model a binary dependent variable. In simpler words, it deals with situations where you need to predict an outcome that can have only two possible types of values.</p> <p>The goal of logistic regression is to create a best-fit model to describe the relationship between the predictor and the response variable.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Assume we have a 'df' DataFrame with 'target' as the prediction label.\nx = df.drop('target', axis=1)\ny = df.target\n\n# Splitting our data into training and test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state=123)\n\n# Normalizing our data\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n# Initialize our classifier\nlogistic_classifier = LogisticRegression()\n\n# Fitting the model to train set\nlogistic_classifier.fit(x_train, y_train)\n\n# Predicting the test set result\ny_pred = logistic_classifier.predict(x_test)\n</code></pre> <p>The logistic regression function has a characteristic 'S' shape and it can seem complex at first, but it's rather simple when you understand it.</p> <p></p> <p>In the graph above, you can see the logistic function; it's an 'S' shaped curve that can take any real-valued number and map it into a value from 0 to 1. If the curve goes to positive infinity, y approaches 1, and if the curve goes to negative infinity, y will become 0.</p> <p>If the output of logistic regression is higher than 0.5, we can conclude that the model predicts class 1. If logistic regression output is less than 0.5, then the model predicts class 0.</p> <p>Though it is one of the simpler and older algorithms, Logistic Regression is still one of the most widely used algorithms due to its simplicity and the fact that it can be implemented quickly and provide very efficient predictions on binary classification problems.</p>"},{"location":"ML/Classification/LogisticRegression/#advantages-of-logistic-regression","title":"Advantages of Logistic Regression:","text":"<ol> <li> <p>Efficiency: Computationally inexpensive compared to complex methods.</p> </li> <li> <p>Easily understandable output: The probabilities provided can be interpreted as a risk, which is directly readable unlike other classifiers like SVMs or Random Forests whose outputs are difficult to interpret in probabilistic terms.</p> </li> <li> <p>Less vulnerable to overfitting: Penalized logistic regression models can avoid overfitting by using built-in feature selection properties of the regularization technique itself.</p> </li> <li> <p>Works well with smaller dataset: It performs well even if you have fewer training samples available for your model creation.</p> </li> <li> <p>Robustness:</p> </li> <li>No assumption about distribution.</li> <li>Robust against statistically irrelevant features (e.g., transformations).</li> </ol>"},{"location":"ML/Classification/LogisticRegression/#disadvantages-of-logistic-regression","title":"Disadvantages of Logistic Regression:","text":"<ol> <li> <p>Binary Targets only Suitable:</p> </li> <li> <p>Cannot predict continuous outcomes.</p> </li> <li> <p>Outcomes must not have multi-class categories.</p> </li> <li> <p>Requires large sample size: To achieve stable results, it requires at least 50 observations per predictor variable because maximum likelihood parameters are quite unstable unless sufficient details are present.</p> </li> <li> <p>Controversy around interpretation when used with non-linear effects and interactions</p> </li> <li> <p>Nonlinearity needs transformation which makes it difficult maintaining its original ease-of-use appeal.</p> </li> <li> <p>Retains all outliers and influential values without adjustment</p> </li> <li> <p>Vulnerable towards 'Perfect Seperation'</p> </li> <li> <p>Provides less insight into individual predictors than decision trees</p> </li> </ol> <p>Instructions regarding assumptions:</p> <ul> <li>Logit should be linear in X(variable)</li> <li>Independent errors(multicollinearity)</li> </ul>"},{"location":"ML/Classification/LogisticRegression/#appropriate-usage-scenarios","title":"Appropriate Usage Scenarios:","text":"<ol> <li>When your target variable has binary outputs i.e., \"yes\" or \"no\", \"true\" or \"false\", etc., you may use Logistic JavaScript SDKs gression Classifier predictive analytics technique</li> <li>Use case scenarios include Email Spam/ Not-spam classifier; Churn Prediction,\"Will customer buy this product?\" , Health diagnosis such as Diabetes prediction: Will patient get diabetes?</li> <li> <p>In credit risk modelling result will either be default or non-default class making it classic scenario for applying logistic regression classifier.</p> </li> <li> <p>Logistic regression is best suited for situations where data is cleanly separable linearly.</p> </li> <li> <p>The main advantage lies in its simplicity since it creates straightforward decision boundaries which align along axes within input space(This gives it property-term\u201clinear classification\u201d).</p> </li> <li> <p>Can handle several categorical variables-     If you may want include categorical variables(example: gender,race)there\u2019s no requirement convert these vars into numbers-their presence adequate enough deal with multiple categories deliver robust insights.</p> </li> </ol>"},{"location":"ML/Classification/NaiveBayes/","title":"Naive Bayes Classifier","text":"<p>The Naive Bayes Classifier is a type of probabilistic machine learning model used for large scale classification problems. It assumes strong independence between features. It is simple but powerful algorithm for predictive modeling.</p> <p></p>"},{"location":"ML/Classification/NaiveBayes/#basic-principle-of-naive-bayes-classifier","title":"Basic Principle of Naive Bayes Classifier","text":"<p>Naive Bayes classifier apply Bayes' theorem with strong (naive) independence assumptions between the features. Let\u2019s understand the Bayes Theorem.</p>"},{"location":"ML/Classification/NaiveBayes/#bayes-theorem","title":"Bayes' Theorem","text":"<p>Bayes\u2019 Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayes\u2019 theorem is stated mathematically as the following equation:</p> <pre><code>P(A|B) = P(B|A) * P(A) / P(B)\n</code></pre> <p>where A and B are events.</p> <p>Here,</p> <ul> <li> <p>P(A|B) is Posterior probability: Probability of hypothesis A on the observed event B.</p> </li> <li> <p>P(B|A) is Likelihood probability: Probability of the evidence given that the probability of a hypothesis is true.</p> </li> <li> <p>P(A) is Prior Probability: Probability of hypothesis before observing the evidence.</p> </li> <li> <p>P(B) is Marginal Probability: Probability of Evidence.</p> </li> </ul>"},{"location":"ML/Classification/NaiveBayes/#implementing-naive-bayes-with-python-scikit-learn","title":"Implementing Naive Bayes with Python (Scikit-Learn)","text":"<p>Let's implement a simple demonstration of Naive Bayes classifier using Scikit-Learn library's <code>GaussianNB</code> module.</p>"},{"location":"ML/Classification/NaiveBayes/#importing-required-libraries","title":"Importing Required Libraries","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import datasets\nfrom sklearn import metrics\n</code></pre>"},{"location":"ML/Classification/NaiveBayes/#load-dataset","title":"Load Dataset","text":"<pre><code>wine = datasets.load_wine()\n</code></pre>"},{"location":"ML/Classification/NaiveBayes/#split-the-data","title":"Split the Data","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=109)\n</code></pre>"},{"location":"ML/Classification/NaiveBayes/#generate-a-model","title":"Generate a Model","text":"<pre><code>gnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_test)\n</code></pre>"},{"location":"ML/Classification/NaiveBayes/#evaluating-the-model","title":"Evaluating the Model","text":"<pre><code>print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre> <p>The output will give you the accuracy of the model. Generally, Naive Bayes classifier gives high accuracy and is very fast in prediction.</p>"},{"location":"ML/Classification/NaiveBayes/#advantages","title":"Advantages","text":"<ol> <li> <p>Ease of implementation: With the use of strong assumptions, implementation becomes straightforward and fast.</p> </li> <li> <p>Efficiency: They require a small number of training data to estimate the parameters necessary for classification because they are capable using maximum likelihood estimates.</p> </li> <li> <p>Fast and Scalable: Naive Bayes classifiers are extremely fast compared to more sophisticated methods.</p> </li> <li> <p>Good Performance in Multi Class Prediction When it comes onto handling multiple classes, naive bayesian models give better results as compared to others like logistic regression etc.</p> </li> <li> <p>Perform well with categorical input variables This makes it highly optimal for text categorization problems which involves categorical variable inputs.</p> </li> </ol>"},{"location":"ML/Classification/NaiveBayes/#disadvantages","title":"Disadvantages","text":"<ol> <li>Conditional Independence Assumption: One main limitation of Naive Bayes is the assumption of conditional independence between all pairs of predictors.</li> </ol>"},{"location":"ML/Classification/NaiveBayes/#appropriate-usage-scenarios","title":"Appropriate Usage Scenarios","text":"<ul> <li> <p>Document classification and spam filtering :Na\u00efve Bayesian model performs particularly well in case of document classification and spam filtering.</p> </li> <li> <p>Real time prediction : Since this algorithm helps predict multiclasses, it can also be used where we need probablity outputs like weather predictions or predicting diseases etc.</p> </li> <li> <p>Text Classification : Its high speed and ability handle huge dataset effectively make it perfect choice Surprisingly despite its over-simplified assumptions,</p> </li> <li> <p>Recommendation System: Though recommendation system uses complex techniques but at heart some form if naive base is definitely present for initial recommendations where you don't have enough data about likes/dislikes/preferences about users both from his/her behaviour or product side/filters/content descriptions/details/collaborative details/other decision making parameters or classifiers</p> </li> </ul>"},{"location":"ML/Classification/NaiveBayes/#conclusion","title":"Conclusion","text":"<p>The Naive Bayes classifier is a simple and effective classification algorithm, being particularly useful when working with text data and high dimensional datasets. It relies on the Bayesian theorem and assumes all variables to be independent in order to predict the class.</p> <p>In practice, the independence assumption is often violated, but Naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, Naive Bayes classifiers can outperform the more powerful alternatives.</p>"},{"location":"ML/Classification/RandomForest/","title":"Random Forest Classifier in Python","text":"<p>Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees. Random Forests corrects for decision trees' habit of overfitting to their training set.</p> <p>Random forests are a powerful tool with several advantages:</p> <ul> <li>They are very easy to use because they require very few parameters to set and they are easy to understand.</li> <li>An efficient method for estimating missing data and maintains accuracy when a large proportion of the data are missing.</li> <li>It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.</li> </ul> <p></p>"},{"location":"ML/Classification/RandomForest/#python-implementation","title":"Python Implementation","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# Create a random forest Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n# Train the model using the training sets\nclf.fit(X_train,y_train)\n\n# Predict the response for test dataset\ny_pred=clf.predict(X_test)\n\n# Model Accuracy\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre> <p>In this example, we are importing the RandomForestClassifier from sklearn.ensemble. After importing we are creating an object of RandomForestClassifier class. The RandomForestClassifier takes several parameters such as n_estimators, max_features, etc. After creating an object of RandomForest Classifier we are fitting our model on train data. The fit method takes two parameters as arguments namely, X_train and y_train. After training our model, we are predicting the target values for the test data. By using metrics.accuracy_score method, we are finding accuracy score which is around 98%.</p> <p>Remember, one thing a Random Forest algorithm is just an extension of decision tree and it is just a bunch of decision trees. More the number of decision trees in a single predictor, better the prediction will be.</p> <p>However, keep in mind that increasing number of trees in the predictor is a bit computationally expensive.</p>"},{"location":"ML/Classification/RandomForest/#advantages-of-randomforest-classifier","title":"Advantages of RandomForest Classifier:","text":"<ol> <li>Ease of Use: The Random forest algorithm can handle missing values, so there's no need for imputation.</li> <li>High Accuracy: Random forests generate higher accuracy than decision trees as it uses several trees rather then relying on individual decision trees.</li> <li>Data Versatility: It can be used for both regression and classification type problems.</li> <li>Prevents Overfitting: By aggregating the results from different trees, this reduces overfitting problem in decision trees hence delivering more accurate results.</li> </ol>"},{"location":"ML/Classification/RandomForest/#disadvantages-of-randomforest-classifier","title":"Disadvantages of RandomForest Classifier:","text":"<ol> <li>Computationally Expensive: This algorithm require much computational resources, as it builds numerous complexly structured models (trees).</li> <li>Time Consuming: While executing on large datasets, training requires high computational power and may be time consuming.</li> <li>Complex Model: Interpretability could be an issue because while random forect provides great predictive power but has very less interpretability.</li> </ol>"},{"location":"ML/Classification/RandomForest/#appropriate-usage-scenarios","title":"Appropriate Usage Scenarios:","text":"<ul> <li>Predicting if an email is a spam or not.</li> <li>Predicting whether a given patient record (of features like age, bmi etc.) will get diabetes or not</li> <li>In Bioinformatics it is widely used for genome-based disease prediction .</li> </ul> <p>In conclusion <code>RandomForestClassifier</code>, just like any other classifier has its pros and cons yet remains one of most effective classifiers with high performance across many tasks &amp; domains!</p>"},{"location":"ML/Classification/SupportVectorMachines/","title":"Support Vector Machines In Python","text":"<p>Support Vector Machines (SVMs) are a type of classification algorithm, which is used in Machine Learning for classification and regression analysis. This supervised learning method is known for its kernel trick, allowing it to handle linear as well as non-linear data.</p> <p></p>"},{"location":"ML/Classification/SupportVectorMachines/#understanding-support-vector-machines","title":"Understanding Support Vector Machines","text":"<p>The theory behind SVMs involves the concept of finding a hyperplane that separates the classes as much as possible. A hyperplane is a decision boundary that splits the data into classes. For a 2-dimensional dataset, the hyperplanes are simply lines, and for a 3-dimensional dataset, the hyperplanes are a plane; similarly, for higher-dimensional datasets, the hyperplane becomes n-dimensional.</p> <p></p> <p>The distance between the hyperplane and the nearest data points from each class is called the 'margin'. The SVM algorithm's objective is to maximize this margin, to create the most efficient, reliable and generalized model. The data points that reside at the edge of the margin are called 'support vectors'.</p>"},{"location":"ML/Classification/SupportVectorMachines/#why-use-svms","title":"Why Use SVMs?","text":"<ol> <li>SVMs are effective in high-dimensional spaces.</li> <li>They are memory-efficient because they use a subset of training points (support vectors) in the decision function.</li> <li>They can classify complex, non-linear data thanks to the kernel function.</li> </ol> <p>However, note that:</p> <ol> <li>SVMs do not perform well on large datasets because the training time could be cubic in the size of the dataset.</li> <li>They perform poorly if the dataset has more features than samples.</li> </ol>"},{"location":"ML/Classification/SupportVectorMachines/#svms-in-python","title":"SVMs in Python","text":"<p>Python's Scikit-Learn provides built-in functions to use SVMs for both regression and classification tasks. A basic example of SVM-based classification using Scikit-Learn looks like this:</p> <pre><code>from sklearn import svm\nfrom sklearn import datasets\n\n# load iris dataset as an example\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# create a classifier\nclf = svm.SVC(kernel='linear') # Linear Kernel\n\n# train the model\nclf.fit(X, y)\n\n# predict a new example\nnew_sample = [[5.1, 3.5, 1.4, 0.2]]\nprint(clf.predict(new_sample))\n</code></pre> <p>This code trains an SVM classifier using the 'linear' kernel on the iris dataset, and makes a prediction for a new iris flower sample.</p>"},{"location":"ML/Classification/SupportVectorMachines/#advantages","title":"Advantages","text":"<ol> <li> <p>Effective in high-dimensional spaces: SVM works well with even a large number of features as they compute the hyperplane using support vectors reducing computational complexity.</p> </li> <li> <p>Works well on smaller cleaner datasets: They can perform better than other algorithms when the data set isn\u2019t too large and parameters are correctly configured.</p> </li> <li> <p>Versatile: When classifying, it uses a mechanism called kernels, which makes the method very versatile and able to handle different types of data structures.</p> </li> <li> <p>Maximization principle implementation: It has solid theoretical foundations because its foundation comes from perturbation theory and statistical learning techniques, leading to solid algorithmic performances.</p> </li> <li> <p>Avoid overfitting through regularization parameter adjustment: Overfitting typically happens when you have complicated decision boundaries; however, Adjustable regularization feature means we can reduce this risk significantly.</p> </li> </ol>"},{"location":"ML/Classification/SupportVectorMachines/#disadvantages","title":"Disadvantages","text":"<ol> <li> <p>Requires full labeling of input data: This is computationally expensive especially for larger datasets.</p> </li> <li> <p>Doesn't work well with large dataset: The training time tends to increase exponentially with an increase in size of the dataset making SVMs inefficient for larger datasets.</p> </li> <li> <p>Parameters tuning required: As free parameters need tuning such as the C parameter or kernel choice that may not be simple or intuitive without sufficient domain knowledge making efficient use tricky at times.</p> </li> <li> <p>Inefficiency dealing with overlapping classes:If some classes overlap each other in several areas then separating them based on just one threshold value might result in errors while predicting data points close to these thresholds.</p> </li> </ol>"},{"location":"ML/Classification/SupportVectorMachines/#appropriate-usage-scenarios","title":"Appropriate Usage Scenarios","text":"<ul> <li> <p>Face Detection: SVMc classify parts of images as face &amp; non-face</p> </li> <li> <p>Text Categorization: Commonly employed text categorisation techniques such as linear classifiers only offer effective solutions for linear separable sets but usage of boolean kernel rich context information can be captured , thus achieving greater accuracy levels .</p> </li> <li> <p>Image Classification: Effectively used due to their ability working effectively in spaces higher dimensions .</p> </li> <li> <p>Bioinformatics : Widely used application includes identifying people at cancer risks or protein remote homology detection .</p> </li> </ul> <p>Overall, SVM is ideally suited for situations where there's clear margin between separation within your classes/ objects under consideration identified by unique characteristics displayed by each category /type .</p> <p>In summary, Support Vector Machines are a powerful technique for classification tasks, equipped with the ability to manage high-dimensionality data and with the flexibility offered by the kernel function.</p>"},{"location":"ML/Classification/adaboost/","title":"AdaBoost Classifier","text":"<p>AdaBoost (Adaptive Boosting) is a powerful ensemble machine learning algorithm that\u2019s used to improve the performance of a model by combining several weak learners to produce a highly accurate prediction. First introduced by Freund and Schapire in 1996, the algorithm works sequentially by fitting the initial model to the data and then fitting subsequent models to the previously mis-classfied instances.</p> <p><code>AdaBoost</code> is best used to boost the performance of decision trees on binary classification problems.</p> <pre><code>from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier()\nmodel.fit(x_train,y_train)\n</code></pre> <p></p>"},{"location":"ML/Classification/adaboost/#how-adaboost-works","title":"How AdaBoost Works","text":"<ol> <li> <p>The AdaBoost algorithm begins by first selecting a training subset randomly.</p> </li> <li> <p>It then iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.</p> </li> <li> <p>It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.</p> </li> <li> <p>Also, it assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.</p> </li> <li> <p>This process iterate until the complete training data fits without any error or until reached to the specified maximum number of estimators (n_estimators).</p> </li> <li> <p>To classify, perform a \"vote\" across all of the learning algorithms AdaBoost constructed.</p> </li> </ol>"},{"location":"ML/Classification/adaboost/#advantages-of-adaboost","title":"Advantages of AdaBoost","text":"<ul> <li> <p>AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners.</p> </li> <li> <p>AdaBoost is not prone to overfitting.</p> </li> <li> <p>No prior knowledge is needed about weak learners.</p> </li> </ul>"},{"location":"ML/Classification/adaboost/#disadvantages-of-adaboost","title":"Disadvantages of AdaBoost","text":"<ul> <li> <p>AdaBoost is sensitive to noisy data and outliers.</p> </li> <li> <p>It's performance depends on data and weak learner's selection.</p> </li> <li> <p>It requires enough Data.</p> </li> <li> <p>Computationally expensive.</p> </li> </ul>"},{"location":"ML/Classification/adaboost/#when-to-use-adaboost","title":"When to Use AdaBoost","text":"<p>The AdaBoost Algorithm is used for:</p> <ul> <li> <p>Solving a variety of Complex problems in different fields such as biology, computer vision, and speech processing.</p> </li> <li> <p>Solving Binary and Multiclass classification problems.</p> </li> <li> <p>Conducting face detection as a binary classification; the parts of the image are either a face or background.</p> </li> </ul> <p>In general, AdaBoost is a powerful tool when you're working with large amounts of data to make predictions with binary outcomes. But do consider it might be more resource demanding and computationally expensive than simpler models.</p>"},{"location":"ML/Classification/bagging/","title":"Bagging Classifier","text":""},{"location":"ML/Classification/bagging/#introduction","title":"Introduction","text":"<p>Bagging, which stands for Bootstrap Aggregation, is a type of ensemble learning technique. The primary principle behind Bagging is to generate several subsets of the original data and then to train our model on each subset. The final output prediction is decided by averaging the individual predictions made by each model. In this write-up, we will provide a detailed explanation of the Bagging Classifier with a practical example in Python.</p> <p></p>"},{"location":"ML/Classification/bagging/#applications-of-bagging-classifier","title":"Applications of Bagging Classifier","text":"<p>Bagging classifiers are highly suitable for high-variance and low-bias models. This includes algorithms such as Decision Trees and Neural Networks.</p> <p>Bagging is also great for tackling over-fitting issues. Over fitted models usually have a high variance, making Bagging an excellent choice to improve these models.</p>"},{"location":"ML/Classification/bagging/#python-implementation","title":"Python Implementation","text":"<p>Below is a short Python example on how to use the BaggingClassifier from sklearn:</p> <pre><code>from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\n\n# load iris dataset as an example\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Train a KNN classifier\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Create a BaggingClassifier\nbagging = BaggingClassifier(knn, max_samples=0.5, max_features=0.5)\n\n# Fit the model to the data\nbagging.fit(X, y)\n\n# Make predictions\npredictions = bagging.predict(X)\n</code></pre> <p>In the above code, we first imported the necessary libraries and loaded the iris dataset. We then defined a KNN classifier and a Bagging Classifier. The Bagging Classifier was then trained with the iris data, and predictions were made.</p>"},{"location":"ML/Classification/bagging/#advantages-and-disadvantages-of-bagging-classifier","title":"Advantages and Disadvantages of Bagging Classifier","text":""},{"location":"ML/Classification/bagging/#advantages","title":"Advantages:","text":"<ol> <li>Handling Overfitting: Bagging helps reduce the variance error, thus helping complex models from overfitting the data.</li> <li>Parallel Training: Each model is built independently; thus, Bagging is naturally suitable for parallel execution.</li> <li>Handling Large Datasets: Bagging makes it possible to learn from a dataset that would otherwise be too large to fit in memory.</li> </ol>"},{"location":"ML/Classification/bagging/#disadvantages","title":"Disadvantages:","text":"<ol> <li> <p>Bias Error: Bagging improves accuracy by reducing the variance term, but it remains ineffective for models with high bias errors.</p> </li> <li> <p>Complex and Time-consuming: The Bagging Classifier has greater computational costs because it needs to build multiple models on different subsets of the dataset.</p> </li> <li> <p>Predictability: As randomness is used for creating subsamples, the model can become slightly less interpretable than individual models.</p> </li> </ol>"},{"location":"ML/Classification/bagging/#when-to-use-bagging-classifier","title":"When to use Bagging Classifier","text":"<ul> <li>One common application for the bagging algorithm is to apply it to decision tree methods.</li> <li>It can be used when our data set has lots of features and instances, and probability of model overfitting is high.</li> <li>It can be applied to reduce the variance in a prediction by combining multiple decision trees to build an ensemble of trees.</li> </ul> <p>Bagging, in the Random Forest method, helps to mitigate the variance problem with decision trees and also aids in avoiding overfitting.</p>"},{"location":"ML/Classification/bagging/#conclusion","title":"Conclusion","text":"<p>In conclusion, Bagging Classifier is a powerful tool for increasing model performance by reducing variance error. Despite its disadvantages, its ability to learn from large datasets and mitigate overfitting is a major advantage in various practical applications.</p>"},{"location":"ML/Classification/extra_tree/","title":"Extra Trees Classifier","text":"<p><code>Extra Trees Classifier</code> or Extremely Randomized Trees is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a \u201cforest\u201d to output it\u2019s final result. It was first introduced by Geurts et al. (2006). It's part of Scikit-Learn library in Python programming.</p> <p>To each split in the data subset, it fits each decision tree on a random subset of the total data. This randomness and variance leads to a model that is robust and prone to overfiting.</p> <p>This randomness actually adds some bias to the model, but due to averaging of several uncorrelated models, it reduces the variance and hence leads to better generalization.</p> <p>Extra Trees differ from Random Forest in the way how they choose the split of each internal node.</p> <p></p> <p>For the Random Forest classifier, The best split is selected among all possible splits. However, Extra Trees induce further randomness: They select splits at random. This leads to even more variance in the model, leading to an overall reduction in the model bias.</p>"},{"location":"ML/Classification/extra_tree/#advantages","title":"Advantages","text":"<ol> <li>Handling Overfitting: One advantage of Extra Trees Classifier over Random Forest is that it is less likely to overfit the training set.</li> <li>Speedy Training: It can train multiple trees in parallel. Moreover, due to the randomness of the cut point selection, it is less computational expensive than Random Forest.</li> <li>Flexibility: This model can also be useful for both regression and classification problems.</li> </ol>"},{"location":"ML/Classification/extra_tree/#disadvantages","title":"Disadvantages","text":"<ol> <li>Randomness: There\u2019s a high randomness due to randomized cut-points, which can lead to a less optimal decision boundary, hence less accurate model.</li> <li>High Variance: Extra Trees Classifier models can have a high variance themselves, which can make the model sensitive to the specific data used for training.</li> <li>Less Interpretability: The model is not easily interpretable, this is, you cannot understand how a prediction was made.</li> </ol>"},{"location":"ML/Classification/extra_tree/#when-to-use-extra-trees-classifier","title":"When to use Extra Trees Classifier","text":"<p>Extra Trees Classifier can be used in various fields including healthcare and e-commerce. It is also commonly used in situations where predictive accuracy is more important than model interpretability.</p> <p>If you have a complex dataset with a mixture of types and you\u2019re in a hurry and don\u2019t mind a bit of a \u201cblack box\u201d solution, Extra Trees (or Random Forests) might be a good starting point. Extra Trees is also less sensitive to the specific hyper-parameters used, which makes it easier to use without tweaking parameters.</p> <p>In conclusion, while the Extra Trees Classifier is powerful and can save computational time and resources, it is not always the most accurate model depending on your needs and your data set. It is best to understand and consider these factors when choosing which model to use.</p>"},{"location":"ML/Classification/gbc/","title":"Gradient Boosting Classifier","text":"<p>Gradient Boosting Classifier is a set of machine learning algorithms that's primarily used for handling regression and classification problems in a supervised learning setting. It belongs to the class of boosting ensemble methods, where new models are added to correct the errors made by the existing models.</p> <p>The term 'Gradient Boosting' originates from the fact that the algorithm uses gradient descent to minimize errors.</p> <p></p>"},{"location":"ML/Classification/gbc/#how-it-works","title":"How it works","text":"<p>Here's a step-by-step breakdown of how Gradient Boosting Classifier works:</p> <ol> <li>A model is built on a subset of data.</li> <li>This model is used to make predictions on the whole dataset.</li> <li>Errors are then calculated by comparing the predictions and actual values.</li> <li>A new model is created that predicts these errors.</li> <li>This process is repeated until the algorithm can't minimize the error anymore.</li> </ol>"},{"location":"ML/Classification/gbc/#gradient-boosting-involves-three-elements","title":"Gradient Boosting involves three elements:","text":"<ol> <li> <p>A Loss function: The loss function is a measure showing how good our model\u2019s predictions are. We want a loss function that can be optimized better. So it should be differentiable.</p> </li> <li> <p>A weak learner: For boosting problem, decision trees are used as a weak learner. They are used because of their interpretability, simplicity and computational effectiveness</p> </li> <li> <p>An Additive model: Gradient boosting involves the creation of an ensemble of sequential weak learners, each correcting the predecessor's mistakes.</p> </li> </ol> <pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\n#Creting an instance of GradientBoosting classifier\ngbc = GradientBoostingClassifier(random_state=0)\n\n#Training the classifier\ngbc.fit(X_train, y_train)\n\n#Making predictions\ny_pred = gbc.predict(X_test)\n</code></pre>"},{"location":"ML/Classification/gbc/#advantages-of-gradient-boosting-classifier","title":"Advantages of Gradient Boosting Classifier","text":"<ol> <li>Great predictive power: Gradient Boosting algorithms are known for their impressive predictive capacities.</li> <li>Flexibility: They can be used in both regression and classification problems, and it works well with various types of data.</li> <li>Handle missing data: Gradient Boosting can handle missing data without the need for imputation.</li> <li>Feature Importance: GBC use decision trees where feature importance is a measurable strategic benefit.</li> </ol>"},{"location":"ML/Classification/gbc/#disadvantages-of-gradient-boosting-classifier","title":"Disadvantages of Gradient Boosting Classifier","text":"<ol> <li>Overfitting: If the data sample is too small, Gradient Boosting algorithms tend to overfit.</li> <li>Requires careful tuning: It requires careful tuning of different hyperparameters, which can sometimes be computationally expensive.</li> <li>Long training period: Gradient Boosting might take longer to train as trees are built sequentially.</li> </ol>"},{"location":"ML/Classification/gbc/#when-to-use-gradient-boosting-classifier","title":"When to use Gradient Boosting Classifier","text":"<p>Gradient Boosting can be used effectively for a variety of machine learning tasks like classification, regression, ranking etc. It works extremely well on structured data, where there is a clear definition of entities - customers, products, user, etc.</p> <p>On an industry level, it can be used for credit scoring, churn prediction, anomaly detection in cyber security, predicting machine faults in predictive maintenance etc.</p> <p>In scenarios where accuracy is more important than interpretability, and where computational time isn\u2019t a focus point, it shines out as a good choice.</p> <p>Gradient Boosting Classifier is often your best bet if you prioritize predictive power over model interpretability. It performs well in situations where the dataset has multiple complex relationships because it combines multiple 'weak learners' to create one strong rule.</p> <p>Furthermore, if you are working with a combination of categorical and numerical features, or just numerical but expect non-linear relationship, you can use Gradient Boosting Classifier. It also perfectly fits in scenarios where you're less bothered about model interpretability and more focused on creating accurate predictions.</p> <p>Overall, Gradient Boosting Classifier is a powerful algorithm that performs excellently in a wide range of tasks. However, depending on your specific needs and the constraints of your project, other algorithms might be a better fit. Therefore, always consider the strengths and weaknesses of each algorithm and strive to understand the needs of your project before making a decision.</p>"},{"location":"ML/Classification/gpc/","title":"Gaussian Process Classification explanation","text":""},{"location":"ML/Classification/gpc/#introduction","title":"Introduction","text":"<p>A Gaussian Process Classification (GPC) is a type of probabilistic model that is primarily used in the field of machine learning. This classification model is non-parametric, meaning that it makes no assumptions about the underlying data distribution.</p> <p>GPC models are quite useful due to their ability to provide not only the classification results but also the associated uncertainties, finding its purposes in various fields of classification tasks.</p> <p></p> <p>Figure: Illustration of Gaussian Process Classification.</p>"},{"location":"ML/Classification/gpc/#python-explanation","title":"Python Explanation","text":"<p>Below is an example implementation of GPC in Python using the scikit-learn library.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n\n# Load dataset\niris = load_iris()\n\n# Initialize Gaussian Process Classifier\ngpc = GaussianProcessClassifier(random_state=0)\n\n# Fit the model\ngpc.fit(iris.data, iris.target)\n\n# Perform prediction\npredictions = gpc.predict(iris.data)\n</code></pre>"},{"location":"ML/Classification/gpc/#advantages","title":"Advantages","text":"<ol> <li> <p>Uncertainty Estimates: GPC models provide probabilistic predictions, which include uncertainty estimates. This is useful in scenarios where it's important to know the certainty of the prediction.</p> </li> <li> <p>Non-Parametric: GPC models do not presume any specific underlying data distribution, making them dependable for various data distributions.</p> </li> <li> <p>Versatility: They can be used for both regression and classification problems.</p> </li> </ol>"},{"location":"ML/Classification/gpc/#disadvantages","title":"Disadvantages","text":"<ol> <li> <p>Computational Complexity: GPC's main disadvantage is its computational complexity. Especially when dealing with large datasets, the computational expense can become quite significant.</p> </li> <li> <p>Difficult to Scale: Due to its computational cost, GPC models are difficult to scale on large datasets.</p> </li> <li> <p>Sensitive to Noise: GPC models may be quite sensitive and react to observations contaminated with noise.</p> </li> </ol>"},{"location":"ML/Classification/gpc/#when-to-use","title":"When to Use","text":"<p>GPC models are suitable for smaller datasets or when versatile, probabilistic predictions with uncertainty estimates are needed. They are especially useful in scenarios where the data is presumed to have a local structure.</p> <p>Examples of typical applications include,</p> <ol> <li>Forecasting in various fields such as climate, finance or sales</li> <li>Robotics for localization, mapping and path planning</li> <li>Bioinformatics for protein secondary structure prediction</li> <li>Medical diagnosis, a probabilistic outcome might be essential in the decision-making process.</li> </ol> <p>In summary, GPC is a powerful tool for probabilistic modeling and is valuable in machine learning when predictions with uncertainty estimates are helpful.</p>"},{"location":"ML/Classification/gpc/#conclusions","title":"Conclusions","text":"<p>In conclusion, Gaussian Process Classification is a versatile machine learning model that while having some disadvantages such as computational complexity, offers valuable capabilities such as providing probabilistic predictions with associated uncertainty estimates. It can be effectively utilized on smaller datasets and when the certainty of predictions is of importance.</p>"},{"location":"ML/Classification/knn/","title":"K-Nearest Neighbors Classifier","text":"<p>K-Nearest Neighbors (K-NN) is one of the simplest machine learning algorithms. It is a type of instance-based learning where the function is only approximated locally and all computation is deferred until classification. The K-NN algorithm is a non-parametric method used for both regression and classification problems.</p> <p>When the K-NN is used for classification, the output is a class membership. An object is assigned to the class most common among its K nearest neighbors, where K is a positive integer, typically small. If K = 1, then the object is simply assigned to the class of that single nearest neighbor.</p> <p></p>"},{"location":"ML/Classification/knn/#how-does-it-work","title":"How does it work?","text":"<p>K-NN works on the principle of similarity or proximity. Here are the general steps:</p> <ol> <li>A positive integer K is specified.</li> <li>The K nearest data points are selected based on a distance metric.</li> <li>The majority class among the K nearest neighbors is then assigned to the test point.</li> </ol> <p>If a data point is in close vicinity to several points that belong one category, chances are it'll belong to that same category.</p>"},{"location":"ML/Classification/knn/#python-implementation","title":"Python Implementation","text":"<p>The Scikit-learn library in Python provides a function called <code>KNeighborsClassifier</code> from the <code>sklearn.neighbors</code> package, that can be used to implement K-NN.</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n</code></pre> <ul> <li><code>n_neighbors</code> is the parameter to set the <code>K</code> value. It's the number of neighbors to consider.</li> <li><code>fit()</code> function fits the model to the training data.</li> <li>We can then use <code>predict()</code> function to predict the class labels.</li> </ul> <pre><code>knn.predict(X_test)\n</code></pre> <p>Where <code>X_test</code> is the testing data which the classifier will predict.</p>"},{"location":"ML/Classification/knn/#choosing-the-correct-k-value","title":"Choosing the correct K value","text":"<p>Choosing the value of <code>K</code> can be tricky. A low value for <code>K</code> will be influenced by the noise in the data and a high value will be more computationally expensive. As a general guideline a good starting point for <code>K</code> is <code>sqrt(n)</code> where <code>n</code> is total number of data points.</p> <p>A commonly used method is to choose <code>K</code> is the cross-validation.</p> <p>Using the cross-validation, we could calculate the accuracy of the K-NN algorithm for different K values. Then the K with the highest accuracy could be chosen.</p>"},{"location":"ML/Classification/knn/#advantages","title":"Advantages","text":"<ul> <li>No assumptions about data \u2014 useful, for example, for nonlinear data</li> <li>Simple algorithm \u2014 to explain and understand/interpret</li> <li>High accuracy (relatively) \u2014 it is pretty high but not competitive in comparison to better supervised learning models</li> </ul>"},{"location":"ML/Classification/knn/#limitations","title":"Limitations","text":"<ul> <li>Computationally expensive \u2014 because the algorithm stores all of the training data</li> <li>High memory requirement \u2014 Stores all (or almost all) of the training data</li> <li>Prediction stage might be slow with big N</li> <li>Sensitive to irrelevant features and the scale of the data</li> </ul> <p>In summary, the K-NN algorithm is good for large dataset with fewer attributes (low dimensional space) and where data points are more uniformly spread.</p>"},{"location":"ML/Classification/knn/#k-nearest-neighbors-knn-classifier","title":"K-Nearest Neighbors (KNN) Classifier","text":"<p>The K-nearest neighbors (KNN) algorithm is a type of instance-based learning, or lazy learning. This classifier algorithm measures the distance between samples and classify them based on it.</p>"},{"location":"ML/Classification/knn/#advantages_1","title":"Advantages:","text":"<ol> <li> <p>No Assumption About Data: The algorithm makes no assumption about the underlying data distribution pattern which makes it very useful for nonlinear data.</p> </li> <li> <p>Updating Algorithm: New training examples can be added easily to model, thereby this approach remains robust in changing scenarios.</p> </li> <li> <p>Ease Of Use: It\u2019s a very simple and easy to understand Machine Learning algorithms yet powerful tool,</p> </li> <li> <p>Naturally handles multi-class cases: Different classes in target variable are treated equally irrespective to their frequency.</p> </li> <li> <p>Robust to noisy training data : Works well with noise in the dataset as long as noise    does not completely obscure the signal.</p> </li> </ol>"},{"location":"ML/Classification/knn/#disadvantages","title":"Disadvantages:","text":"<ol> <li> <p>Computationally Expensive: As the dataset grows efficiency or speed of algorithm declines rapidly due to its operation in calculating distances from each point to every other point in the dataset</p> </li> <li> <p>Normalization Of Dataset: Before applying KNN, normalization should always be performed; otherwise higher ranged features might dominate when computing distance.</p> </li> <li> <p>Sensitive To Noisy And Missing Data: Outlier values will mislead prompting faulty analysis</p> </li> <li> <p>Doesn\u2019t work well with high dimensional data: As dimensions increase model begins losing significant performance due \u201cCurse Of Dimensionality\u201d.</p> </li> <li> <p>Dimensions sensitivity : As dimensions/features increases its effectiveness decreases due increased space leading curse dimensionality.</p> </li> </ol>"},{"location":"ML/Classification/knn/#appropriate-usage-scenarios","title":"Appropriate Usage Scenarios:","text":"<p>1.K- Nearest-Neighbors widely used for both Classification &amp; Regression predictive problems .</p> <p>2: If your problem requirement doesn\u2019t involve time constraint you plan on working with small datasets</p> <p>In those cases where numerical output variable based prediction is not required.</p> <p>3: Classification problems where you have labelled data such as spam detection,email classification etc</p> <p>4: Recommender Systems - once trained ,the k- nearest neighbours of product can suggest similar items</p> <p>5: Features have identical scales \u2013 since k-NN work by calculating distances if range are not comparable then normalisation needed else use different approach</p> <p>May incur problem dealing these scenarios :</p> <ul> <li> <p>Large Datasets: Given computational cost typically doesn't turn out huge datasets,</p> </li> <li> <p>High Dimensions: Due Curse dimensionality avoid it using high-dimension spaces.</p> </li> </ul>"},{"location":"ML/Classification/passive_aggre/","title":"Passive Aggressive Classifier","text":"<p>A Passive-Aggressive Classifier is an online-learning algorithm that remains passive for a correct classification outcome, and turns aggressive in the case of a miscalculation, updating and adjusting. Unlike most other algorithms, it does not converge. Its purpose is to make updates that correct the loss, causing very little change in the norm of the weight vector.</p> <p>The Passive-Aggressive algorithms are called so because:</p> <ul> <li>Passive: If the prediction is correct, keep the model and do not make any changes. i.e., the data in the example is not enough to cause any changes in the model.</li> <li>Aggressive: If the prediction is incorrect, make some changes to the model. i.e., some change to the model may correct it.</li> </ul> <p></p>"},{"location":"ML/Classification/passive_aggre/#python-implementation","title":"Python Implementation","text":"<p>Here is a simple example:</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata = fetch_20newsgroups()\nX = data.data\ny = data.target\n\nvectorizer = TfidfVectorizer()\nX_vector = vectorizer.fit_transform(X)\n\nclf = PassiveAggressiveClassifier()\nclf.fit(X_vector, y)\n\npredictions = clf.predict(X_vector)\nprint(\"Accuracy:\", accuracy_score(y, predictions))\n</code></pre>"},{"location":"ML/Classification/passive_aggre/#advantages","title":"Advantages","text":"<ol> <li> <p>Efficient: Passive Aggressive Classifier is an online learning algorithm and therefore is very efficient on large datasets.</p> </li> <li> <p>Flexible: It does not require a learning rate.</p> </li> <li> <p>Large Margin: It naturally supports the max-margin principle which can help generalize the model.</p> </li> </ol>"},{"location":"ML/Classification/passive_aggre/#disadvantages","title":"Disadvantages","text":"<ol> <li> <p>Non-probabilistic: Passive Aggressive Classifier does not output probabilities.</p> </li> <li> <p>No Learning Rate: Passive Aggressive Classifier does not include a learning rate. This means that the updates it makes can be quite drastic in the event of a misclassification, which can lead to overfitting if too much importance is given to these outliers.</p> </li> </ol>"},{"location":"ML/Classification/passive_aggre/#when-to-use-passive-aggressive-classifier","title":"When to use Passive Aggressive Classifier","text":"<p>IBM\u2019s Watson Natural Language Classifier, text classification, and spam filtering in Emails are some of the major applications of Passive-Aggressive Classifiers. It can also be implemented in cases where early prediction is essential, as in the case of live risk analysis in banks, real-time predictions of election results, etc. It's primarily used when data is arriving sequentially or the model needs to be changed dynamically with each new prediction, and you want to update your model as quickly as possible as computing resources are limited.</p>"},{"location":"ML/Classification/ridge/","title":"Ridge Classifier","text":"<p>Ridge classifier is a classification machine learning algorithm. It is a type of linear model that uses L2 regularization. Ridge Classifiers are essentially a regularized version of Linear Discriminant Analysis (LDA). When you have multi-class targets, a one-vs-rest scheme is used.</p>"},{"location":"ML/Classification/ridge/#basic-explanation","title":"Basic Explanation","text":"<p>In simple terms, Ridge Classifier is a classification algorithm that uses L2 regularization to minimize the magnitude of the coefficients. This is done in order to prevent overfitting, which occurs when the model is overly complex and performs well on the training data but poorly on the test data.</p> <p>Here is a diagram that represents the concept of overfitting, which Ridge Classifier tries to prevent:</p> <p></p> <p>In the case of Ridge Classifier, each feature contributes to the decision function with a weight that is roughly proportional to its importance. It employs the so-called \"ridge regression\" as a base classifier, using a one-vs-rest scheme to deal with multi-class targets.</p>"},{"location":"ML/Classification/ridge/#python-example","title":"Python Example","text":"<p>Below is a simple example of a how to use the Ridge Classifier with Scikit-learn in Python.</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Create a synthetic binary classification problem\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Create Ridge Classifier\nclf = RidgeClassifier()\n\n# Train Ridge Classifier\nclf.fit(X_train, y_train)\n\n# Predict the response for test dataset\ny_pred = clf.predict(X_test)\n</code></pre>"},{"location":"ML/Classification/ridge/#advantages-of-ridge-classifier","title":"Advantages of Ridge Classifier","text":"<ol> <li>Prevention of Overfitting: The Ridge Classifier's primary advantage is its ability to avoid overfitting through the use of L2 regularization.</li> <li>Handling Multicollinearity: Ridge Classifier can handle multicollinearity (high correlations between predictor variables) well.</li> <li>Efficiency: It works well even in situations where the number of variables is greater than the number of observations.</li> <li>Simplicity and Speed: It's generally faster than more complex machine learning algorithms.</li> </ol>"},{"location":"ML/Classification/ridge/#disadvantages-of-ridge-classifier","title":"Disadvantages of Ridge Classifier","text":"<ol> <li>Not a Feature Selector: Ridge regression includes all predictors in the final model, so while it will shrink the coefficients of irrelevant variables close to zero, it will never fully eliminate them. This can lead to model interpretation problems if you have a high number of predictors.</li> <li>Sensitivity to High Variance: Ridge classifier can be sensitive to input features with high variances. This is because a feature with higher variance will influence the classifier output more. To circumvent this issue, we often standardize the input features so they all have the same variance.</li> </ol>"},{"location":"ML/Classification/ridge/#scenario-to-use","title":"Scenario to Use","text":"<p>Use Ridge Classifier when you have a multi-collinear dataset (multiple independent variables show some level of correlation with each other), or when you have many independent variables and you want to reduce the effect of overfitting.</p> <p>It can be especially beneficial to use ridge regression when you're dealing with a problem where it's necessary to include all predictors, such as when you need to assess the effect of all predictors but don't necessarily care to create a sparse model.</p>"},{"location":"ML/Classification/sgd/","title":"Stochastic Gradient Descent Classifier","text":"<p>Stochastic Gradient Descent (SGD) is a simple yet efficient approach to fit linear models. It's an efficient way of implementing linear classifiers and regressors under convex loss functions, such as huber, epsilon_insensitive, squared_loss, and more.</p>"},{"location":"ML/Classification/sgd/#explanation","title":"Explanation","text":"<p>In an iterative manner, SGD selects one training sample at each step and computes the gradient of the loss function. In simpler terms, SGD tries to find minimums or maximums by iteration.</p> <p>Below is a representation of how SGD function estimates parameters in a gradient descent manner:</p> <p></p>"},{"location":"ML/Classification/sgd/#python-explanation","title":"Python Explanation","text":"<pre><code>from sklearn.linear_model import SGDClassifier\n\n# Initializing the model\nclf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n\n# Training the model\nclf.fit(X, Y)\n\n# Predicting the output\nclf.predict([[-0.8, -1]])\n</code></pre> <p>In the above example, we first import <code>SGDClassifier</code> from <code>sklearn.linear_model</code>. We initialize the classifier with the hinge loss and l2 penalty. We train the model using <code>clf.fit()</code> function and predict the output using <code>clf.predict()</code> function.</p>"},{"location":"ML/Classification/sgd/#advantages","title":"Advantages","text":"<ul> <li>Efficiency: SGD allows training on large scale data due its efficiency. It is well-suited for 'online' learning setting.</li> <li>Ease of implementation: It provides a lot of opportunities for code tuning.</li> </ul>"},{"location":"ML/Classification/sgd/#disadvantages","title":"Disadvantages","text":"<ul> <li>No Guaranteed Convergence: One disadvantage of SGD is that it does not always guarantee to rapidly converge.</li> <li>Hyperparameters: Requiring a number of hyperparameters such as the regularisation parameter and the number of iterations.</li> <li>Sensitive to feature scaling: It is sensitive to feature scaling, that is it does not work well when the features are not on a relatively similar scale.</li> </ul>"},{"location":"ML/Classification/sgd/#usage-scenario","title":"Usage Scenario","text":"<p>SGD is used in a variety of applications. It is excellent for large-scale and sparse machine learning problems, often encountered in text classification and natural language processing.</p> <p>If the data is sparse, the code in SGDClassifier easily scales to problems with more than 10^5 training examples and more than 10^5 features.</p>"},{"location":"ML/Classification/sgd/#references","title":"References","text":"<ul> <li>Stochastic Gradient Descent-Sklearn</li> <li>Machine Learning with Python</li> </ul>"},{"location":"ML/Clustering/","title":"Clustering Algorithms","text":"<p>In real world, data does not always come with labels and in such cases, we need to create our own clusters/labels in order to make some groupings in the data. This is called unsupervised learning. The following algorithms are used to do this:</p>"},{"location":"ML/Clustering/#clustering-algorithms-and-their-features","title":"Clustering Algorithms and Their Features","text":"Algorithm Category Features K-Means Partitioning - Simple and fast- Works well with large datasets- Assumes clusters are spherical and balanced Hierarchical Clustering Hierarchical - Does not require the number of clusters to be specified- Can be visualized using dendrograms DBSCAN (Density-Based) Density-Based - Can find arbitrarily shaped clusters- Good for data with noise and outliers Mean Shift Centroid-Based - Does not require the number of clusters to be specified- Can find clusters of any shape OPTICS (Ordering Points To Identify the Clustering Structure) Density-Based - Generalizes DBSCAN by addressing varying density clusters- Provides a reachability plot for cluster hierarchy Spectral Clustering Graph-Based - Good for non-convex clusters- Uses graph distance to cluster points Affinity Propagation Graph-Based - Does not require the number of clusters to be specified- Sends messages between pairs of samples Agglomerative Clustering Hierarchical - A type of hierarchical clustering- Uses a bottom-up approach BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) Hierarchical - Designed for very large datasets- Builds a tree-like structure to cluster <p>Note that this is not an exhaustive list, and there are many other clustering algorithms and variations thereof. Each algorithm has its own set of parameters and assumptions that can affect its performance on different datasets.</p>"},{"location":"ML/Clustering/DBSCAN/","title":"Understanding DBSCAN (Density-Based Spatial Clustering of Applications with Noise)","text":"<p>DBSCAN is a popular clustering algorithm that is primarily used in data mining and machine learning. Below we will discuss the advantages and disadvantages of DBSCAN, include images for better understanding, provide sample code, and explain the scenarios where DBSCAN can be effectively used.</p>"},{"location":"ML/Clustering/DBSCAN/#advantages-of-dbscan","title":"Advantages of DBSCAN","text":"<ol> <li>No need to specify the number of clusters: Unlike K-means, DBSCAN does not require you to specify the number of clusters beforehand.</li> <li>Capable of finding arbitrarily shaped clusters: DBSCAN can find non-linearly separable clusters that other algorithms might not be able to detect.</li> <li>Robust to outliers: DBSCAN is less affected by noise and outliers because it groups together densely packed points and labels low-density points as outliers.</li> <li>Only two parameters: DBSCAN requires only two parameters: the neighborhood size (<code>eps</code>) and the minimum number of points required to form a dense region (<code>min_samples</code>).</li> </ol> <p> The image above illustrates how DBSCAN clusters data points based on density.</p>"},{"location":"ML/Clustering/DBSCAN/#disadvantages-of-dbscan","title":"Disadvantages of DBSCAN","text":"<ol> <li>Difficulty in finding appropriate parameters: Choosing the right <code>eps</code> and <code>min_samples</code> for different data densities can be challenging.</li> <li>Not suitable for varying density clusters: DBSCAN can struggle with clusters of varying densities. It might not correctly separate clusters with different density levels.</li> <li>Struggles with high-dimensional data: As with many clustering algorithms, DBSCAN's performance deteriorates in high-dimensional spaces due to the curse of dimensionality.</li> </ol>"},{"location":"ML/Clustering/DBSCAN/#sample-code","title":"Sample Code","text":"<p>Here is a simple example of how to use DBSCAN in Python with the <code>scikit-learn</code> library:</p> <pre><code>from sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nclusters = dbscan.fit_predict(X)\n\n# Plot the clusters\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', marker='o')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n</code></pre> <p>This code generates a two-dimensional dataset with a moon shape and applies DBSCAN to it. The resulting clusters are plotted with different colors.</p>"},{"location":"ML/Clustering/DBSCAN/#scenarios-for-using-dbscan","title":"Scenarios for Using DBSCAN","text":"<p>DBSCAN is particularly useful in the following scenarios:</p> <ol> <li>When the number of clusters is unknown: If you have no idea how many clusters to expect in your data, DBSCAN can be a good choice.</li> <li>For complex geometric shapes: Use DBSCAN when you have complex-shaped data or clusters that are not spherical.</li> <li>When dealing with spatial data: DBSCAN is ideal for spatial data clustering where density can indicate the grouping.</li> <li>Handling noise and outliers: In datasets where there are significant noise and outliers, DBSCAN can help by identifying these points.</li> </ol> <p>DBSCAN is a powerful clustering algorithm when used in the right context. Understanding its advantages and limitations can help you decide whether it's the right tool for your specific problem.</p>"},{"location":"ML/Clustering/HierarchicalClustering/","title":"Hierarchical Clustering","text":"<p>The idea behind this clustering algorithm is similar to K-means clustering but is different in quite a few aspects. It works as follows:</p> <ol> <li>Assign each data point in the dataset a single cluster of its own</li> <li>Take 2 closest data points in the dataset and join them to make 1 cluster (resulting in n-1 clusters)</li> <li>Take two closest clusters and make them 1 cluster (resulting in n-2 clusters)</li> <li>Repeat Step 3 until there is only one cluster</li> <li>Finish</li> </ol> <p>One might find the criterion for selection of closest clusters in Step 3 to be ambiguous as a concept and unclear from the algorithm above. It is because, although it is based on Euclidean distances between clusters, there are multiple options within Euclidean distances when we are talking about more than two points, and we are discussing clusters here. There are four kinds of Euclidean distances that can be used for the selection of closest clusters and they are defined as follows:</p> <ol> <li>Distance between closest points</li> <li>Distance between farthest points</li> <li>Mean of distances between all the points</li> <li>Distance between centroids of the two clusters</li> </ol>"},{"location":"ML/Clustering/HierarchicalClustering/#dendrograms","title":"Dendrograms","text":"<p>A graph that allows us to remember what we did through the course of the entire algorithm. They typically represent the points and progression of clustering using HC algorithm. The height in a Dendrogram represents the distance between any two given points. In the above figure the points 10 and 2 are the closest to each other and form the first cluster reducing the size from 10 clusters in Step 1 to 9 clusters in Step 2. This keeps on progressing  as we go up the dendrogram, and merge clusters on the way</p>"},{"location":"ML/Clustering/HierarchicalClustering/#dissimilarity-threshold","title":"Dissimilarity Threshold","text":"<p>The dissimilarity threshold is the horizontal bar that we set for obtaining clusters from a dendrogram. In the above example we set it to be 0.56 which means that any clusters with distance more than 0.56 should not be merged together to form any more clusters. This leaves us with 5 clusters containing points (3), (7, 6), (1, 4), (9, 5, 8) and (10, 2) respectively. </p> <p>Intuitively, the final number of clusters should always be equal to the number of lines passing by the Dissimilarity Threshold of the final dendrogram. </p> <p>Selecting the correct number of clusters is a decision that can be easily made on the basis of height of each of the vertical lines. The longest vertical lines usually go on to form the worst clusters. </p> <p>It is important to note that the vertical lines that we consider for selecting the correct number of clusters, are not the original vertical lines but the lines that were formed after we extend all the horizontal lines as well. So the steps would be to extend all horizontal lines infinitely and then look for the longest vertical line possible. This distance would give us the optimal number of clusters. </p>"},{"location":"ML/Clustering/K-meansClustering/","title":"K-means Clustering","text":"<p>This is a classic clustering algorithm that relies on the concept of centroids and their Euclidean distances from the observed data points. The basic concept works on the following set of rules:</p> <ol> <li>Assign a fixed number of centroids randomly in the parameter space (the number of centroids will define the number of clusters formed at the end of execution of the algorithm). These centroids need not be one of the points in the observation set, and can literally be random coordinates in the multi-dimensional space that we have.</li> <li>Calculate the closest centroid from each data point in the observation set and assign the data point to that centroid's cluster.</li> <li>Move the centroid to the 'center-of-mass' of the cluster that it has created with help of our data points from observation set.</li> <li>Repeat Step 2 and see if any points have changed their clusters, from the ones they were previously assigned. If the condition holds true then move to Step 3 otherwise proceed to Step 5. </li> <li>Finish</li> </ol> <p>Although, the algorithm might appear like its' construction of clusters based on the distances, this assumptions is untrue. The k-means clustering algorithm works primarily on minimizing the intra-cluster variance and that is the reason why metric of computation for accuracy of a k-means cluster is WCSS (within-cluster sum of squares).</p>"},{"location":"ML/Clustering/K-meansClustering/#objective-function-for-soft-k-means","title":"Objective Function for Soft K-means","text":"<p>The objective function for K-means clustering is given by:</p> <p></p> <p>This equation is the sum of squared distances weighted by the responsibilities. This means that if  is far away from cluster , that responsibility should be very low. This is an example of coordinate descent, which means that we are moving in the direction of a smaller J with respect to only one variable at a time. As we have already established, although with each iteration, we converge towards a smaller J, there is absolutely no guarantee that it will converge to a global minimum.</p> <p>It is interesting to observe that the k-means clustering algorithm relies on Euclidean distances for formation of clusters and computation of intra-cluster variation. This is an implicit underlying bias of the algorithm and can be exploited for other kinds of correlations between the attributes by transforming them into Euclidean distances. Click here for a more detailed explanation regarding this. </p> <p>Drawbacks: As it should be already obvious at this point, the selection of random points could cause serious problems because this randomness would let the algorithm to figure out different clusters than the ones that are actually present in the hyper-dimensional space. This sensitivity to initialization can be alleviated to some extent by the use of following techniques:</p> <ol> <li>Run the algorithm multiple times and choose the centroids that give us the best cost</li> <li>Soft K-means algorithm: This allows for each point to be assigned to more than one cluster at a time, allowing for a probability distribution over the centroids.</li> <li>K-means++ algorithm: </li> </ol>"},{"location":"ML/Clustering/K-meansClustering/#soft-k-means-algorithm","title":"Soft K-means algorithm","text":"<p>The Soft K-means algorithm works as follows:</p> <ol> <li>initialize m<sub>1</sub> ... m<sub>x</sub> as random points</li> <li>Calculate cluster responsibilities using </li> <li>Calculate new means using </li> <li>If converged, goto Step 5, else goto Step 2</li> <li>Finish</li> </ol>"},{"location":"ML/Clustering/K-meansClustering/#research-articles","title":"RESEARCH ARTICLES","text":"<ol> <li> <p>K-Means++: The advantages of careful seeding; David Arthur, Sergei Vassilvitskii</p> <p>[Solved] The random selection of centroids would often let the k-means algorithm figure out different clusters than the ones that are actually present in the hyper-dimensional space.</p> <p>This algorithm selects only the first centroid at random and then picks the remaining centroids by using a probability distribution function over the data points.  (Section 2.2) </p> </li> <li> <p>A comparative study between fuzzy clustering algorithm and Hard Clustering algorithm;  Dibya Joti Bora, Dr. Anil Kumar Gupta</p> <p>[Solved] Sensitivity to random start of K-means has been alleviated to some extent using fuzzy clustering. Any point does not fully belong to one cluster and there's a probability of over the asignment of any point to a cluaster.</p> </li> </ol>"},{"location":"ML/Clustering/K-meansClustering/#code-references","title":"CODE REFERENCES","text":"<p>Read the documentation for more </p>"},{"location":"ML/Clustering/MeanShift/","title":"Mean Shift Clustering","text":"<p>Mean Shift clustering is a versatile algorithm that aims to discover blobs in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</p>"},{"location":"ML/Clustering/MeanShift/#advantages-of-mean-shift-clustering","title":"Advantages of Mean Shift Clustering","text":"<ol> <li>No Assumption of Cluster Shape: Mean Shift does not assume any prior knowledge on the number of clusters or their shape, unlike K-means which assumes the clusters to be spherical.</li> <li>Robustness to Outliers: The algorithm is not much affected by outliers since it takes the mean of points in a region.</li> <li>Versatility: The bandwidth parameter can be adjusted to fine-tune the number of clusters.</li> <li>Only One Parameter to Set: The bandwidth parameter is the only parameter that needs to be set manually.</li> <li>Applicability to Non-linear Clustering: It can find non-linear clusters that other algorithms like K-means cannot find.</li> </ol>"},{"location":"ML/Clustering/MeanShift/#disadvantages-of-mean-shift-clustering","title":"Disadvantages of Mean Shift Clustering","text":"<ol> <li>Computationally Expensive: It can be slower than other clustering methods due to the computation of the mean for points within the window at each iteration.</li> <li>Bandwidth Selection: Choosing the right bandwidth is crucial, and it can be difficult to find the optimal value.</li> <li>Not Scalable: The algorithm can be inefficient for high-dimensional data or very large datasets.</li> <li>Ambiguity in Cluster Assignment: Points equidistant to multiple cluster centers can be ambiguously assigned.</li> </ol>"},{"location":"ML/Clustering/MeanShift/#images-to-explain-mean-shift-clustering","title":"Images to Explain Mean Shift Clustering","text":"<p>The image above shows how Mean Shift clustering iteratively shifts points (in blue) towards the region of the highest density (in red) to find cluster centers.</p>"},{"location":"ML/Clustering/MeanShift/#sample-code","title":"Sample Code","text":"<p>Here's a basic example of how to use Mean Shift clustering in Python with the <code>scikit-learn</code> library:</p> <pre><code>import numpy as np\nfrom sklearn.cluster import MeanShift\nimport matplotlib.pyplot as plt\n\n# Sample data\nX = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n\n# Mean Shift clustering\nms = MeanShift()\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\n# Plotting\nplt.scatter(X[:, 0], X[:, 1], c=labels, marker=\"o\")\nplt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker=\"x\")\nplt.title('Mean Shift Clustering')\nplt.show()\n</code></pre>"},{"location":"ML/Clustering/MeanShift/#scenarios-to-use-mean-shift-clustering","title":"Scenarios to Use Mean Shift Clustering","text":"<p>Mean Shift clustering is particularly useful in scenarios where:</p> <ol> <li>The number of clusters is not known a priori: Since Mean Shift does not require specifying the number of clusters, it is suitable for exploratory data analysis.</li> <li>Cluster shapes are irregular: The algorithm can handle clusters of different shapes and sizes, unlike K-means which is limited to spherical clusters.</li> <li>Robust clustering is required: In cases where the data might contain outliers, Mean Shift can provide a more robust clustering solution.</li> <li>Image processing tasks: Mean Shift is popular in computer vision for tasks such as image segmentation and tracking due to its ability to handle spatial data and find clusters of pixels.</li> </ol> <p>Remember, the performance and suitability of Mean Shift clustering will depend on the specific characteristics of the dataset and the computational resources available.</p>"},{"location":"ML/Clustering/OPTICS/","title":"Understanding OPTICS Clustering","text":"<p>OPTICS (Ordering Points To Identify the Clustering Structure) is an algorithm for finding density-based clusters in spatial data. It's similar to DBSCAN but addresses some of its shortcomings, such as the need to specify a global density threshold.</p>"},{"location":"ML/Clustering/OPTICS/#advantages-of-optics","title":"Advantages of OPTICS","text":"<ul> <li> <p>Deals with Varying Density: Unlike DBSCAN, OPTICS can identify clusters of varying density, as it does not require a global density threshold.</p> </li> <li> <p>Hierarchical Clustering: OPTICS creates a reachability plot, which can be used to extract a hierarchical clustering structure.</p> </li> <li> <p>Less Sensitive to Parameters: OPTICS requires two parameters, <code>min_samples</code> and <code>max_eps</code>, but it is less sensitive to the choice of <code>max_eps</code> compared to DBSCAN's <code>eps</code>.</p> </li> <li> <p>Outlier Detection: The algorithm can detect outliers as points that are not included in any cluster.</p> </li> </ul> <p> An example of a reachability plot generated by the OPTICS algorithm.</p>"},{"location":"ML/Clustering/OPTICS/#disadvantages-of-optics","title":"Disadvantages of OPTICS","text":"<ul> <li> <p>Higher Computational Complexity: OPTICS is computationally more expensive than DBSCAN, especially for large datasets.</p> </li> <li> <p>Complexity of Parameter Selection: Although less sensitive, choosing the correct <code>min_samples</code> can still be challenging.</p> </li> <li> <p>Interpretation of Results: The reachability plot and the extracted clustering structure can be difficult to interpret, especially for those unfamiliar with the algorithm.</p> </li> </ul>"},{"location":"ML/Clustering/OPTICS/#sample-code","title":"Sample Code","text":"<p>Here's an example of how to use the OPTICS clustering algorithm with Python's scikit-learn library:</p> <pre><code>from sklearn.cluster import OPTICS\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\nX = np.array([[1, 2], [2, 5], [3, 6], [8, 7], [8, 8], [7, 3]])\n\n# Run OPTICS\noptics = OPTICS(min_samples=2, max_eps=np.inf)\noptics.fit(X)\n\n# Reachability plot\nreachability = optics.reachability_[optics.ordering_]\nplt.figure(figsize=(10, 4))\nplt.bar(range(len(reachability)), reachability)\nplt.title('Reachability Plot')\nplt.xlabel('Points (ordered)')\nplt.ylabel('Reachability Distance')\nplt.show()\n\n# Plot clusters\nspace = np.arange(len(X))\nreachability = optics.reachability_[optics.ordering_]\nlabels = optics.labels_[optics.ordering_]\n\nplt.figure(figsize=(10, 7))\nG = space[labels == -1]\nR = space[labels != -1]\n\nplt.bar(G, reachability[G], color='r', label='Noise')\nplt.bar(R, reachability[R], color='b', label='Clustered')\nplt.legend()\nplt.title('OPTICS Clustering')\nplt.xlabel('Points (ordered)')\nplt.ylabel('Reachability Distance')\nplt.show()\n</code></pre>"},{"location":"ML/Clustering/OPTICS/#scenarios-for-using-optics","title":"Scenarios for Using OPTICS","text":"<ul> <li> <p>Spatial Data Analysis: When working with geographical or spatial data where clusters may have varying densities.</p> </li> <li> <p>Large Datasets: For larger datasets where the hierarchical structure may be of interest.</p> </li> <li> <p>Complex Data Structures: When the data contains complex structures and noise, making it difficult for algorithms like K-means or DBSCAN to find the true clusters.</p> </li> </ul> <p>Remember, OPTICS is best suited for situations where the density variation within the dataset is significant, and the user is interested in identifying a hierarchical clustering structure. It is less effective for high-dimensional data due to the \"curse of dimensionality,\" and it may not be the best choice for very large datasets due to its computational cost.</p>"},{"location":"ML/Clustering/Spectral/","title":"Spectral Clustering","text":"<p>Spectral clustering is a technique in machine learning that is used to group data points into distinct clusters. It is based on the spectrum (eigenvalues) of the similarity matrix of the data. Spectral clustering has become quite popular due to its simplicity and its ability to cluster data that is not linearly separable.</p> <p></p>"},{"location":"ML/Clustering/Spectral/#advantages-of-spectral-clustering","title":"Advantages of Spectral Clustering","text":"<ul> <li> <p>Flexibility in Cluster Shapes: Unlike K-means, spectral clustering can find clusters with irregular boundaries, as it relies on the connectivity of the points rather than distance measures.</p> </li> <li> <p>Similarity Measures: It allows for the use of custom similarity measures, which can be tailored to the specific dataset or problem at hand.</p> </li> <li> <p>Dimensionality Reduction: Spectral clustering can be viewed as a dimensionality reduction technique, which can be useful for visualization and may improve the performance of other algorithms when used as a preprocessing step.</p> </li> <li> <p>Global Information: It uses the global information of the dataset, which often results in better performance compared to algorithms that use local information, like K-means.</p> </li> </ul>"},{"location":"ML/Clustering/Spectral/#disadvantages-of-spectral-clustering","title":"Disadvantages of Spectral Clustering","text":"<ul> <li> <p>Scalability: It can be computationally expensive for large datasets because it requires the eigen decomposition of the similarity matrix, which is a dense matrix of size <code>n x n</code> where <code>n</code> is the number of data points.</p> </li> <li> <p>Sensitivity to Parameters: The results can be quite sensitive to the choice of the similarity graph (e.g., the scale of the Gaussian kernel) and the number of clusters specified.</p> </li> <li> <p>No Out-of-sample Extensions: Unlike K-means, spectral clustering does not provide an explicit function to assign new points to existing clusters.</p> </li> </ul>"},{"location":"ML/Clustering/Spectral/#sample-code","title":"Sample Code","text":"<p>Below is a sample Python code snippet that uses the <code>scikit-learn</code> library to perform spectral clustering:</p> <pre><code>import numpy as np\nfrom sklearn.cluster import SpectralClustering\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n\n# Apply Spectral Clustering\nsc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')\nlabels = sc.fit_predict(X)\n\n# Plotting the results\nplt.scatter(X[labels == 0, 0], X[labels == 0, 1], s=50, marker='o', color='red')\nplt.scatter(X[labels == 1, 0], X[labels == 1, 1], s=50, marker='s', color='blue')\nplt.title(\"Spectral Clustering\")\nplt.show()\n</code></pre> <p>In this code, we generate a dataset with two interleaving half circles (a typical non-linearly separable dataset) and then apply spectral clustering to it. The resulting plot shows how spectral clustering manages to separate the two groups effectively.</p>"},{"location":"ML/Clustering/Spectral/#scenarios-for-using-spectral-clustering","title":"Scenarios for Using Spectral Clustering","text":"<p>Spectral clustering is particularly useful in scenarios where the data is not linearly separable and clusters have irregular shapes. Some common scenarios include:</p> <ul> <li>Image segmentation, where you want to group pixels that have similar colors or textures.</li> <li>Social network analysis, where you want to find communities based on the connection patterns between individuals.</li> <li>Any domain where the similarity between data points can be better expressed with a graph structure rather than Euclidean distances.</li> </ul>"},{"location":"ML/Clustering/affinity-propagation/","title":"Affinity Propagation Clustering","text":"<p>Affinity Propagation (AP) is an unsupervised machine learning algorithm that is particularly useful for clustering data without prior knowledge of the number of clusters. It works by sending messages between pairs of samples until convergence. A high-quality set of exemplars (cluster centers) and corresponding clusters are then chosen based on these messages.</p>"},{"location":"ML/Clustering/affinity-propagation/#advantages-of-affinity-propagation","title":"Advantages of Affinity Propagation","text":"<ol> <li> <p>Automatic Selection of Cluster Centers: AP does not require the number of clusters to be determined or estimated before running the algorithm. It automatically selects the most representative data points as cluster centers.</p> </li> <li> <p>Flexibility: It can handle complex structures and is effective at finding clusters in a dataset with preference information.</p> </li> <li> <p>Quality of Clustering: Often produces higher-quality clustering results compared to other algorithms like K-means, especially when the number of clusters is not known beforehand.</p> </li> </ol> <p> An example of clustering using Affinity Propagation. The squares represent selected exemplars.</p>"},{"location":"ML/Clustering/affinity-propagation/#disadvantages-of-affinity-propagation","title":"Disadvantages of Affinity Propagation","text":"<ol> <li> <p>Computational Complexity: AP has a higher computational complexity than many other clustering algorithms, which can make it impractical for very large datasets.</p> </li> <li> <p>Memory Usage: The algorithm requires O(N\u00b2) memory for processing, where N is the number of samples, which can be prohibitive for large datasets.</p> </li> <li> <p>Sensitivity to Parameters: While it does not require the number of clusters, it is sensitive to the preference and damping parameters, which can affect the number and quality of the resulting clusters.</p> </li> </ol>"},{"location":"ML/Clustering/affinity-propagation/#sample-code-for-affinity-propagation","title":"Sample Code for Affinity Propagation","text":"<p>Here is a basic example of how to use the Affinity Propagation algorithm in Python with scikit-learn:</p> <pre><code>from sklearn.cluster import AffinityPropagation\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\n\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n\n# Compute Affinity Propagation\naf = AffinityPropagation(preference=-50).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\n\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.figure(figsize=(8, 6))\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()\n</code></pre>"},{"location":"ML/Clustering/affinity-propagation/#scenarios-to-use-affinity-propagation","title":"Scenarios to Use Affinity Propagation","text":"<ol> <li> <p>Small to Medium Datasets: Due to its computational and memory demands, AP is best suited for small to medium-sized datasets.</p> </li> <li> <p>Unknown Number of Clusters: AP is ideal in scenarios where the number of clusters is not known beforehand.</p> </li> <li> <p>Preference Information Available: If there is information available that can be used to inform the preference of each data point to be chosen as an exemplar, AP can leverage this to improve clustering.</p> </li> <li> <p>Quality over Speed: In cases where the quality of the clustering result is more important than the speed or computational resources, AP can be a good choice.</p> </li> </ol> <p>Affinity Propagation can be a powerful tool for data analysis, but it is important to consider its computational demands and ensure that the parameters are tuned appropriately for the dataset at hand.</p>"},{"location":"ML/Clustering/agglomerative/","title":"Agglomerative Clustering","text":"<p>Agglomerative Clustering is a type of hierarchical clustering method that builds a hierarchy of clusters. This type of clustering makes use of the bottom-up approach where each data point starts in its own cluster and clusters are successively merged together.</p> <p></p> <p>Image Source: DataCamp</p>"},{"location":"ML/Clustering/agglomerative/#advantages-of-agglomerative-clustering","title":"Advantages of Agglomerative Clustering","text":"<ol> <li>Ideal for utilizing non-flat geometry data: Agglomerative clustering is efficient in handling non-elliptical shapes.</li> <li>You can visualize the data structure: With Hierarchical clustering, you can visualize the grouping more effectively using Dendrograms.</li> <li>Greater flexibility: It allows different distance metrics and different linkage criteria.</li> </ol>"},{"location":"ML/Clustering/agglomerative/#disadvantages-of-agglomerative-clustering","title":"Disadvantages of Agglomerative Clustering","text":"<ol> <li>Can\u2019t handle large datasets: Like K-Means, Agglomerative Clustering also is not suitable for large datasets.</li> <li>Can\u2019t undo the previous step: Once the instances have been assigned to the clusters, moving them is not possible.</li> </ol>"},{"location":"ML/Clustering/agglomerative/#sample-code","title":"Sample Code","text":"<p>Here is a simple implementation of Agglomerative Clustering using Sci-kit learn library in Python:</p> <pre><code>from sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\n\n# standardize data\nscaler = StandardScaler()\ndata = scaler.fit_transform(data)\n\n# Agglomerative clustering\nmodel = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\nmodel.fit(data)\nlabels = model.labels_\n</code></pre> <p>This code will calculate clusters of data array into 3 clusters with Euclidean distance measure.</p>"},{"location":"ML/Clustering/agglomerative/#use-scenario","title":"Use scenario","text":"<p>Agglomerative Clustering is used in wide variety of fields like:</p> <ol> <li>Organizing Business Hierarchies: Businesses can group their resources and perform heirarchical level data analysis.</li> <li>Sentence Clustering in NLP: In Natural Language Processing, we often need to represent meaning of sentences/ paragraphs in a collection. Using use agglomerative clustering, you can group similar sentences together.</li> <li>Classification of Returned Queries: In search engines, returned queries need to be grouped into different categories. This can be achieved using agglomerative clustering.</li> </ol>"},{"location":"ML/Clustering/birch/","title":"BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) Cluster","text":"<p>BIRCH is a very efficient clustering algorithm when having to deal with large datasets.</p> <p></p>"},{"location":"ML/Clustering/birch/#advantages-of-birch-algorithm","title":"Advantages of BIRCH algorithm:","text":"<ol> <li> <p>Livestream data clustering: A BIRCH clustering algorithm can process data in a single run, without any need to keep the entirety of the data in memory.</p> </li> <li> <p>Memory Management: The BIRCH algorithm uses its CF-Tree structure to store summary information and discards the exact details of the dataset, making it much more memory efficient than other clustering algorithms such as K-means or DBSCAN.</p> </li> <li> <p>Outliers handling: BIRCH can iteratively eliminate outliers and refine the clusters, making it robust in the presence of noise and outliers.</p> </li> </ol>"},{"location":"ML/Clustering/birch/#disadvantages-of-birch-algorithm","title":"Disadvantages of BIRCH algorithm:","text":"<ol> <li> <p>Distance Metric: BIRCH algorithm only uses Euclidean distance and does not support other distance metrics, which makes this algorithm less flexible.</p> </li> <li> <p>High-dimensional data: BIRCH algorithm does not perform well on high-dimensional data.</p> </li> <li> <p>Lack of density representation: Unlike DBSCAN, BIRCH algorithm does not represent density in their clusters,which can be problematic in some scenarios.</p> </li> </ol>"},{"location":"ML/Clustering/birch/#sample-code","title":"Sample Code","text":"<p>Here is a snippet of Python code showing how the BIRCH algorithm can be implemented using Sklearn library.</p> <pre><code>from sklearn.cluster import Birch\nfrom sklearn.datasets import make_blobs\n\n# generate sample data\nX, y = make_blobs(n_samples=1000, centers=5, cluster_std=0.6, random_state=0)\nbirch_model = Birch(branching_factor=50, n_clusters=None, threshold=1.5)\n\n# train model\nbirch_model.fit(X)\n\n# predict clusters\npredictions = birch_model.predict(X)\n</code></pre>"},{"location":"ML/Clustering/birch/#scenario-when-to-use-birch","title":"Scenario When to Use BIRCH","text":"<p>Because of its memory efficiency and ability to handle large amounts of data, BIRCH is best suited for large datasets where the instances have many attributes. It's also an optimal choice when the data is continuously incoming, as BIRCH can incrementally process the data without requiring the whole dataset to be stored in memory.</p>"},{"location":"ML/DeepLearning/","title":"Neural Networks","text":"<p>Neural Networks are the basis of most commonly used, and hyped machine learning algorithms. Deep Learning, NLP and many other branches have stemmed from them and there's a great deal of content to be learnt about them. </p>"},{"location":"ML/DeepLearning/#key-terms","title":"Key Terms","text":""},{"location":"ML/DeepLearning/#neurons","title":"Neurons","text":"<p>It is the basic building block of any neural network. The connection between two neurons is called a Synapse and it is where the signal is being passed. In a real world neuron (as shown in the figure below) the receptors are called dendrites and the transmitter is called the axon. Usually there is many dendrites and just one axon per neuron which makes sense, because a neuron should be able to receive a lot of information but the output from it should be clear and unambiguous. </p> <p></p> <p>Every neuron is composed of two parts:</p> <ol> <li>Summation Function: In a neural network, the summation operator sums all of a node's inputs to create a net input. It is mathematically represented as  </li> <li> <p>Activation Function: When a set of inputs is added into one (by the summation function) and the final input that is generated, it is processed using an activation function in order to be passed out as output from the neuron. This is the second component of any neuron in the hidden or output layer. The following list contains the names, and brief introductions of the predominant Activation Functions, used in the industry:</p> <p>(i) Threshold Function: The threshold function is the most basic activation function in practice. It basically says that if the value provided by summation function is less than a given threshold then return 0, else return 1. </p> <p>(ii) Sigmoid Function: This function is a bit more interesting than the one above. It is given by the equation </p> <p></p> <p>and can be plotted as</p> <p></p> <p>In the graph above, x is the sum of inputs. It is a smooth function and has a gradual progression hence making it useful in predicting probabilities. </p> <p>(iii) Rectifier Function: Another very popular activation function used for neural networks is the rectifier function. It returns 0 up to a certain threshold and then starts returning the original value obtained from the input function. It is shown in the image below</p> <p></p> <p>(iv) Hyperbolic Tangent Function: This function is similar to the Sigmoid function but the major difference is that it goes from -1 to 1. It is given by the equation </p> <p></p> <p>The graph of this function is:</p> <p></p> </li> </ol> <p>There are many other kinds of activation functions and probably, in due time, we might add a separate section dedicated to cover each one of those in detail and their possible applications, but these four are the most popular ones and necessary to start with. </p>"},{"location":"ML/DeepLearning/#layers","title":"Layers","text":"<p>There are three kinds of layers in any neural network:</p> <ol> <li>Input Layer: This is the layer that is responsible for receiving the input observations one at a time. It is imperative (or normalize, subject to use case) that the input values are standardized before they are passed into the neural network. </li> <li>Hidden Layer: This layer is responsible for all the heavylifting and is the matrix that is configured through our various observations. </li> <li>Output Layer: This is the neuron layer that gives whatever output is required, as per our definition. If it is a regression problem then the output will have only one neuron, while in case of a classification problem, we will have as many neurons in the output layer, as are the number of classes. </li> </ol> <p>You can see the structure of all available types of neural networks, here.</p> <p>Any hidden neuron, in the context of machine learning, takes its input from other neurons which are at the receiving end of data, i.e. sensors. The outputs from these input layer neurons serve as the input for the hidden layer neurons. </p>"},{"location":"ML/DeepLearning/#synapses","title":"Synapses","text":"<p>Weights or Synapses are very important in neural networks as they are the controlling elements of the information that is being passed along. They basically control to what extent does a signal gets passed along?</p>"},{"location":"ML/DeepLearning/#working-procedure","title":"Working Procedure","text":"<p>The way that Neural Networks work can be seen in the image below</p> <p></p> <p>This allows us to have a detailed and complete understanding of the Neural Network's individual units. </p> <ol> <li>The yellow circles constitute the input layer, i.e. the layers that will be recieving the observations.</li> <li>The yellow arrows show the trajectory of every observation in our dataset</li> <li>w1, w2 and w3 are the weights that we are trying to find. </li> <li></li> </ol> <p>The weights are the only things that we intend to find/learn in a neural network</p>"},{"location":"ML/DeepLearning/#research-references","title":"RESEARCH REFERENCES","text":"<ol> <li> <p>Deep Sparse Rectifier neural networks; Xavier Glorot et al.</p> <p>[Solved] The explanation for why the rectifier function is such an important asset for neural networks and often perform better over hyperbolic tangent activation functions.(Activation Functions.Rectifier Function)</p> </li> </ol>"},{"location":"ML/Dimensionality_Reduction/","title":"Dimensionality Reduction","text":"<p>There are two ways to reduce the dimensions of any data set. Either you select some features and discard the others or you create some new features that allow you concisely represent the same amount of information as your existing features.</p> Feature Selection Feature Extraction Backward Elimination PCA Forward Selection LDA Bidirectional Elimination Kernel PCA Score Comparison <p>The things that we will cover in this section are as follows:</p> <ol> <li>Principal Component Analysis</li> <li>LDA</li> </ol>"},{"location":"ML/Dimensionality_Reduction/PrincipalComponentAnalysis/","title":"Principal Component Analysis","text":"<p>Principal Component Analysis, essentially takes as input, the original dimensions, i.e. the m independent variables originally defined and then produces as output p new variables or dimensions. These are new extracted independent variables are the ones that explain the most variance of the dataset regardless of the dependent variables. The fact that we consider PCA as an unsupervised learning model, is the fact that it does not consider the dependent variable in the final model that it builds. </p>"},{"location":"ML/Dimensionality_Reduction/PrincipalComponentAnalysis/#functioning","title":"Functioning","text":"<p>The algorithm works on the basis of vector-matrix multiplication of the form  where  is a matrix and  is a vector. </p> <p>scalar * vector = another vector, same direction</p> <p>matrix * vector = another vector, possibly different direction</p> <p>So we could say, that PCA rotates the original input vectors. </p>"},{"location":"ML/Dimensionality_Reduction/PrincipalComponentAnalysis/#advantages","title":"Advantages","text":"<p>There are various benefits to using PCA:</p> <ol> <li>Decorrelates all the input data.</li> <li>Data is ordered by information content after the transformation</li> <li>Dimensionality Reduction (duh!)</li> </ol> <p>Removing data does not always mean that the prediction ability will go down. In many cases the data is too noisy and dimensionality reduction proves to be an essential tool to smooth the data and generalize it. </p>"},{"location":"ML/Dimensionality_Reduction/PrincipalComponentAnalysis/#research-references","title":"RESEARCH REFERENCES","text":"<ul> <li> <p>Principal Component Analysis and Factor Analysis </p> <p>[Explains] The difference between Principle Component Analysis and Factor Analysis.</p> </li> </ul>"},{"location":"ML/Natural_Language_Processing/","title":"Natural Language Processing","text":""},{"location":"ML/Natural_Language_Processing/#models","title":"Models","text":""},{"location":"ML/Natural_Language_Processing/#bag-of-words-model","title":"Bag of Words Model","text":""},{"location":"ML/Natural_Language_Processing/#preprocessing","title":"Preprocessing","text":"<p>Usually this would involve the following steps:</p> <ol> <li>Removal of special characters</li> <li>Change case for all words to lowercase</li> <li>Do stemming and remove the words that don't matter</li> <li>Do tokenization by using the following procedure</li> </ol>"},{"location":"ML/Natural_Language_Processing/#procedure","title":"Procedure","text":"<p>This is the most basic model for Natural Language Processing. It works as follows:</p> <ol> <li>Select all distinct words and set them as columns of a matrix</li> <li>Create as many rows as the number of observations</li> <li>Each cell should now contain for each observation, the number of times the word appeared in it</li> <li>Filter out other non-relevant words by setting a minimum frequency that a word must have across all observations in order for it to stay, </li> <li>Train this matrix for the classes available in a similar fashion to classification.</li> </ol> <ul> <li> <p>The matrix created in Step 3 in the example above is called the Term Document Matrix</p> </li> <li> <p>The commonly used models for classification on NLP based problems are Naive Bayes, Decision Tree Learning &amp; Random Forest classification but others maybe used as well if they fit the data well.</p> </li> </ul>"},{"location":"ML/Natural_Language_Processing/#types-of-bow-models","title":"Types of BoW Models","text":"<ol> <li> <p>Word Frequency Proportions : The proportions of each word with respect to all the words in the document. It is usually given by the equation:</p> <p></p> </li> <li> <p>Raw Word count: The raw word count for every word is taken as such.</p> </li> <li>Binary: if a word appears then 1, else 0</li> <li>TF-IDF: Term Frequency - Inverse Document Frequency (takes into account the words that appear.</li> </ol>"},{"location":"ML/Natural_Language_Processing/#use-cases","title":"Use Cases","text":""},{"location":"ML/Natural_Language_Processing/#parts-of-speech-tagging","title":"Parts of Speech Tagging","text":"<p>This is the segment of NLP that deals with tagging various elements of a sentence with the parts of speech group that they belong to.</p>"},{"location":"ML/Natural_Language_Processing/#name-entity-recognition","title":"Name-Entity Recognition","text":"<p>In the sentence <code>Albert Einstein is a genius</code>, the ability of an algorithm to decipher that Abert Einstein is a person, is called NER, and is another application of NLP. The results are provided in form parse tree.</p>"},{"location":"ML/Natural_Language_Processing/#latent-semantic-analysis","title":"Latent Semantic Analysis","text":"<p>There are often more advanced problems that researchers of NLP face, like synonymy(multiple words having the same meaning) and polysemy(one word having multiple meanings). A common technique used for this is the creation of Latent Variables.</p>"},{"location":"ML/Natural_Language_Processing/#article-spinning","title":"Article Spinning","text":"<p>This is the art of changing certain aspects of a particular article that it appears to be a different one and avoids plagiarism. This algorithm works on the principal of Bayesian ML. A popular technique that we use for this is called Trigram Model. It models each word in the document as </p> <p></p> <p>Then, we replace each word in the document with a certain probability, P. If we change literally every word then there will be high chance for the document to make no sense at all. Therefore it's essential that we only change some words and let others be. </p> <p>Both LSA and Trigram Models are unsupervised algorithms(have no class labels) and tend to learn the structure of our data.</p>"},{"location":"ML/Natural_Language_Processing/#jargon","title":"Jargon","text":""},{"location":"ML/Natural_Language_Processing/#tf-idf-term-frequency-inverse-document-frequency","title":"TF-IDF: Term Frequency - Inverse Document Frequency","text":"<p>Taking into account the words that appear most frequently in many documents and therefore neglecting words like is, an, the etcetera. </p>"},{"location":"ML/Natural_Language_Processing/#tokenization","title":"Tokenization","text":"<p>Split all the different sentences into different words, each word gets a column which would contain the frequency of appearance of that word. This would be a sparse matrix that we would then operate upon it at will.</p>"},{"location":"ML/Natural_Language_Processing/#stemming-lemmatization","title":"Stemming &amp; Lemmatization","text":"<p>The process of collecting only the roots of words so that even if the same words appear in different forms, we always have a steady output and our machine learning models learn to recognize them properly and as the same words. While stemming is the more basic/crude version of the above, Lemmatization is more sophisticated than that. While a stemmer might give you the word <code>theiv</code> after stemming the word <code>theives</code>, the lemmatizer will give you <code>theif</code> as the answer. </p>"},{"location":"ML/Natural_Language_Processing/#latent-variables","title":"Latent Variables","text":"<p>The idea to combine words that often appear together using a probabilistic distribution over the terms. After this, these pairs of variables with very correlation will then be used to revamp the data accordingly, and hopefully the dimensions of this new data will be much lesser than the original one. Although, this would almost certainly help solve the Synonymy problem but it's not quite proven whether or not it helps solve the polysemy problem adn I think, ideally, it shouldn't. </p>"},{"location":"ML/Natural_Language_Processing/#corpus","title":"Corpus","text":"<p>A corpus is a collection of texts of the same type.</p>"},{"location":"ML/Natural_Language_Processing/#sparsity","title":"Sparsity","text":"<p>The property that a matrix holding a lot of zeroes and very few values, is called sparsity. This is usually not a good thing and can be reduced, either by removing the least frequent words by using the <code>max_features</code> parameter of <code>CountVectorizer</code> method (in Python) or using Dimensionality Reduction techniques. </p>"},{"location":"ML/Natural_Language_Processing/#code-references","title":"CODE REFERENCES","text":"<p>[1] CountVectorizer: It can be used to all the text cleaning necessary for the Bag of Words model, instead of having do to it manually. Read documentation for more.</p>"},{"location":"ML/Prerequisites/","title":"Prerequisites","text":"<ol> <li>Linear Algebra</li> <li>Statistics</li> <li>Types of Data</li> </ol>"},{"location":"ML/Prerequisites/LinearAlgebra/","title":"Linear Algebra","text":""},{"location":"ML/Prerequisites/LinearAlgebra/#linear-equations","title":"Linear Equations","text":""},{"location":"ML/Prerequisites/LinearAlgebra/#structure","title":"Structure","text":"<p>One of the most basic motivations towards making computers was solving a system linear equations given by  </p> <p>where , , ...  are unknowns and  's and  's  denote the constants, either complex or real. </p>"},{"location":"ML/Prerequisites/LinearAlgebra/#definitions","title":"Definitions","text":"<p>A system  is called a solution of the above system of linear equations if  satisfies each of the eqations in our system of linear equations simultaneously and when  , then the system of equations is called to be homogeneous. Such system of equations have one certain solution  = ,  = , ...  =  called the trivial solution. A system of Linear equations is said to be consistent if it has at least one solution and inconsistent if it has no solutions.</p>"},{"location":"ML/Prerequisites/LinearAlgebra/#vectors","title":"Vectors","text":"<p>A vector is a numeric measurement with directions. In two dimensions, a vector  is of the form  and the magnitude of this vector is given by </p> <p> </p>"},{"location":"ML/Prerequisites/LinearAlgebra/#vector-multiplications","title":"Vector Multiplications","text":""},{"location":"ML/Prerequisites/LinearAlgebra/#dot-product","title":"Dot Product","text":"<p>There are two ways to go about this:</p> <ol> <li> <p></p> <p>This is the summation of element wise multiplication of the two vectors. The notation  denotes that the vectors are column vectors and the result of the equation above would be a 1x1 vector which is a scalar quantity. </p> </li> <li> <p></p> </li> </ol> <p>This notation is not very convenient for vector multiplication unless a the angle on the right hand side is known to us. Although, it is a much more common practice to use this equation for finding out the angle between two vectors using </p> <p></p>"},{"location":"ML/Prerequisites/LinearAlgebra/#outer-product","title":"Outer Product","text":"<p>The outer product of two vectors results in a matrix and is given by the equation:</p> <p>If there are two column vectors <code>u1</code> and <code>v1</code> that are given by  and  respectively. Then their outer product is written as </p> <p>which then results to be .</p>"},{"location":"ML/Prerequisites/LinearAlgebra/#matrices","title":"Matrices","text":"<p>Matrices are two dimensional set of numbers. These are very efficient data types for quick computations.</p>"},{"location":"ML/Prerequisites/LinearAlgebra/#matrix-multiplications","title":"Matrix Multiplications","text":""},{"location":"ML/Prerequisites/LinearAlgebra/#dot-product_1","title":"Dot Product","text":"<p>The product of two matrices A and B in given by the formulae </p> <p></p>"},{"location":"ML/Prerequisites/Statistics/","title":"Statistics","text":""},{"location":"ML/Prerequisites/Statistics/#basics","title":"Basics","text":"<p>Dataset: Any group of values retrieved through a common method/procedure of collection.</p> <p>Weighted Mean: Weighted mean of a group of numbers with weights given in percentages and scores given in the same range is given by:</p> <p></p> <p>Note: Always question about how the weights and categories were collected and why and to what extent it holds importance. </p> <p>Standard Deviation: A quantity expressing by how much the members of a group differ from the mean value for the group. In more formal terms, it's the average distance between a datapoint and the mean of the dataset and is given by the equation:</p> <p></p> <p>z-score: The z-score is the distance in standard deviation units for any given observation:</p> <p></p> <p>Empirical Rule or Three-Sigma Rule: Most of the data points fall within three standard deviations, with 68% would lie within 1 sd, 95% within 2 sd and 99.7% would fall within 3 sd.</p> <p></p> <p>Note: It only works for symmetrically distributed data</p> <p>Percentile Score: The percentile score for any value x in the dataset is given by:</p> <p></p>"},{"location":"ML/Prerequisites/Statistics/#probabilities","title":"Probabilities","text":"<p>Event: An event is a set of probabilities e.g. When rolling a dice, an event A = (4,2,6) represents the event that an even number will appear.</p> <p>Sample Space: All possible outcomes for a particular random experiment constitute it's sample space.</p> <p>Probability: The number of favorable outcomes divided by the number of all possible outcomes in a given situation. </p>"},{"location":"ML/Prerequisites/Statistics/#types-of-probabilities","title":"Types of Probabilities","text":"<ol> <li> <p>Classical Probability: The flipping of a coin is an example of classical probability because it is known that there are two sides to it and the likelihood of any one of them turning up is 50%. Objective Probabilities are based on calculations and classical probability is a type of Objective Probability.</p> </li> <li> <p>Empirical Probability: The probabilities are the ones that are based on previously known data. For instance, Messi scoring more than ten goals in this season of FIFA is an example of Empirical probability because it is calculated on the basis of Messi's previous record. This too is an example of Objective Probability since it's also based on calculations.</p> </li> <li> <p>Subjective Probability: These probabilities are not based on mathematical calculations and people use their opinions, their experiences to assert their viewpoints with some amount of relevant data that makes their own point stronger. </p> </li> </ol> <p>Addition Rule: The addition rule in probabilities is a rule that ensures that an event is not counted twice when calculating probabilities (where P(overlap) is the probability of both E1 and E2 occurring:</p> <p></p> <p>Conditional Probability: The probability of occurrence of an event given that some other event has already occurred. </p> <p>Independent Events: If the probability of two events are completely unrelated. If we can prove that the probability of two occurring together is equal to the product of their individual probabilities, then we can say that the two events are independent. </p> <p>Random Variable: The result of an experiment that has random outcomes is called a random variable. They can be of two types:</p> <ol> <li>Discrete RV: The number of drinks a person will order at Tank is an example of discrete random variable because it has to be a whole number.</li> <li>Continuous RV: The waiting time in line before one can order at a Burger King is an example of continuous variable because there are no fixed values that could be outcomes. The possibilities are infinite and continuous.</li> <li>Binomial RV: When an event has only two possible outcomes, the result is called a Binomial Random Variable.</li> </ol> <p>Probability Density: The curves that represent the distribution of probabilities are called Probability Density curves.</p>"},{"location":"ML/Prerequisites/Statistics/#sampling","title":"Sampling","text":"<p>There are a few conditions that need to be considered before sampling is done from various sources:</p> <ol> <li>Size to Cost ratio: The appropriate size of the sample based upon the cost per data point in the sample </li> <li>Inherent Bias: If any bias was knowingly/unknowingly introduced while creating the sample, it will need to be considered!</li> <li>Quality of Sample</li> </ol> <p>A simple random sample is the gold standard when collecting samples. This means, that any given point during the sample selection process, any individual has the same probability of being chosen as any other individual.</p> <p>Some alternative sampling methods are:</p> <ol> <li>kth point: The first data point is selected and then every kth data point is selected in this method.</li> <li>Opportunity Sampling: The first n values are selected from the total data.</li> <li>Stratified Sampling: The whole sample is broken out into homogenous groups. Then we select few samples from each strata.</li> <li>Cluster Sampling:  The whole sample is collected from heterogenous groups with data points with different characteristics. Then we select few samples from each group. </li> </ol>"},{"location":"ML/Prerequisites/Statistics/#confidence-intervals","title":"Confidence Intervals","text":"<p>As the name suggests, the confidence intervals present a level of confidence for a given interval.</p>"},{"location":"ML/Prerequisites/Statistics/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>The process to be able to test a hypothesis that has been presented. </p>"},{"location":"ML/Prerequisites/Statistics/#visualization-tips","title":"Visualization Tips","text":"<ol> <li>Tables: Useful for detailed recording and sharing of actual data</li> <li>Frequency Table: Displays the frequency of each observation in the data set</li> <li>Dot Plots: When you want to convey information that is discrete and individual to each observation.</li> <li>Histograms: When you want to convey frequencies of grouped bins, this could be useful.</li> <li>Pie Charts: Relative Frequency distributions are best represented with Pie Charts. They are also useful for representing distributions among qualitative variables where histograms wouldn't be a very good measure.</li> </ol>"},{"location":"ML/Prerequisites/TypesOfData/","title":"Types of Data","text":""},{"location":"ML/Prerequisites/TypesOfData/#cross-sectional-data","title":"Cross-Sectional Data","text":"<p>It consists of variables, that are either quantitative or qualitative for a lot of different observations (usually called 'cases') and are taken from some defined population. All the cases are registered at a single point in time, or a reasonably short period of time. The techniques commonly used for this kind of data are 't-tests' analysis of variance or regression depending on the kind and number of variables that are there in the data. It is noteworthy that each observation of a given variable or each set of variables are independent of every other observation in the dataset. The independence of the variables is a critical assumption when modelling cross-sectional data. </p> <p>It is called cross-sectional data because we are measuring a cross-section of a defined population at a particular point in time or a very short period in time.</p>"},{"location":"ML/Prerequisites/TypesOfData/#time-series-data","title":"Time Series Data","text":"<p>If measurement on variables are taken over or through time. Every variable in a time series dataset is measured at equally spaced time intervals. Usually, the observations are not independent of each other in this case. Time series data can be classified into two types other thank the univariate and multivariate distinction and they are discussed in the time series chapter.</p>"},{"location":"ML/Regression/","title":"Regression","text":""},{"location":"ML/Regression/#introduction","title":"Introduction","text":"<p>The idea of looking at a lot of data samples and trying to predict the dependent variable in a continuous numeric domain is called regression in statistical terms.</p>"},{"location":"ML/Regression/#assumptions","title":"Assumptions","text":"<p>In order to perform regression on any dataset, it must satisfy the following assumptions:</p> <ol> <li>Normality: The erros are assumed to be normally distributed</li> <li>Independent: The errors must be independent of each other</li> <li>Mean and Variance: They must have zero mean and constant variance (this property of having a constant variance is also called homoscedasticity</li> </ol> <p>These assumptions are usually verified using Q-Q Plots, S-W test etc.</p> <p>This chapter offers introduction to various kind of regressions and their use cases.</p> <ol> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Polynomial Regression</li> <li>Stepwise Regression</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>ElasticNet Regression</li> <li>Support Vector Regression</li> <li>Decision Tree Regression</li> <li>Random Forest Regression</li> <li>Gradient Boosting &amp; AdaBoost</li> <li>XGBoost Regression</li> <li>Bayesian Linear Regression</li> <li>Generalized Linear Model (GLM)</li> <li>Poisson Regression</li> <li>Negative Binomial Regression</li> <li>Cox Regression</li> <li>Multivariate Adaptive Regression Splines (MARS)</li> <li>Robust Regression</li> <li>Principal Components Regression (PCR)</li> <li>Partial Least Squares (PLS) Regression</li> <li>Tweedie Regression</li> <li>Quantile Regression</li> <li>Neural Network Regression</li> <li>SVR (Support Vector Regression)</li> <li>Stochastic Gradient Descent Regression</li> <li>k-Nearest Neighbors Regression</li> <li>LightGBM Regression</li> <li>CatBoost Regression</li> </ol>"},{"location":"ML/Regression/#article-references","title":"ARTICLE REFERENCES","text":"<ol> <li>Q-Q Plot</li> </ol>"},{"location":"ML/Regression/LinearRegression/","title":"Linear Regression","text":"<p>The idea is to fit a straight line in the n-dimensional space that holds all our observational points. This would constitute forming an equation of the form y = mx + c. Because we have multiple variables, we might need to extend this mx to be m<sub>1</sub>x<sub>1</sub>, m<sub>2</sub>x<sub>2</sub> and so on. This extensions results in the following mathematical representation between the independent and dependent variables:</p> <p></p> <p>where </p> <ol> <li>y = dependent variable/outcome</li> <li>x<sub>1</sub> to x<sub>n</sub> are the dependent variables</li> <li>b<sub>0</sub> to b<sub>n</sub> are the coefficients of the linear model</li> </ol> <p>A linear regression models bases itself on the following assumptions:</p> <ol> <li>Linearity</li> <li>Homoscedasticity</li> <li>Multivariate normality</li> <li>Independence of errors</li> <li>Lack of multicollinearity</li> </ol> <p>If these assumptions do not hold, then linear regression probably isn't the model for your problem. </p>"},{"location":"ML/Regression/LinearRegression/#variable-selection","title":"Variable Selection","text":"<p>The dataset will, more often than not, contain columns that do not have any effect on the dependent variable. This becomes problematic because we don't want to add too much noise to the dataset. The variables that do not effect the dependent variable (the outcome) will usually only decrease the performance of our model, and even if they do not, there's always an additional computing complexity that comes along with them. This could influence the costs of building the model, especially when we want to do it iteratively. </p> <p>There are five ways for doing feature selection while building a model:</p> <ol> <li>All-in: If we are aware that all the variables in the dataset are useful and are required for building the model, then we simply use all the variables that are available to us.</li> <li>Backward Elimination: The following steps need to be taken in order to conduct a backward elimination feature selection:<ol> <li>We set a significance level for a feature to stay in the model</li> <li>Fit the model with all possible predictors</li> <li>Consider the predictor with the highest p-value. If P &gt; SL go to Step 4, otherwise go to Step 6</li> <li>Remove the variable</li> <li>Fit the model again, and move to Step 3</li> <li>Finish</li> </ol> </li> <li>Forward Selection: Although, it may seem like a straightforward procedure, it is quite intricate in practical implementation. The following steps need to be performed in order to make a linear regressor using forward selection:<ol> <li>We set a significance level to enter the model</li> <li>Fit regression models with each one of those independent variables and then select the one with the lowest p-value</li> <li>Fit all possible regression models with the one that we selected in previous step and one additional variable.  </li> <li>Consider the model with the lowest p-value. If p &lt; SL go to Step 3 otherwise go to Step 5</li> <li>Finish and select the second last model </li> </ol> </li> <li>Bidirectional Elimination: The algorithm works as follows:<ol> <li>Select a Significance Level to enter and a Significance Level to stay in the model, viz. SLENTER and SLSTAY</li> <li>Perform the next step of Forward Selection, i.e. all the new variables that are to be added must have p &lt; SLENTER in order to enter the model</li> <li>Perform all the steps of Backward Elimination, i.e. all the existing variables must have p &lt; SLSTAY in order to stay in the model</li> <li>No new variables can be added, and no new variables can be removed</li> </ol> </li> </ol> <p>The details on variable selection by Score Comparison is yet to be found.</p> <p>The lower the p-value is, the more important a particular variable is for our model.</p> <p>The term 'Step wise regression' is often used for 2, 3, and 4 but sometimes, it refers only to 4, depending on context. </p> <p>Dummy variables: The variables that are created when categorical variables are encoded, are called dummy variables. </p> <p>We usually use one-hot encoding to do this, and it might seem like not including one last categorical dummy variable would cause a positive bias towards the rest of the equation but this is not the case. The coefficient for the last dummy variable is included in the b<sub>0</sub> term of the equation.</p> <p>Dummy Variable trap: One can never have all the dummy variables and b<sub>0</sub> in a model at the same time. We need to remove at least one dummy variable for each of the corresponding categorical variables because all of that will be modeled into b<sub>0</sub>. </p>"},{"location":"ML/Regression/LinearRegression/#measure-of-accuracy","title":"Measure of Accuracy","text":""},{"location":"ML/Regression/LinearRegression/#mean-squared-error","title":"Mean Squared Error","text":"<p>The root mean squared error in a linear regression problem is given by the equation  which is the sum of squared differences between the actual value  and the predicted value  for each of the rows in the dataset (index iterated over <code>i</code>).</p>"},{"location":"ML/Regression/LinearRegression/#intuition-univariate-linear-regression","title":"Intuition (Univariate Linear Regression)","text":""},{"location":"ML/Regression/LinearRegression/#minimizing-the-error-term-we-have-above","title":"Minimizing the error term we have above","text":"<p>We do so by going through the following steps: </p> <ol> <li> <p>We write the equation  again but we replace  with the equation of the line that we are to predict. Let's say  where we don't know the values of  and  yet. </p> <p>With this, our updated equation or the error term becomes . </p> </li> <li> <p>We now need to minimize the error term  with respect to  and  both. For this we use calculus method of partial derivatives. </p> </li> <li> <p>We calculate partial derivative of  w.r.t.  and that can be written as:</p> <p> <code>-1</code></p> <p>Now we calculate partial derivate of the same equation w.r.t.  and that can be written and simplified as:</p> <p> <code>-2</code></p> </li> <li> <p>In order to minimize we equate these equations to zero and solve the equations:</p> <p>By solving <code>Eq 1</code>, we get </p> <p> <code>-3</code></p> <p>By solving <code>Eq 2</code>, we get </p> <p> <code>-4</code></p> </li> <li> <p>Now we have two equations <code>3</code> and <code>4</code>. We can use these to solve for  and , upon doing so we get the following values:</p> <p> <code>-5</code></p> <p> <code>-6</code></p> </li> <li> <p>Now that we have  these equations, we can divide boh tops and bottoms by N, so that all our summation terms can be turned into means. For instance . We can divide the equations <code>5</code> and <code>6</code> with  to get the following results:</p> <p>, </p> <p></p> </li> </ol>"},{"location":"ML/Regression/LinearRegression/#intuition-multivariate-linear-regression","title":"Intuition (Multivariate Linear Regression)","text":""},{"location":"ML/Regression/LinearRegression/#base-equation","title":"Base equation","text":"<p>A multivariate Linear Regression can be represented as </p> <p></p> <p>where  is the list of predictions,  is the vector of weights for each variable,  is the set of parameters </p>"},{"location":"ML/Regression/LinearRegression/#code-references","title":"CODE REFERENCES","text":"<p>The function used for backward elimination in such models is from the class <code>statsmodels.formula.api</code> and is titled OLS (used as sm.OLS). The important thing to remember about this function is the fact that it requires the users to add a column of 1s in the matrix of features (at the beginning of the matrix) explicitly, to be a partner for the b<sub>0</sub> coefficient. </p>"},{"location":"ML/Regression/RegressionModelsComparison/","title":"Performance Evaluation","text":""},{"location":"ML/Regression/RegressionModelsComparison/#r-squared","title":"R-Squared","text":"<p>There are a few terms that need to be understood before we can calculate the R-squared value for any regression model and are as follows:</p> <ol> <li> <p>Sum of squared residuals  : It is exactly what the name suggests. A residual is the difference between actual values and the predicted values. If we add the squares of all residuals, that would give us the value of , shown as follows in mathematical terms:     </p> </li> <li> <p>Total sum of squares :  Instead of calculating the residuals, we calculate the difference between the average y values and the actual y values present in the dataset. We then add the squares of these values, represented mathematically as :     </p> </li> </ol> <p>R-sqaure is given by the following formula  </p> <p>The idea is to minimize  so as to keep the value of  as close as possible to 1. The calculation essentially gives us a numeric value as to how good is our regression line, as compared to the average value.</p>"},{"location":"ML/Regression/RegressionModelsComparison/#adjusted-r-squared","title":"Adjusted R-Squared","text":"<p>The value of  is considered to be better as it gets closer to 1, but there's a catch to this statement. The  value can be artifically inflated by simply adding more variables. This is a problem because the complexity of the model would increase due to this and would result in overfitting. The formulae for Adjusted R-squared is mathematically given as:</p> <p> </p> <p>where p is the number of regressors and n is the sample size. Adjusted R-squared has a penalizing factor that reduces it's value when a non-significant variable is added to the model.</p> <p>p-value based backward elimination can be useful in removing non-significant variables that we might have added in our model initially. (full process explained in Linear Regression section of the book.</p>"},{"location":"ML/Regression/SupportVectorRegression/","title":"Support Vector Regression","text":"<p>This method is regression equivalent of classification using Support Vector Machines.</p>"},{"location":"ML/Regression/SupportVectorRegression/#basic-principle","title":"Basic Principle","text":"<p>Much like Support Vector Machines in general, the idea of SVR, is to find a plane(linear or otherwise) in a plane that allows us to make accurate predictions for future data. The regression is done in a way that all the currently available datapoints fit in an error width given by . This allows us to find a plane which fits the data best and then this can be used to make future predictions on more data.</p> <p>Sometimes a soft error margin  may also be used to include additional points that lie outside the error margin .</p> <p></p>"},{"location":"ML/Regression/SupportVectorRegression/#nonlinearity","title":"Nonlinearity","text":""},{"location":"ML/Regression/SupportVectorRegression/#nonlinearity-by-preprocessing","title":"Nonlinearity by preprocessing","text":"<p>One way to application of SVR to represent nonlinear distributions is to map the training instances  using a map  into some feature space .</p> <p>This is obviously a computationaly expensive method and would require an understanding of the data and its relationship with the dependent variable before any actions can be performed since the mapping function needs to be figures out.</p>"},{"location":"ML/Regression/SupportVectorRegression/#implicit-mapping-via-kernels","title":"Implicit mapping via kernels","text":"<p>All the usual kernels that can be used with SVM classifiers, can be used in SVR as well. Code Reference [1] provides link to implementation and performance evaluations for various options available to be used as kernels.</p>"},{"location":"ML/Regression/SupportVectorRegression/#features","title":"Features","text":"<ol> <li>The implicit bias of the RBF based SVR model allows it to deal efficiently with outliers</li> <li>The penalty parameter makes it the best choice for problems where data is high risk and noisy.</li> </ol>"},{"location":"ML/Regression/SupportVectorRegression/#research-references","title":"RESEARCH REFERENCES","text":"<ol> <li>Support vector regression; Basak, Debasish, Srimanta Pal, and Dipak Chandra Patranabis; Neural Information Processing-Letters and Reviews 11.10 (2007): 203-224.</li> </ol> <p>[SOLVED] Review of various techniques, future scope in Support Vector Regression.</p> <ol> <li>A tutorial on support vector regression; Alex J. Smola and Bernhard Sc\u1e27olkopf</li> </ol> <p>[SOLVED] The basic principle of Support Vector Regression.</p>"},{"location":"ML/Regression/SupportVectorRegression/#code-references","title":"CODE REFERENCES","text":"<ol> <li>Numpy Based SVR: This article provides various tutorials for implementation of SVR using various kernel options.</li> </ol> <p>The penalty parameter C is used to regulate the problem of overfitting in SVR models. It can be tuned using Grid search or any better method for reducing the possibilitiy of overfitting.</p> <p>Note There is no implicit feature scaling in SVR Python, we need to do it explicity using</p>"},{"location":"ML/Regression/bayesian_regression/","title":"Bayesian Linear Regression","text":"<p>Bayesian Linear Regression is an approach to linear regression where statistical analysis is applied within the context of Bayesian statistics. Instead of just fit a linear regression to the data, it goes a step further in treating parameters (intercept and slope) as random variables with their own distributions.</p> <p></p> <p>Linear regression is a common method to model the relationship between a dependent variable and one or more independent variables. Bayesian Linear Regression uses the Bayesian method to model and understand the relationship between the dependent and independent variables.</p>"},{"location":"ML/Regression/bayesian_regression/#how-does-bayesian-linear-regression-work","title":"How does Bayesian Linear Regression work?","text":"<p>The Bayesian Linear Regression model works by calculating the posterior distribution of the parameters. In the Bayesian approach, we start with a prior distribution for parameters \ud835\udc4e and \ud835\udc4f, and the likelihood function which we get from the observed data is used to update our prior beliefs, to give the posterior distribution of \ud835\udc4e and \ud835\udc4f.</p> <pre><code>  # Implementing Bayesian Regression\n  from sklearn import linear_model\n\n  # Initialize Bayesian Ridge Regression\n  bayes = linear_model.BayesianRidge()\n\n  # Fit the Model to the training data\n  bayes.fit(X_train, y_train)\n\n  # Predict the response variable\n  y_pred = bayes.predict(X_test)\n</code></pre>"},{"location":"ML/Regression/bayesian_regression/#advantages-of-bayesian-linear-regression","title":"Advantages of Bayesian Linear Regression","text":"<ol> <li>Responds to Uncertainty: Bayesian Linear Regression has the advantage that it accounts for uncertainty in a way that general linear regression models do not.</li> <li>Flexibility: The model is flexible to the prior distribution.</li> <li>Updates Easily with More Data: Bayesian models can update the estimated probability easily with new data.</li> </ol>"},{"location":"ML/Regression/bayesian_regression/#disadvantages-of-bayesian-linear-regression","title":"Disadvantages of Bayesian Linear Regression","text":"<ol> <li>Priors can be Biased: The major disadvantage is heavy dependence on the choice of the prior. If the prior is wrongly chosen, then it takes much data to override that and come up to the true value.</li> <li>Computationally Expensive: Calculating the posterior distribution involves multiple integral calculations which can be computational expensive.</li> </ol>"},{"location":"ML/Regression/bayesian_regression/#when-to-use-bayesian-linear-regression","title":"When to Use Bayesian Linear Regression","text":"<p>Bayesian Linear Regression is particularly useful when we have limited data or we expect certain correlation or patterns to hold. It is also ideal if your data involves complex relationships that you have a strong reason to believe follow a particular pattern.</p> <p>The Bayesian approach becomes especially useful when we have the small dataset or multi-collinear features where linear regression can become unstable.</p> <p>Sources: </p> <ol> <li>Bayesian Linear Regression</li> <li>Bayesian Ridge Regression</li> </ol>"},{"location":"ML/Regression/cox_regression/","title":"Cox Regression","text":"<p>Cox regression (or proportional hazards regression) is a method for investigating the effect of several variables upon the time a specified event takes to happen. In the context of an outcome such as death, this event could occur in five days (mortality in the high-risk group) or perhaps three years (in the low-risk group). It is named after the statistician who developed it, Sir David Cox.</p> <p></p>"},{"location":"ML/Regression/cox_regression/#introduction","title":"Introduction","text":"<p>Cox regression uses the proportional hazards model to express the effect of the predictors on survival time. What sets it apart is its ability to incorporate time-dependent covariates: variables whose values can vary with time. An example could be the amount of physical exercise: maybe a subject starts to exercise more at a certain point during the study, which could impact the time-to-event (survival).</p> <p>The Cox model is expressed by the hazard function, which describes the risk at a time t, conditional on survival until time t, as a function of a set of covariates:</p> <pre><code>h(t | X) = h0(t) \u00d7 exp(b1X1 + b2X2 + ... + bpXp).\n</code></pre> <p>where:</p> <ul> <li>t is the survival time.</li> <li>h(t) is the hazard function determined by a set of p covariates (X1, X2, ..., Xp).</li> <li>The coefficients (b1, ..., bp) measure the effect (i.e., the change in hazard) of the covariates.</li> <li>The term h0 is called the baseline hazard, and is the value of the hazard if all the Xi are equal to zero (the quantity exp(0) equals 1).</li> </ul>"},{"location":"ML/Regression/cox_regression/#advantages","title":"Advantages","text":"<ul> <li> <p>Cox Regression accounts for censoring in the sense that it's able to handle the scenarios in which the outcome of interest has not yet occurred for several observations in the sample.</p> </li> <li> <p>Naturally, it can handle more than one predictor variable.</p> </li> <li> <p>It allows to study how the strength and nature of relationships between mortality and predictors changes with age.</p> </li> </ul>"},{"location":"ML/Regression/cox_regression/#disadvantages","title":"Disadvantages","text":"<ul> <li> <p>The main limitation of Cox regression is its assumption of proportional hazards, which may not be appropriate for all datasets and must be empirically tested.</p> </li> <li> <p>It does not naturally admit random effects or a natural way to model interactions between covariates.</p> </li> <li> <p>It is not as interpretable as other models, since its output is a hazard ratio and not a clear-cut probability.</p> </li> </ul>"},{"location":"ML/Regression/cox_regression/#application-scenarios","title":"Application Scenarios","text":"<p>Cox regression can be widely used in clinical trials or community trials to compare survival curves of two or more groups (like drug vs. placebo or different diet groups) and to explore the effects of predictor variables. It is commonly used in medical research for investigating the association between the survival time of patients and one or more predictor variables.</p>"},{"location":"ML/Regression/cox_regression/#python-implementation","title":"Python Implementation","text":"<p>Python's <code>lifelines</code> module has a class called <code>CoxPHFitter</code> to fit the Cox regression model. Here is a sample usage:</p> <pre><code>from lifelines import CoxPHFitter\n\n# Instantiate the class to create a cph object\ncph = CoxPHFitter()\n\n# Fit the data to train the model\ncph.fit(data, 'Survival_in_days', event_col='Death')\n\n# Have a look at the significance of the features\ncph.print_summary()\n</code></pre> <p>References:</p> <ul> <li>Cox Proportional Hazards Model</li> <li>Survival Analysis in Python</li> </ul> <pre><code>\n</code></pre>"},{"location":"ML/Regression/decisiontree_regression/","title":"Decision Tree Regression With Python","text":""},{"location":"ML/Regression/decisiontree_regression/#introduction","title":"Introduction","text":"<p>Decision Tree Regression is a machine learning algorithm that uses a decision tree model for predicting a continuous or categorical outcome. Like decision trees for classification, regression trees also split data based on feature values and assign predictions based on means or modes. However, instead of the output being a category, a decision tree regression gives an output that is a real value (or continuous value).</p> <p></p> <p>The Decision Tree Regression Model is created using two steps:</p> <ul> <li> <p>Splitting: This is a process of partitioning the data into subsets. Splitting is done on the basis of entropy and information gain.</p> </li> <li> <p>Tree Pruning: In this process the unnecessary branches which have little role in decision making are removed. Generally, pruning is performed by removing the sections of the tree that provide little power to classify instances.</p> </li> </ul>"},{"location":"ML/Regression/decisiontree_regression/#python-implementation-of-decision-tree-regression","title":"Python Implementation of Decision Tree Regression","text":"<p>The Python machine learning library, Scikit-Learn, supports decision tree regression through the 'DecisionTreeRegressor' class.</p> <p>Here's a basic example of what python code might look like for decision tree regression:</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\n\n# create a regressor object\nregressor = DecisionTreeRegressor(random_state = 0)\n\n# fit the regressor with X and Y data\nregressor.fit(X, Y)\n</code></pre> <p>Here, X and Y are the input and output values respectively.</p>"},{"location":"ML/Regression/decisiontree_regression/#advantages-of-decision-tree-regression","title":"Advantages of Decision Tree Regression","text":"<ol> <li> <p>Ease to understand: Decision tree regressor models are visually intuitive and easy to interpret.</p> </li> <li> <p>Requires less data preprocessing: It does not require normalization, dummy variables, etc.</p> </li> <li> <p>Handles both continuous and categorical variables: Decision Tree Regressor is a type of algorithm that is capable of handling both types of variables.</p> </li> <li> <p>Supports Multi-output: It is capable of predicting multiple outputs.</p> </li> </ol>"},{"location":"ML/Regression/decisiontree_regression/#disadvantages-of-decision-tree-regression","title":"Disadvantages of Decision Tree Regression","text":"<ol> <li> <p>Overfitting: Decision Tree Regressor sometimes creates over-complex trees that do not generalize the data well.</p> </li> <li> <p>Instability: Small variations in the data might result in a completely different decision tree regression. This issue can be mitigated by using decision trees within an ensemble.</p> </li> <li> <p>Possibility of bias: Decision-tree learners can create biased trees if some classes dominate.</p> </li> </ol>"},{"location":"ML/Regression/decisiontree_regression/#scenarios-for-usage","title":"Scenarios for usage","text":"<p>Decision Tree regression is used in several scenarios including:</p> <ul> <li> <p>When the relationship between features and the target variable is non-linear and complex.</p> </li> <li> <p>When you need a predictive model that can be easily visualized and communicated.</p> </li> <li> <p>When you need a model that can handle both categorical and numerical data.</p> </li> <li> <p>In business decision making due to its easy to understand the logic behind the model.</p> </li> </ul>"},{"location":"ML/Regression/elasticNet_regression/","title":"ElasticNet Regression","text":"<p>ElasticNet is a linear regression model trained with both <code>L1</code> and <code>L2</code>-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.</p> <p>Elastic-net is useful when there are multiple correlated features.</p>"},{"location":"ML/Regression/elasticNet_regression/#how-it-works","title":"How it works","text":"<p>Roughly speaking, ElasticNet is a model which combines both L1 and L2 regularization of Lasso and Ridge methods:</p> <p>Lasso helps to reduce overfitting by setting some feature coefficients to zero thus completely eliminating them and Ridge Regression helps to control the multicollinearity effect.</p> <p>ElasticNet incorporates penalties from both L1 and L2 regularization:</p> <p></p> <p>Where,</p> <ul> <li><code>\u03bb</code> is the tuning parameter</li> <li>\u03b1 is the proportion of L1 penalty and 1- \u03b1 is the L2 penalty.</li> </ul>"},{"location":"ML/Regression/elasticNet_regression/#python-example","title":"Python Example","text":"<p>Here is a simple implementation of ElasticNet Regression in Python using the sklearn library.</p> <pre><code>    from sklearn.linear_model import ElasticNet\n    from sklearn.datasets import make_regression\n\n    # Generate a regression data set\n    X, y = make_regression(n_features=2, random_state=0)\n\n    # Define the model\n    regr = ElasticNet(random_state=0)\n\n    # Train the model\n    regr.fit(X, y)\n\n    # Print the coefficients\n    print(regr.coef_)\n</code></pre> <p>The above code creates a simple regression model, fits it to the data <code>X</code>, <code>y</code>, and then prints out the coefficients of the model.</p>"},{"location":"ML/Regression/elasticNet_regression/#advantages","title":"Advantages","text":"<ol> <li>It combines the L1 &amp; L2 properties, It works well on some specific datasets where the features are correlated.</li> <li>It has built-in feature selection.</li> <li>It's efficient when dealing with large number of features.</li> </ol>"},{"location":"ML/Regression/elasticNet_regression/#disadvantages","title":"Disadvantages","text":"<ol> <li>It includes a hyper-parameter that needs to be tuned, so computational expense is an issue.</li> <li>Performance is compromised when we have uncorrelated variables.</li> </ol>"},{"location":"ML/Regression/elasticNet_regression/#when-to-use-elasticnet-regression","title":"When to use ElasticNet Regression?","text":"<p>ElasticNet is a middle ground between Ridge Regression and Lasso Regression. The \u03b1 parameter is a mix ratio between Ridge and Lasso, giving some balance between the two.</p> <ul> <li>When you have highly correlated variables, Ridge regression makes sense.</li> <li>When you have sparse data or wish to automate certain parts of model selection, the Lasso makes sense.</li> <li>If multicollinearity is suspected in your dataset and/or you wish to automate certain parts of model selection, an Elastic Net could work.</li> </ul>"},{"location":"ML/Regression/elasticNet_regression/#conclusion","title":"Conclusion","text":"<p>ElasticNet is a powerful tool that combines the benefits of two other regression methods. It is, however, more complex and requires careful tuning of its parameters. In return, it can bring substantial benefits in terms of model accuracy, especially for datasets exhibiting multicollinearity.</p>"},{"location":"ML/Regression/general_linear_regression/","title":"Generalized Linear Model Regression in Python","text":"<p>Generalized Linear Models or GLMs extend the linear modeling process, allowing for response variables that have error distribution models other than a normal distribution.</p> <p>GLMs consist of three elements:</p> <ol> <li> <p>A random component: This represents the probability distribution of the response variable (Y). It could be normal, exponential, binomial, or any other suitable distribution based on the nature of Y.</p> </li> <li> <p>A systematic component: This represents the independent variables (X1, X2, etc.), which could be linear or a complex polynomial function based on the nature of the predictors.</p> </li> <li> <p>A link function: This associates the random and systematic components. The link function could be identity, log, logit, etc., which depends on the nature of Y and X.</p> </li> </ol> <p>An example of GLM regression applied in Python would generally look like this:</p> <pre><code>import statsmodels.api as sm\n\n# Load data\ndata = sm.datasets.scotland.load(as_pandas=True)\ndata_x = data.exog\ndata_y = data.endog\n\n# Fit and summarize GLM:\nglm_gauss = sm.GLM(data_y, data_x, family=sm.families.Gaussian())\nglm_gauss_results = glm_gauss.fit()\nprint(glm_gauss_results.summary())\n</code></pre> <p></p>"},{"location":"ML/Regression/general_linear_regression/#advantages-of-glm","title":"Advantages of GLM","text":"<ol> <li> <p>Flexibility: GLMs are versatile and flexible because they can handle any kind of distribution, not just normal ones.</p> </li> <li> <p>Interpretability: GLMs can provide insights into the effects of predictors and help identify influential points.</p> </li> </ol>"},{"location":"ML/Regression/general_linear_regression/#disadvantages-of-glm","title":"Disadvantages of GLM","text":"<ol> <li> <p>Assumptions: GLMs require specific distributional assumptions. When these assumptions don\u2019t hold, it can affect the model\u2019s performance.</p> </li> <li> <p>Overfitting: GLMs can overfit to the training data leading to poor prediction performance on unseen data.</p> </li> <li> <p>Sensitive to Outliers: GLM regression can be sensitive to outliers. It could significantly distort the model, resulting in a line of poor fit.</p> </li> </ol>"},{"location":"ML/Regression/general_linear_regression/#when-to-use-glm-regression","title":"When to use GLM Regression","text":"<p>Generalized Linear Models can be most helpful when your data distribution can't be reasonably modeled by the usual linear regression or if you want to predict a binary outcome. They are useful in various scenarios, such as predicting the counts of events, rates, the outcome of yes/no scenarios, proportions or percentages etc.</p>"},{"location":"ML/Regression/general_linear_regression/#conclusion","title":"Conclusion","text":"<p>Therefore, GLM regression offers a broad, flexible approach to model relationships between variables. It is crucial to visualize your data and validate your model's assumptions before making conclusions.</p>"},{"location":"ML/Regression/gradient_adaboost_regression/","title":"Gradient Boosting &amp; AdaBoost Regression","text":"<p>Gradient Boosting and AdaBoost are popular ensemble methods used in machine learning. They utilise multiple learning algorithms to obtain better predictive performance that could not be obtained from any of the constituent learning algorithms alone.</p> <p>Let's discuss these two methods and how they apply to regression tasks:</p>"},{"location":"ML/Regression/gradient_adaboost_regression/#gradient-boosting-regression","title":"Gradient Boosting Regression","text":"<p>Gradient Boosting is a machine learning algorithm based on decision freeze that develops multiple weak learners (typically decision trees), and then combines them to make a single strong learner.</p> <p>Gradient Boosting algorithm works on the principle of boosting weak learners. Here, the losses of one tree are recovered by the next tree. It makes new learners focus more on the instances that previous learners classified incorrectly.</p> <p>Code sample using Python's scikit-learn library:</p> <pre><code>from sklearn.ensemble import GradientBoostingRegressor\nregressor = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\nregressor.fit(X_train, y_train)\n</code></pre>"},{"location":"ML/Regression/gradient_adaboost_regression/#advantages","title":"Advantages","text":"<ol> <li> <p>It's very flexible - it can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.</p> </li> <li> <p>No data pre-processing required - often produces great results without scaling or handling outliers.</p> </li> </ol>"},{"location":"ML/Regression/gradient_adaboost_regression/#disadvantages","title":"Disadvantages","text":"<ol> <li> <p>GBMs are more sensitive to overfitting if the data is noisy.</p> </li> <li> <p>Training generally takes longer because of the fact that trees are built sequentially.</p> </li> <li> <p>It's harder to tune than other models due to complexity of the hyperparameters.</p> </li> </ol>"},{"location":"ML/Regression/gradient_adaboost_regression/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Anomaly detection in supervised learning settings where the data is often highly unbalanced.</p> </li> <li> <p>Regression and classification problems.</p> </li> </ol>"},{"location":"ML/Regression/gradient_adaboost_regression/#adaboost-regression","title":"AdaBoost Regression","text":"<p>AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm. It can be used in conjunction with many other types of learning algorithms to improve their performance on regression tasks. It focuses on classification problems and aims to convert a set of weak classifiers into a strong one.</p> <p>Code sample using Python's scikit-learn library:</p> <pre><code>from sklearn.ensemble import AdaBoostRegressor\nregressor = AdaBoostRegressor(random_state=0, n_estimators=100)\nregressor.fit(X_train, y_train)\n</code></pre>"},{"location":"ML/Regression/gradient_adaboost_regression/#advantages_1","title":"Advantages","text":"<ol> <li> <p>AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners.</p> </li> <li> <p>The user only needs to choose: (1) which weak classifier might work best to solve their given classification problem, and (2) the number of boosting rounds that should be used during the training phase.</p> </li> </ol>"},{"location":"ML/Regression/gradient_adaboost_regression/#disadvantages_1","title":"Disadvantages","text":"<ol> <li> <p>AdaBoost can be sensitive to noisy data and outliers.</p> </li> <li> <p>The performance of AdaBoost depends on the data and the weak learner. Consequently, there's a need for tweaking and testing to make it work perfectly.</p> </li> </ol>"},{"location":"ML/Regression/gradient_adaboost_regression/#use-cases_1","title":"Use Cases","text":"<ol> <li> <p>Face detection as a binary categorization problem of face vs non-face.</p> </li> <li> <p>Predicting customer churn and classifying the types of topics customers are talking/calling about.</p> </li> </ol>"},{"location":"ML/Regression/gradient_adaboost_regression/#comparing-gradient-boosting-and-adaboost","title":"Comparing Gradient Boosting and AdaBoost","text":"<p>While both Gradient Boosting and AdaBoost work on the principle of boosting weak learners, there are some differences in their implementation of this principle.</p> <ol> <li> <p>Gradient Boosting improves the model using the gradient descent algorithm, which adjusts the weight of an instance based on the residual error. AdaBoost, on the other hand, adjusts the weight based on the sum of the weighted error.</p> </li> <li> <p>In AdaBoost, the weak learner's decision stump makes decisions based on single features, while in Gradient Boosting, decision trees can be used, which make decisions based on multiple features.</p> </li> </ol> <p>Overall, both are powerful algorithms that have proven to be very useful in machine learning tasks, more specifically in regression problems.</p>"},{"location":"ML/Regression/lasso_regression/","title":"Lasso Regression Classifier","text":""},{"location":"ML/Regression/lasso_regression/#introduction","title":"Introduction","text":"<p>Lasso Regression, or <code>least absolute shrinkage and selection operator</code>, is a linear regression analysis method that performs both variable selection and regularization. This method was originally formulated by Robert Tibshirani in 1996.</p> <p>The key feature of Lasso Regression is that it can shrink the coefficients of less important features to exactly zero, which works like a feature selection method. This makes it quite useful in handling high-dimension data where it can provide a sparse solution.</p> <p></p>"},{"location":"ML/Regression/lasso_regression/#python-explanation","title":"Python Explanation","text":"<p>In Python, Lasso Regression can be easily performed by using sklearn's <code>Lasso</code> function. Here is a basic example:</p> <pre><code>from sklearn import linear_model\n\n# Create lasso instance\nlasso = linear_model.Lasso(alpha=0.1)\n\n# Fit the model\nlasso.fit(X_train, y_train)\n\n# Perform prediction\npredictions = lasso.predict(X_test)\n</code></pre> <p>In the above codes, <code>alpha</code> is the regularization strength. Larger values of alpha increase the amount of regularization and thus decrease the chance of overfitting.</p>"},{"location":"ML/Regression/lasso_regression/#advantages-of-lasso-regression","title":"Advantages of Lasso Regression","text":"<ol> <li> <p>Feature Selection: As mentioned earlier, Lasso Regression performs L1 regularization, i.e., it adds a factor of the sum of the absolute values of the coefficients in the optimization objective. This results in shrinking the less important feature\u2019s coefficient to zero, thus, removing some features altogether.</p> </li> <li> <p>Prevents Overfitting: The regularization property also helps to reduce overfitting by restricting the model's complexity.</p> </li> <li> <p>Simplicity: Lasso Regression can produce simpler and more interpretable models that involve only a subset of the predictors.</p> </li> </ol>"},{"location":"ML/Regression/lasso_regression/#disadvantages-of-lasso-regression","title":"Disadvantages of Lasso Regression","text":"<ol> <li> <p>Cannot Handle Multicollinearity Well: If there are two highly correlated variables, Lasso tends to select one randomly and ignore the other.</p> </li> <li> <p>Stability Issues: A small change in the data can cause a large change in the estimate of the regression coefficients, which in turn can change which predictor variables get ignored.</p> </li> <li> <p>Performance: Lasso may not perform well if there are high levels of noise in the data.</p> </li> </ol>"},{"location":"ML/Regression/lasso_regression/#usage-scenarios","title":"Usage Scenarios","text":"<p>Given its ability to manage high-dimension datasets and prevent overfitting, Lasso Regression can be ideal for scenarios where we have plenty of features, some of which may not be significant. Moreover, it can also be used when we aim for a simple model with fewer features, for example, in scenarios where interpretability is important and we want a sparse, easily-interpretable model.</p> <p>In general, Lasso Regression should be used when dealing with a large number of features and a complex dataset, especially when feature selection is of interest.</p> <p>Technically, Lasso Regression is a great tool within the arsenal of a data scientist for model building. However, the nature of the problem and the data should always guide which tools to use.</p>"},{"location":"ML/Regression/linear_regression/","title":"Linear Regression: A Detailed Explanation with Python Implementation","text":"<p>Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables \u2014 a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increase (or decrease), the dependent variable increases (or decrease) too.</p> <p></p> <p>Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line).</p>"},{"location":"ML/Regression/linear_regression/#math-behind-linear-regression","title":"Math behind Linear Regression","text":"<p>It is represented by an equation <code>Y=a+b*X + e</code>, where <code>a</code> is the intercept, <code>b</code> is the slope of the line, and <code>e</code> is the error term. The equation can be used to predict the value of the target variable based on given predictor variable(s).</p> <p>The intercept <code>a</code> is the predicted value of Y when the X is 0. The slope <code>b</code> is the predicted increase in Y resulting from a one unit increase in X. The error <code>e</code> is the difference between the actual value and the predicted value of the target variable.</p> <p></p>"},{"location":"ML/Regression/linear_regression/#python-code-example","title":"Python Code Example","text":"<pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Create linear regression object\nlr = LinearRegression()\n\n# Split the dataset into training and testing datasets\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)\n\n# Train the model using the training sets\nlr.fit(x_train, y_train)\n\n# Make predictions using the testing set\ny_pred = lr.predict(x_test)\n</code></pre>"},{"location":"ML/Regression/linear_regression/#advantages-of-linear-regression","title":"Advantages of Linear Regression","text":"<ol> <li>It is a fairly simple model that doesn\u2019t require high computation power.</li> <li>Linear regression analysis, when used with statistical techniques, can help to determine the reliability of the predictions.</li> <li>It is useful when the dataset is linearly separable.</li> </ol>"},{"location":"ML/Regression/linear_regression/#disadvantages-of-linear-regression","title":"Disadvantages of Linear Regression","text":"<ol> <li>It assumes a linear relationship between variables. If the relationship is non-linear, it might produce a high bias error.</li> <li>It's can be sensitive to outliers.</li> <li>It might overfit or underfit the data depending on the nature of the dataset.</li> </ol>"},{"location":"ML/Regression/linear_regression/#when-should-you-use-linear-regression","title":"When should you use Linear Regression?","text":"<p>Linear regression should be used when the target variable is of a continuous nature (e.g. Sales, Price etc.) and there is a linear relationship between the dependent (target variable) and independent variables.</p> <p>In business it could be used for sales forecasting, resource consumptions forecasting, supply cost forecasting, etc.</p>"},{"location":"ML/Regression/linear_regression/#_1","title":"Linear Regression","text":""},{"location":"ML/Regression/logistic_regression/","title":"Logistic Regression Classifier in Python","text":"<p>Logistic Regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross-entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019.</p> <p></p> <p>In the below sections, we will discuss more about Logistic Regression, along with its Python implementation.</p>"},{"location":"ML/Regression/logistic_regression/#python-explanation","title":"Python Explanation","text":"<pre><code># Import library\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Create logistic regression object\nlogReg = LogisticRegression()\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model using the training sets\nlogReg.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = logReg.predict(X_test)\n</code></pre>"},{"location":"ML/Regression/logistic_regression/#advantages","title":"Advantages","text":"<ol> <li>Easy to Implement: Logistic regression is easy to understand and less complex than other algorithms.</li> <li>Speed: It is less intense computationally, thus making it faster.</li> <li>Probabilistic Approach: Logistic Regression provides a probability score for observations.</li> <li>Useful for Feature Importance: Logistic regression is not a black box, you can understand what variable contribute to the outcomes and by how much.</li> </ol>"},{"location":"ML/Regression/logistic_regression/#disadvantages","title":"Disadvantages","text":"<ol> <li>Binary Outcome Only: Logistic regression is trying to predict a binary outcome. For multi-class problems, one will need to tweak the logistic regression model or turn to other algorithms.</li> <li>Sensitive to Outliers: Logistic regression can be sensitive to outliers and may produce a biased model.</li> </ol>"},{"location":"ML/Regression/logistic_regression/#scenarios-to-use","title":"Scenarios to Use","text":"<ol> <li>Medical Fields: Logistic Regression is used in certain fields, such as medical fields, where outcomes are often binary (e.g., patient survived/did not survive).</li> <li>Credit Scoring: Logistic Regression is also used in credit scoring when the outcome is whether a customer will default or not.</li> <li>Marketing Applications: When predicting if a customer will buy a product or not.</li> <li>Anywhere Probability is Needed: Logistic regression is useful anywhere we need probability as an outcome.</li> </ol>"},{"location":"ML/Regression/logistic_regression/#conclusion","title":"Conclusion","text":"<p>Logistic Regression has a wide variety of applications and is a powerful tool when used correctly. Despite its simplicity, it can provide a good baseline in many applications. It is essential to understand the assumptions and limitations of any algorithm before usage and Logistic Regression is no exception.</p>"},{"location":"ML/Regression/negative_binomial_regression/","title":"Negative Binomial Regression","text":"<p>Negative binomial regression is a type of generalized linear model (GLM), where the target variable is a count of the number of times an event occurs. The fundamental condition is the target variable or the response variable should be a count, such as the number of emails you receive each day, the number of customer complaints, etc.</p> <p>When the variance is larger than the mean, data scientists deploy negative binomial regression. The mathematical form of the negative binomial distribution usually contains a dispersion parameter that adjusts the variance independently from the mean.</p>"},{"location":"ML/Regression/negative_binomial_regression/#fundamental-concept","title":"Fundamental Concept","text":"<p>Negative binomial regression is used when the dataset, containing the number of occurrences of an event, shows variances that grow with the mean as well as the presence of over-dispersion.</p> <p>Here is the formula:</p> <p></p> <p>Here, mu is the expected value of the response, and alpha is the dispersion parameter.</p> <p></p>"},{"location":"ML/Regression/negative_binomial_regression/#advantages","title":"Advantages","text":"<ul> <li>Negative binomial regression retains the efficiency and flexibility of ordinary regression.</li> <li>It allows the modeling of non-integer-based events and adjusts the variance independently from the mean.</li> <li>It is especially helpful for over-dispersed data, where the variance is greater than the mean.</li> </ul>"},{"location":"ML/Regression/negative_binomial_regression/#disadvantages","title":"Disadvantages","text":"<ul> <li>One disadvantage of negative binomial regression is that it can easily misspecified or misinterpreted.</li> <li>It has high sensitivity against the outliers.</li> <li>It requires a large sample size to achieve sufficient power.</li> </ul>"},{"location":"ML/Regression/negative_binomial_regression/#code-snippet-in-python","title":"Code Snippet in Python","text":"<p>Here is a simple example of a negative binomial regression in Python.</p> <pre><code>import statsmodels.api as sm\nimport numpy as np\n\ndata = sm.datasets.get_rdataset(\"mtcars\").data\ndata[\"constant\"] = 1\nexog_vars = [\"constant\",\"mpg\",\"hp\",\"qsec\"]\nexog = sm.add_constant(data[exog_vars])\nendog = data[\"am\"]\n\nmod = sm.GLM(endog, exog, family=sm.families.NegativeBinomial())\nres = mod.fit()\nprint(res.summary())\n</code></pre> <p>In this example, dataset \"Mtcars\" is used. EXOG refers to the predictor variables (mpg, hp, qsec). ENDOG is the dependent variable (\"am\").</p>"},{"location":"ML/Regression/negative_binomial_regression/#scenario-for-usage","title":"Scenario for usage","text":"<p>Generally, negative binomial regression is used in scenarios where the data consists of count of events and the variance is greater than the mean. For example, it could be used in predicting the number of times a website will be visited in a given time frame.</p> <p>Moreover, negative binomial regression can be used in various fields like predicting number of traffic accidents, disease counts in epidemiology, and so on.</p>"},{"location":"ML/Regression/poisson_regression/","title":"Poisson Regression","text":"<p>Poisson Regression is a form of regression analysis, which is utilized to model count data and contingency tables. You would use Poisson regression when the outcome you are interested in is counts. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables.</p> <p>Looking into the generalized linear models (GLMs), Poisson regression assumes the response variable Y has a Poisson distribution and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters.</p> <p>With that said, if:</p> <p><code>Y ~ Poisson (\u03bb)</code> <code>Log (\u03bb) = \u03b7 = X\u03b2</code></p> <p>A regression equation is a deterministic part and the stochastic (random) part. The deterministic part consists of the independent variable(s) and the parameter(s), including the intercept and the slope. The stochastic part shows the random variability around the deterministic prediction.</p> <p>(lambda) \u03bb stands for the Poisson distribution parameter or the expected count.</p> <p></p>"},{"location":"ML/Regression/poisson_regression/#python-implementation","title":"Python Implementation","text":"<p>Step 1: Import your needed libraries</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n</code></pre> <p>Step 2: Import your data. Let's use a dataset from seaborn for instance:</p> <pre><code># import necessary libraries\nimport seaborn as sns\n\n#Load dataset\ndf = sns.load_dataset('tips')\ndf.head()\n</code></pre> <p>Step 3: Define X and y and fit the model using sm.GLM:</p> <pre><code>X = df[['total_bill', 'size']]\ny = df['tip']\n\n# add constant to predictor variables\nX = sm.add_constant(X)\n\n# fit Poisson regression model\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# view model summary\nprint(model.summary())\n</code></pre>"},{"location":"ML/Regression/poisson_regression/#advantages-of-poisson-regression","title":"Advantages of Poisson Regression","text":"<ol> <li> <p>The main advantage of Poisson regression is its ability to handle count data. Count data often have unique features that are best captured by Poisson distribution.</p> </li> <li> <p>It can establish relationships between a range of predictor variables.</p> </li> <li> <p>It's an easy-to-understand model, even persons with a basic understanding of regression can interpret the result.</p> </li> </ol>"},{"location":"ML/Regression/poisson_regression/#disadvantages-of-poisson-regression","title":"Disadvantages of Poisson Regression","text":"<ol> <li> <p>Since Poisson regression requires count data, its utility is limited in situations where outcome variables are not counts.</p> </li> <li> <p>The model has strict assumptions, such as the mean and variance of the outcome variable are identical.</p> </li> <li> <p>It can't handle negative numbers. You would need to use different regression modeling techniques in this case.</p> </li> <li> <p>Poisson regression can be influenced by over-dispersion and under-dispersion, which might lead to incorrect inferences.</p> </li> </ol>"},{"location":"ML/Regression/poisson_regression/#when-to-use-poisson-regression","title":"When to Use Poisson Regression?","text":"<p>The assumption of Poisson regression is that the dependent variable follows a Poisson distribution. Therefore, you can use it in scenarios when your dependent variable is a count and a number of trials are known, and success probability is small.</p> <p>Essentially, when modeling events that happen randomly but with a known average rate, Poisson Regression is the ideal choice. This includes transportation vehicle crashes, disease incidences, and customers calling a help-center.</p>"},{"location":"ML/Regression/polynomial_regression/","title":"Polynomial Regression","text":"<p>Polynomial Regression is a form of regression analysis in which the relationship between the independent variable <code>x</code> and the dependent variable <code>y</code> is modelled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of <code>x</code> and the corresponding conditional mean of <code>y</code>.</p> <p></p>"},{"location":"ML/Regression/polynomial_regression/#python-explanation","title":"Python Explanation","text":"<p>Polynomial Regression in python can be implemented by importing the <code>PolynomialFeatures</code> class from sklearn.preprocessing library. This class is used to convert the original features into polynomial features. Then the polynomial features are fed into a linear regression classifier for prediction.</p> <p>Here's a basic example of how it works:</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Create polynomial features\npoly = PolynomialFeatures(degree=2)\nx_poly = poly.fit_transform(x)\n\n# Feed it into Linear Regression\nmodel = LinearRegression()\nmodel.fit(x_poly,y)\n\n# Predict\npredictions = model.predict(x_poly)\n</code></pre>"},{"location":"ML/Regression/polynomial_regression/#advantages-of-polynomial-regression","title":"Advantages of Polynomial Regression","text":"<ol> <li>Broad range of function can be fit under it.</li> <li>Polynomial basically fits wide range of curvature.</li> <li>Polynomial provides the best approximation of the relationship between the dependent and independent variable.</li> </ol>"},{"location":"ML/Regression/polynomial_regression/#disadvantages-of-polynomial-regression","title":"Disadvantages of Polynomial Regression","text":"<ol> <li>These are too sensitive to the outliers.</li> <li>The presence of one or two outliers in the data can seriously affect the results of the nonlinear analysis.</li> <li>In addition there are unfortunately fewer model validation tools for the detection of outliers in nonlinear regression than there are for linear regression.</li> </ol>"},{"location":"ML/Regression/polynomial_regression/#when-to-use-polynomial-regression","title":"When to use Polynomial Regression","text":"<p>Polynomial Regression can be used when the data points closely follow a sensed non-linear trend. The trend which suggests that the data points are more likely fitted in a non-linear line, in that case Polynomial Regression can be used.</p> <p>In which scenarios:</p> <ul> <li>When we try to model the yield of a chemical synthesis in terms of temperature,</li> <li>Or steam usage rate in terms of the settings of different heating dials.</li> </ul> <p>These scenarios help in finding the real non-linear relationship between temperature and yield, and sometimes these relationships are not linear but they are quadratic or cubic.</p> <p>Remember, it\u2019s your job to test different polynomial degrees and make sure you are avoiding overfitting. Proper tools such as cross-validation and validation curves are there to help you. Please be responsible when creating your models.</p>"},{"location":"ML/Regression/random_forest_regression/","title":"Random Forest Regression","text":""},{"location":"ML/Regression/random_forest_regression/#random-forest-regression","title":"Random Forest Regression","text":"<p>Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees.</p> <p></p>"},{"location":"ML/Regression/random_forest_regression/#how-does-random-forest-work","title":"How does Random Forest work?","text":"<p>Random Forest regression works in the following way. Suppose you have a set of predictors (independent variables) and a target variable (dependent variable). The Random Forest regression model would then take a random subset of predictors and a random subset of data points from your data to grow a decision tree. The decision tree predicts the value of the target variable given the predictors.</p> <p>This process repeats, each time with a new random subset of predictors and a new random subset of data points. The result is a \"forest\" of decision trees. Each tree gives you a prediction. The Random Forest model averages these predictions to deliver a final prediction that is robust and less prone to overfitting.</p> <p></p>"},{"location":"ML/Regression/random_forest_regression/#python-implementation","title":"Python Implementation","text":"<p>Random Forest model can be implemented in Python using the RandomForestRegressor class in Scikit-Learn library.</p>"},{"location":"ML/Regression/random_forest_regression/#advantages-of-random-forest-regression","title":"Advantages of Random Forest Regression","text":"<ol> <li> <p>Random Forest can handle large data sets with high dimensionality.</p> </li> <li> <p>It can handle missing values and maintains accuracy for missing data.</p> </li> <li> <p>The Random Forest algorithm develops a robust model because it takes the average of all the predictions of the decision trees.</p> </li> <li> <p>The model does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which balances the biasness.</p> </li> <li> <p>Random Forest regressor has a less variance then single decision trees. Hence, the model will not be very much affected by noise.</p> </li> </ol>"},{"location":"ML/Regression/random_forest_regression/#disadvantages-of-random-forest-regression","title":"Disadvantages of Random Forest Regression","text":"<ol> <li> <p>Complexity: Random Forest creates a lot of trees (unlike only one tree in case of decision tree) and combines their outputs. By default, it creates 100 trees in Python sklearn library.</p> </li> <li> <p>Longer Training Period: Random Forest require much more time to train as compared to decision trees as it generates a lot of trees (instead of one tree in case of decision tree).</p> </li> <li> <p>More Resources: The Random Forest algorithm is quick to use resources to the maximum extent. It requires more computational resources and it is less intuitive in case when we have a large collection of decision trees.</p> </li> </ol>"},{"location":"ML/Regression/random_forest_regression/#scenario-to-use-random-forest-regression","title":"Scenario to use Random Forest Regression","text":"<p>Random Forest can be used in a variety of applications such as recommendation engines, image classification, feature selection, etc. It can be used for both regression and classification tasks. But it is mainly considered as a robust technique for regression and yet, not the first preference for classification tasks.</p> <p>When it comes to regression, Random Forest is most useful with a large data set and complex problems. It is especially useful when the relationships between the variables are complex and difficult to tease apart with a linear model or other simple regression techniques.</p> <p>Random Forest regression is a very powerful machine learning model and holds great promise for predictive modeling tasks. However, it should be used with proper care and attention to details since it is a more complex model and can be computationally intensive.</p>"},{"location":"ML/Regression/ridge_regression/","title":"Ridge Regression Classifier","text":"<p>Ridge regression is a regularization technique and an extension of linear regression where the loss function is altered by adding a penalty equivalent to the square of the magnitude of the coefficients.</p>"},{"location":"ML/Regression/ridge_regression/#detailed-understanding","title":"Detailed Understanding","text":"<p>In a standard linear equation, we try to minimize the error between our predictions and actual values. In ridge regression, we try to minimize this same error, but with a penalty term added. This penalty term is the L2 regularization which is the square sum of magnitude of coefficient values multiplied by lambda.</p> <p>The ridge coefficients minimize a penalized residual sum of squares:</p> <p></p> <p>where, \u03b1 &gt;= 0 is a complexity parameter that controls the amount of shrinkage: the larger the value of \u03b1, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</p>"},{"location":"ML/Regression/ridge_regression/#implementation-in-python","title":"Implementation in Python","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\n\n# create feature matrix (X) and target vector (y)\nX = data[data.columns.tolist()[:-1]]\ny = data[data.columns.tolist()[-1]]\n\n# split the dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create Ridge regression object\nridge = Ridge(alpha=1.0)\n\n# Train the model using the training sets\nridge.fit(X_train,y_train)\n</code></pre>"},{"location":"ML/Regression/ridge_regression/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":""},{"location":"ML/Regression/ridge_regression/#advantages","title":"Advantages","text":"<ol> <li>Ridge regression allows you to regularize or shrink coefficient estimates towards zero, to prevent overfitting.</li> <li>Ridge can handle multicollinearity and high-dimension data well by introducing bias.</li> <li>It includes all the features in the model.</li> </ol>"},{"location":"ML/Regression/ridge_regression/#disadvantages","title":"Disadvantages","text":"<ol> <li>It includes all the features in the model, so can lead to overfitting, even though it reduces the coefficient magnitude.</li> <li>It introduces bias in its predictions.</li> <li>The regularization parameter (\u03b1) selection is tricky.</li> </ol>"},{"location":"ML/Regression/ridge_regression/#when-can-we-use-ridge-regression","title":"When can we use Ridge Regression?","text":"<p>Ridge regression can be used when we need to analyze multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are \u2018unstable\u2019, so slight changes to the input data can lead to large changes in the model estimate. Ridge regression stabilizes these estimates and provides a way to create more reliable, interpretable models.</p>"},{"location":"ML/Regression/stepwise_regression/","title":"Stepwise Regression Classifier","text":"<p>Stepwise regression is a type of regression technique that builds a model by adding or removing the predictor variables, typically via a series of F-tests or T-tests. The selection of predictor variables is controlled by the p-value in the ANOVA table. An alpha-to-enter and alpha-to-remove are set to control when predictor variables enter (be included) and drop out from the model. Alpha-to-remove is usually set larger than alpha-to-enter.</p> <p>The method starts with an empty equation. It tests the addition of each variable using a chosen model fit criterion, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit, and repeating this process until none improves the model to a statistically significant extent.</p> <p></p> <p>This method consists of two types:</p> <ol> <li>Forward Stepwise Regression: It starts with one feature and adds a feature at each step which improves the model the most until an adding further feature does not improve the model.</li> <li>Backward Stepwise Regression: It starts with all the features and removes one feature at a time which improves the model the most until removing further does not improve the model.</li> </ol>"},{"location":"ML/Regression/stepwise_regression/#python-implementation","title":"Python Implementation","text":"<p>The following is a basic example where we apply stepwise regression to the Boston housing dataset using the Python <code>statsmodels</code> package.</p> <pre><code>import statsmodels.api as sm\nfrom sklearn import datasets\n\ndata = datasets.load_boston()\nX = data[\"data\"]\nY = data[\"target\"]\nX = sm.add_constant(X)\n\nmodel = sm.OLS(Y, X).fit()\n\nprint(model.summary())\n</code></pre>"},{"location":"ML/Regression/stepwise_regression/#advantages-of-stepwise-regression","title":"Advantages of Stepwise Regression","text":"<ol> <li>It is a simple and effective method to deal with multiple variables.</li> <li>This method can solve the problems of multiple indexes, making the model more interpretable.</li> <li>This method can deal with the multicollinearity problem.</li> </ol>"},{"location":"ML/Regression/stepwise_regression/#disadvantages-of-stepwise-regression","title":"Disadvantages of Stepwise Regression","text":"<ol> <li>It is not guaranteed to give the best model.</li> <li>It only considers one variable at a time, there may exist a good predictive variable that if by itself is not significantly predictive, but when combined with others could be.</li> <li>It relies heavily on the order in which the variables are entered into the equation.</li> </ol>"},{"location":"ML/Regression/stepwise_regression/#scenario-for-use","title":"Scenario for Use","text":"<p>Stepwise regression is used when dealing with multiple independent variables. It is especially useful when dealing with high-dimensional datasets where you want to select a subset of variables based on their relationship with the outcome variable. These fields can include econometrics, manufacturing, medical fields, environmental science, etc.</p>"},{"location":"ML/Regression/stepwise_regression/#conclusion","title":"Conclusion","text":"<p>While Stepwise regression is an attractive method because it is a quick and easy way to determine which predictors to include in your model. It helps the researcher build a subset of predictors to use within future models, as it identifies predictors that do not contribute to the predictive power of the current model. However, its simplicity and ease can lead to some statistical problems, so it's important to use it with caution and consider other options as well.</p>"},{"location":"ML/Regression/sv_regression/","title":"Support Vector Regression","text":"<p>Support Vector Regression (SVR) is a machine learning model that makes use of SVM (Support Vector Machine) concept to make continuous valued predictions by estimating functions that are dependent on a given set of data. It is a more advanced version of simple linear regression. In simpler terms, SVR delivers a linear regression that is influenced by the chosen margin, especially when dealing with outliers.</p> <p></p>"},{"location":"ML/Regression/sv_regression/#how-does-support-vector-regression-work","title":"How Does Support Vector Regression Work?","text":"<p>Just like SVM, SVR utilizes the concept of a decision plane that defines decision boundaries. Here, a decision boundary is used to estimate the function that best fits the distribution. Anything within the decision boundary is the non-admissible margin.</p> <p>SVR has the capacity to restrict the error of each point in the training set within a certain threshold. Unlike other regression models that reduce the error rate, SVR offers a fitting accuracy specified within an established error boundary.</p>"},{"location":"ML/Regression/sv_regression/#creating-support-vector-regression-models-in-python","title":"Creating Support Vector Regression Models in Python:","text":"<pre><code># Importing Libraries\nfrom sklearn.svm import SVR\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Preprocessing\ndata = pd.read_csv('data.csv')\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n# Feature Scaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ny = scaler.fit_transform(y.reshape(len(y), 1))\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Training the model\nsvr = SVR(kernel='rbf')\nsvr.fit(X_train, y_train)\n\n# Predicting values\npredictions = svr.predict(X_test)\n</code></pre>"},{"location":"ML/Regression/sv_regression/#advantages-of-support-vector-regression","title":"Advantages of Support Vector Regression:","text":"<ol> <li> <p>Mentioned regression method can be used to resolve both linear as well as non-linear issues. It uses Kernel trick which does a good job at non-linear regression tasks.</p> </li> <li> <p>SVR is equipped with a defined regularization parameter, which makes the user think about avoiding over-fitting. It proposes stiffness on selecting hyperplane by regulating the parameters of the python machine learning model.</p> </li> <li> <p>SVR gives a unique prediction irrespective of the size of the input space and it doesn\u2019t get affected by outliers.</p> </li> </ol>"},{"location":"ML/Regression/sv_regression/#disadvantages-of-support-vector-regression","title":"Disadvantages of Support Vector Regression:","text":"<ol> <li> <p>SVR is not suitable for large datasets because the time complexity of one iteration in SVR is between O(n\u00b2) to O(n\u00b3). So, it can\u2019t be used for larger datasets.</p> </li> <li> <p>They have high training time and it\u2019s quite complex and difficult to understand and interpret.</p> </li> <li> <p>Choosing an appropriate kernel function is not always easy and can be tricky. The results are also hard to interpret.</p> </li> <li> <p>Tuning SVR hyperparameters is not easy. Incorrect choice of kernel and hyperparameters may lead to overfitting or underfitting.</p> </li> </ol>"},{"location":"ML/Regression/sv_regression/#scenarios-to-use-svr","title":"Scenarios to Use SVR:","text":"<ol> <li> <p>In cases where predictions that are not observed in the training set are required to be made.</p> </li> <li> <p>When working with datasets that have more features. In this case, SVR can be a feasible choice as it can efficiently handle multi-dimensional space.</p> </li> <li> <p>SVR has been used in applications in many domains, including text and hypertext categorization, image analysis, bioinformatics and medical applications.</p> </li> <li> <p>Especially useful for predicting values that fall within a specified range.</p> </li> <li> <p>In cases where the removal of noise from the dataset priority, as SVR has an excellent capability of noise handling.</p> </li> </ol> <p>In conclusion, it is a powerful regression technique and is much more customizable than other methods. When we are working with data that\u2019s fit is not linear then SVR can be a good technique to go within a margin of tolerance.</p>"},{"location":"ML/Regression/xgboost_regression/","title":"XGBoost Regression","text":"<p>XGBoost stands for \"Extreme Gradient Boosting\" and it is an implementation of gradient boosting machines. The XGBoost is a popular machine learning algorithm used for regression and classification problems. This algorithm improve speed and model performance.</p> <p>The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it reduces the error by following the gradient descent (like other boosting algorithms).</p> <p>In XGBoost Regression, the output is the prediction which is continuous, like predicting a house price or the temperature, etc.</p> <p></p>"},{"location":"ML/Regression/xgboost_regression/#the-process-of-xgboost-regression","title":"The process of XGBoost Regression","text":"<p>XGBoost is an ensemble learning method. In ensembling, weak learners (models) are combined to form a stronger prediction rule. XGBoost used trees as weak learners.</p> <p>For each tree, the algorithm identifies the best split point and variables, and leaves are assigned a real score. New functions are added to the existing functions until no further improvements can be made. Regularization parameters are also used in XGBoost to decrease overfitting and improve overall model.</p>"},{"location":"ML/Regression/xgboost_regression/#advantages-of-xgboost-regression","title":"Advantages of XGBoost Regression","text":"<ol> <li>Performance: XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems.</li> <li>Speed: XGBoost is really fast when compared to other implementations of gradient boosting.</li> <li>Scalability: XGBoost scales on multi core CPU, it supports distributed computing which makes it feasible for large datasets as well.</li> </ol>"},{"location":"ML/Regression/xgboost_regression/#disadvantages-of-xgboost-regression","title":"Disadvantages of XGBoost Regression","text":"<ol> <li>Complexity: XGBoost involves many parameters to tune and this makes it difficult for beginners to use it efficiently.</li> <li>Time consuming for larger datasets: It takes a lot longer time to train the model compared to decision trees.</li> <li>Noisy data: If the data is noisy(which doesn\u2019t show a certain pattern), XGBoost will overfit for sure.</li> <li>Feature interpretation: Since it uses gradient boosting algorithm which is an ensemble of decision trees, results of the model are not easily interpretable.</li> </ol>"},{"location":"ML/Regression/xgboost_regression/#application-scenarios-for-xgboost-regression","title":"Application scenarios for XGBoost Regression","text":"<ol> <li>Predicting House prices: Here the outcome to be predicted is the price of the house which is a continuous value. Therefore, Regression algorithm is used.</li> <li>Stock price prediction: The stock price for the next day is a numeric quantity and is a target variable which can be predicted using the regression algorithms.</li> </ol> <p>Here's how you can perform XGBoost Regression in Python:</p> <pre><code>import xgboost as xgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Load Dataset\nboston = load_boston()\nX = boston.data\ny = boston.target\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Initialize the XGBoost Regressor\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 10)\n\n# Fit the regressor with training data\nxg_reg.fit(X_train, y_train)\n\n# Predict\npreds = xg_reg.predict(X_test)\n</code></pre> <p>And you can find the Root Mean Square Error this way:</p> <pre><code>rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\n</code></pre>"},{"location":"ML/Time_Series/","title":"Time Series","text":"<p>Any data recorded at fixed interval of time over a certain period of time, is called time series. It is a stochastic process, i.e. the values are dependent on, and change with time. There are special techniques that are used for tackling such models.</p> <p>This chapter contains the following topics:</p> <ol> <li> <p>Features</p> </li> <li> <p>Key Terms</p> </li> <li>Types of Time Series</li> <li>Features</li> <li>Autocorrelation</li> </ol>"},{"location":"ML/Time_Series/Features/","title":"Introduction","text":"<p>How is time series different than any other data? It is because it contains a variable Time. It is an continuous ordered variable though. Therefore, we would need to anchor ourselves at some point, and we could call this anchor 0 (denoting the present), then we can represent the future as T+1, T+2... and the past as T-1, T-2 ... In order to build a model and have accurate predictions, we would usually want to have 50 or more degrees of freedom. </p> <p>Note: Degrees of freedom are usually pieces of independent information. Calculating a mean or any other non-parameter and including it in the model would cause us to lose a degree of freedom.</p>"},{"location":"ML/Time_Series/Features/#key-terms","title":"Key Terms","text":"<p>There are a few key terms that are commonly encountered when working with Time Series, these are inclusive of but not exclusively described in the following list:</p> <p>Trend: A time series is supposed to have an upward or downward trend if the plot shows a constant slope when plotted. It is noteworthy that for a pattern to be a trend, it needs to be consistent through the series. If we start getting upwards and downwards trend in the same plot, then it could be an indication of cyclical behavior.  </p> <p>Sesonality: If a TS has a regular repeating pattern based on the calendar or clock, then we call it to have seasonality. Soemtimes there can be different amount of seasonal variation across years, and in such cases it is ideal to take the log of values in order to get a better idea about seasonality. It is noteworthy that Annual Data would rarely have a seasonal component. </p> <p>Cyclical TS: Cyclical patterns are irregular based on calendar or clock. In these cases there are trends in the Time Series that are of irregular lengths, like business data, there can be revenue gain or loss due to a bad product but how long the upward or downward trend would last, is unknown beforehand. Often, when a plot shows cyclical behavior, it would be a combination of big and small cycles. Also, these are very difficult to model because there is no specific pattern and we do not know what would happen next. </p> <p>Autocorrelation/Serial Correlation: The dependence on the past in case of a univariate time series is called autocorrelation.</p>"},{"location":"ML/Time_Series/Features/#types-of-time-series","title":"Types of Time Series","text":"<p>The two major types of time series are as follows:</p> <p>Most of the theory that has been developed was developed for stationary time series.</p> <ol> <li>Stationary: Such time series have constant mean and reasonably constant variance through time. eg. annual rainfall</li> <li> <p>Non-Stationary: If we have a non-stationary time series then there are additional patterns in our series, like Trends, Seasonality or Cycles. These do not necessarily have a constant mean variance through time. </p> <p>a. Univariate Time Series: We usually use regression technique to model non-stationary time series and we also have to consider the time series aspects (like autocorrelation i.e. dependence on the past and differencing, i.e. the technique used in TS to try and get rid of autocorrelation). If we have a Time Series with trend, seasonality and non-constant variance, then it is best to log the data which would then allow us to stabilize the variance, hence the name 'Variance Stablizing Transformation' This allows us to fit our data in additive models, making it a very useful technique. </p> <p>b. Multivariate Time Series: Many variables measured through time, typically measured on the same time points. In some other cases, they may be measured on different time points but within an year. Such data would need to be analyzed using multiple regression models. We might also want to fit the time variable in the model (after transformations maybe) but this could allow us to determince whether there was any trend in the response and capture it. If you do not find any trend in the data, then the time variable can be chucked out but modelling the data without checking for the time variable would be a horrible idea.</p> </li> </ol> <p>More observations is often a good thing because it allows greater degrees of freedom. (Clarification needed, will read a few paper to better this concept)</p> <ol> <li> <p>Panel Data/Logitudinal Data: Panel data is a combination of cross-sectional data and time series. There can be two kinds of time logging for panel data, implicit and explicit. In econometrics and finance studies, the time is implicitly marked which in areas like medical studies explicit marking of time is done on Panel data. The idea is to take a cross-sectional measurements at some point in time and then repeat the measurements again and again to form a time series. </p> </li> <li> <p>Continuous Time Models: In this kind of data, we have a Point(Time component) process and a Stochastic (Random) process. For instance, the number of people arriving at a particular restaurant is an example of this where the groups of people ae arriving at certain points in time(point process) and the size of the group is random (stochastic process). </p> </li> </ol> <p>NOTE: No matter what a time series data is about, on the axis (while modeling) we always use a sequence of numbers starting from 1, up to the total number of observations. This allows us to focus on the sequence of measurements because we already know the frequency the time interval between any two observations.</p>"},{"location":"ML/Time_Series/Features/#features-of-time-series","title":"Features of Time Series","text":"<p>A few key features to remember when modelling time series are as follows: </p> <ol> <li>Past will affect the future but the future cannot affect the past</li> <li>There will rarely be any independence between observations, temperature, economics and all other major fields have dependence on the past in case of Time Series data  </li> <li>The pattern is often exploited to predict and build better models</li> <li>Stationary techniques can be used to model non-stationary time series, and also to remove trends, seasonality, and sometimes even cycles.</li> <li>We can also try to decompose time series into various components, model each one of them separately and then forecast each of the components and then combine again.  </li> <li>we also use Moving Averages (also known as filtering or smoothing) because it tries to remove the seasonal component of time series.</li> <li>We would often also need to transform the data before it can be used. The most common time we need to do transformations in Time Series is when we have an increasing seasonal variation because that represents a multiplicative model, which can be hard to explain. </li> </ol>"},{"location":"ML/Time_Series/Features/#autocorrelation","title":"Autocorrelation","text":"<p>The general concept of dependence on the past is called autocorrelation in statistics. If we are going to use regression models for mdoeling non-stationary data, we need to ensure that they satisfy the underlying assumptions necessary for regression models, i.e. normally distributed, zero mean, constant variance in the residual plot.</p>"},{"location":"ML/Time_Series/Features/#variance-covariance-and-correlation","title":"Variance, Covariance and Correlation","text":""},{"location":"ML/Time_Series/Features/#variance","title":"Variance","text":"<p>The population variance denoted by  is calculated using the sample variance. For any given sample Y, it's variance is calculated as follows:</p> <p></p> <p>Note: Variance is a special case of covariance, it just means how the sample of observations varies with itself.</p>"},{"location":"ML/Time_Series/Features/#covariance","title":"Covariance","text":"<p>It tells us how two varaibles vary with each other. Simply put, are the large values of X related to large values in Y or are the large values in X related to small values in Y? It is calculated as follows:</p> <p></p>"},{"location":"ML/Time_Series/Features/#correlation","title":"Correlation","text":"<p>Standardized covariance is called correlation, therefore it always lies between -1 and 1. It is represented as </p> <p> </p> <p>where  represented the standard deviation for x and  represents the standard deviation for y.</p> <p>Autocorrelation takes place for univariate time series.</p>"},{"location":"ML/Time_Series/Features/#sample-autovariance","title":"Sample Autovariance","text":"<p>Covariance is with respect to itself in this case, for the past values that have occurred in time and is represented as follows.</p> <p></p>"},{"location":"ML/Time_Series/Features/#sample-autocovariance","title":"Sample Autocovariance","text":"<p>In a univariate time series, the covariance between two observations k time periods apart (lag k), is given by</p> <p></p>"},{"location":"ML/Time_Series/Features/#sample-autocorrelation","title":"Sample Autocorrelation","text":"<p>Standardized autocovariance is equal to sample autocorrelation (k time periods apart) which is denoted by  and is given as follows:</p> <p></p>"},{"location":"ML/Time_Series/Features/#detecting-autocorrelation","title":"Detecting Autocorrelation","text":"<p>There are two types of autocorrelation that we look for: </p> <ol> <li>Positive Autocorrelation: Clustering in Residuals</li> <li>Negative Autocorrelation: Oscillations in Residuals (this is rare)</li> </ol> <p>The following methods are used for detecting autocorrelation:</p>"},{"location":"ML/Time_Series/Features/#use-the-tests","title":"Use the tests","text":""},{"location":"ML/Time_Series/Features/#durbin-watson-test-usually-used-in-econometrics","title":"Durbin Watson Test (usually used in Econometrics)","text":""},{"location":"ML/Time_Series/Features/#plot-of-current-vs-lagged-residuals","title":"Plot of Current vs Lagged residuals","text":""},{"location":"ML/Time_Series/Features/#runs-geary-test","title":"Runs (Geary) Test","text":""},{"location":"ML/Time_Series/Features/#chi-squared-test-of-independence-of-residuals","title":"Chi-Squared Test of Independence of Residuals","text":""},{"location":"ML/Time_Series/Features/#autocorrelation-function-acf","title":"Autocorrelation Function (ACF)","text":"<p>This function does the tests for all possible lags and plots them at the same time as well. It is very time efficient for this reason. </p>"},{"location":"ML/Time_Series/Features/#time-series-regression","title":"Time Series Regression","text":"<p>There are certain key points, that need to be considered when we do, Time Series Regression. They are as follows:</p> <ol> <li>Check if time ordered residuals are independent</li> <li>White Noise: Time Series of independent residuals (i.e. independent and identically distributed (iid), normal with 0 mean and constant variance ) It is given by </li> <li>When doing regression models of nonstationary time series, check for autocorrelation in the residual series, because a pattern in the residual series is equivalent to pattern in the data not captured by our model.</li> </ol>"},{"location":"ML/Time_Series/Features/#seasonality","title":"Seasonality","text":""},{"location":"ML/Time_Series/Features/#smoothing-or-filtering","title":"Smoothing or Filtering","text":"<p>Often the smoothing or filtering is done before the data is delivered and this can be bad thing because we don't know how seasonality was removed and also because we can only predict deseasonal values. </p>"},{"location":"ML/Time_Series/Features/#stl-seasonal-trend-lowess-analysis","title":"STL (Seasonal Trend Lowess Analysis","text":"<p>We can also do an STL analysis which would give us the seasonality, trend and remainder plots for any time series. </p> <p>NOTE: If you find that the range of the seasonality plot is more or less the same as the range of the remained plot, then there isn't much seasonality. The STL is designed to look for seasonality so it will always give you a seasonality plot and it'll be for the person reading to determine whether there is actual seasonlity or not.  </p>"},{"location":"ML/Time_Series/Features/#moving-averages","title":"Moving Averages","text":"<p>Both can be used to remove seasonality  </p>"},{"location":"ML/Time_Series/Features/#forecasting","title":"Forecasting","text":"<ol> <li>Report to te same level of significacne as the original data</li> <li>Useall dp when doing calculations for the predictions</li> <li>Only round at the end</li> <li>Use units where available</li> </ol>"},{"location":"classification/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":"<p>Oops! You've caught us in the middle of our creative process.</p> <p>Our team of Generative AI experts are working around the clock to bring you an extraordinary experience in the world of Generative AI.</p> <p>Stay Tuned!</p> <p>We're supercharging our platform with additional examples for JavaScript, broadening your horizons with an array of programming languages and tools. Stay tuned and dive into the mesmerizing universe of Generative AI with even more choices at your fingertips.</p>"},{"location":"deeplearning/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":"<p>Oops! You've caught us in the middle of our creative process.</p> <p>Our team of Generative AI experts are working around the clock to bring you an extraordinary experience in the world of Generative AI.</p> <p>Stay Tuned!</p> <p>We're supercharging our platform with additional examples for JavaScript, broadening your horizons with an array of programming languages and tools. Stay tuned and dive into the mesmerizing universe of Generative AI with even more choices at your fingertips.</p>"},{"location":"maths/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":"<p>Oops! You've caught us in the middle of our creative process.</p> <p>Our team of Generative AI experts are working around the clock to bring you an extraordinary experience in the world of Generative AI.</p> <p>Stay Tuned!</p> <p>We're supercharging our platform with additional examples for JavaScript, broadening your horizons with an array of programming languages and tools. Stay tuned and dive into the mesmerizing universe of Generative AI with even more choices at your fingertips.</p>"},{"location":"nlp/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":"<p>Oops! You've caught us in the middle of our creative process.</p> <p>Our team of Generative AI experts are working around the clock to bring you an extraordinary experience in the world of Generative AI.</p> <p>Stay Tuned!</p> <p>We're supercharging our platform with additional examples for JavaScript, broadening your horizons with an array of programming languages and tools. Stay tuned and dive into the mesmerizing universe of Generative AI with even more choices at your fingertips.</p>"},{"location":"python/classes/","title":"Python Cheat Sheet - Classes","text":"Keyword Description Example Classes A class encapsulates data and functionality - data as attributes, and functionality as methods. It is a blueprint to create concrete instances in the memory. <code>class Dog:</code><code>\"\"\" Blueprint of a dog \"\"\"</code><code># class variable shared by all instances</code><code>species = [\"canis lupus\"]</code><code>def __init__(self, name, color):</code><code>self.name = name</code><code>self.state = \"sleeping\"</code><code>self.color = color</code><code>def command(self, x):</code><code>if x == self.name:</code><code>self.bark(2)</code><code>elif x == \"sit\":</code><code>self.state = \"sit\"</code><code>else:</code><code>self.state = \"wag tail\"</code><code>def bark(self, freq):</code><code>for i in range(freq):</code><code>print(\"[\" + self.name + \"]: Woof!\")</code><code>bello = Dog(\"bello\", \"black\")</code><code>alice = Dog(\"alice\", \"white\")</code><code>print(bello.color) # black</code><code>print(alice.color) # white</code><code>bello.bark(1) # [bello]: Woof!</code><code>alice.command(\"sit\")</code><code>print(\"[alice]: \" + alice.state)</code><code># [alice]: sit</code><code>bello.command(\"no\")</code><code>print(\"[bello]: \" + bello.state)</code><code># [bello]: wag tail</code><code>alice.command(\"alice\")</code><code># [alice]: Woof!</code><code>bello.species += [\"wolf\"]</code><code>print(len(bello.species) == len(alice.species)) # True (!)</code> Instance You are an instance of the class human. An instance is a concrete implementation of a class: all attributes of an instance have a fixed value. Your hair is blond, brown, or black - but never unspecified.Each instance has its own attributes independent of other instances. Yet, class variables are different. These are data values associated with the class, not the instances. Hence, all instance share the same class variable species in the example. <code>bello = Dog(\"bello\", \"black\")</code><code>alice = Dog(\"alice\", \"white\")</code><code>print(bello.color) # black</code><code>print(alice.color) # white</code> Self The first argument when defining any method is always the self argument. This argument specifies the instance on which you call the method.self gives the Python interpreter the information about the concrete instance. To define a method, you use self to modify the instance attributes. But to call an instance method, you do not need to specify self.CreationYou can create classes \"on the fly\" and use them as logical units to store complex data types. <code>class Employee():</code><code>pass</code><code>employee = Employee()</code><code>employee.salary = 122000</code><code>employee.firstname = \"alice\"</code><code>employee.lastname = \"wonderland\"</code><code>print(employee.firstname + \" \"</code><code>+ employee.lastname + \" \"</code><code>+ str(employee.salary) + \"$\")</code><code># alice wonderland 122000$</code> <p>Reference : classes</p>"},{"location":"python/datatypes/","title":"Python Cheat Sheet - Data Types","text":"Type Description Example Boolean The Boolean data type is a truth value, either True or False.  The Boolean operators ordered by priority:  not x \u2192 \"if x is False, then x, else y\"  x and y \u2192 \"if x is False, then x, else y\"  x or y \u2192 \"if x is False, then y, else x\"  These comparison operators evaluate to True:  1 &lt; 2 and 0 &lt;= 1 and 3 &gt; 2 and 2 &gt;=2 and 1 == 1 and 1 != 0 <code>## 1. Boolean Operations</code> <code>x, y = True, False</code> <code>print(x and not y) # True</code> <code>print(not x and y or x) # True</code> <code>## 2. If condition evaluates to False</code> <code>if None or 0 or 0.0 or \"\" or [] or {} or set():</code> <code># None, 0, 0.0, empty strings, or empty</code> <code># container types are evaluated to False</code> <code>print(\"Dead code\") # Not reached</code> Integer, Float An integer is a positive or negative number without floating point (e.g. 3). A float is a positive or negative number with floating point precision (e.g. 3.14159265359).  The '//' operator performs integer division. The result is an integer value that is rounded towards the smaller integer number (e.g. 3 // 2 == 1). <code>## 3. Arithmetic Operations</code> <code>x, y = 3, 2</code> <code>print(x + y) # = 5</code> <code>print(x - y) # = 1</code> <code>print(x * y) # = 6</code> <code>print(x / y) # = 1.5</code> <code>print(x // y) # = 1</code> <code>print(x % y) # = 1s</code> <code>print(-x) # = -3</code> <code>print(abs(-x)) # = 3</code> <code>print(int(3.9)) # = 3</code> <code>print(float(3)) # = 3.0</code> <code>print(x ** y) # = 9</code> String Python Strings are sequences of characters.  The four main ways to create strings are the following.  1. Single quotes  'Yes'  2. Double quotes  \"Yes\"  3. Triple quotes (multi-line)  \"\"\"Yes\"\"\"  4. String method  str(5) == '5'  5. Concatenation  \"Ma\" + \"hatma\" # 'Mahatma'  These are whitespace characters in strings.  \u2022 Newline <code>\\n</code>  \u2022 Space <code>\\s</code>  \u2022 Tab <code>\\t</code> <code>## 4. Indexing and Slicing</code> <code>s = \"The youngest pope was 11 years old\"</code> <code>print(s[0])  # 'T'</code> <code>print(s[1:3])  # 'he'</code> <code>print(s[-3:-1])  # 'ol'</code> <code>print(s[-3:])  # 'old'</code> <code>x = s.split()</code> <code>print(x[-3] + \" \" + x[-1] + \" \" + x[2] + \"s\")</code> <code># 'old popes'</code> <code>## 5. Most Important String Methods</code> <code>y = \"     This is lazy\\t\\n     \"</code> <code>print(y.strip()) # Remove whitespace: 'This is lazy'</code> <code>print(\"DdrEe\".lower()) # Lowercase: 'ddree'</code> <code>print(\"Attention\".upper()) # Uppercase: 'ATTENTION'</code> <code>print(\"smartphone\".startswith(\"smart\")) # True</code> <code>print(\"smartphone\".endswith(\"phone\")) # True</code> <code>print(\"another\".find(\"other\")) # Match index: 2</code> <code>print(\"cheat\".replace(\"ch\", \"m\")) # 'meat'</code> <code>print(','.join([\"F\", \"B\", \"I\"])) # 'F,B,I'</code> <code>print(len(\"Rumpelstiltskin\")) # String length: 15</code> <code>print(\"ear\" in \"earth\") # Contains: True</code> List A container data type that stores a sequence of elements. Unlike strings, lists are mutable: modification possible. <code>l = [1, 2, 2]</code> <code>print(len(l)) # 3</code> Adding elements Add elements to a list with (i) append, (ii) insert, or (iii) list concatenation. The append operation is very fast. <code>[1, 2, 2].append(4) # [1, 2, 2, 4]</code> <code>[1, 2, 4].insert(2,2) # [1, 2, 2, 4]</code> <code>[1, 2, 2] + [4] # [1, 2, 2, 4]</code> Removal Removing an element can be slower. <code>[1, 2, 2, 4].remove(1) # [2, 2, 4]</code> Reversing This reverses the order of list elements. <code>[1, 2, 3].reverse() # [3, 2, 1]</code> Sorting Sorts a list. The computational complexity of sorting is O(n log n) for n list elements. <code>[2, 4, 2].sort() # [2, 2, 4]</code> Indexing Finds the first occurrence of an element in the list &amp; returns its index. Can be slow as the whole list is traversed. <code>[2, 2, 4].index(2) # index of element 4 is \"2\"</code> <code>[2, 4, 2].index(2,1) # index of element 2 after pos 1 is \"2\"</code> Stack Python lists can be used intuitively as stack via the two list operations append() and pop(). <code>stack = [3]</code> <code>stack.append(42) # [3, 42]</code> <code>stack.pop() # 42 (stack: [3])</code> <code>stack.pop() # 3 (stack: [])</code> Set A set is an unordered collection of elements. Each can exist only once. <code>basket = {'apple', 'eggs', 'banana', 'orange'}</code> <code>same set = {'apple', 'eggs', 'banana', 'orange'}</code> Dictionary The dictionary is a useful data structure for storing (key, value) pairs. <code>calories = {'apple': 52, 'banana': 89, 'choco': 546}</code> Reading and writing elements Read and write elements by specifying the key within the brackets. Use the keys() and values() functions to access all keys and values of the dictionary. <code>print(calories['apple'] &lt; calories['choco']) # True</code> <code>calories['capu'] = 74</code> <code>print(calories['banana'] &lt; calories['capu']) # False</code> <code>print('apple' in calories.keys()) # True</code> <code>print(52 in calories.values()) # True</code> Dictionary Looping You can loop over the (key, value) pairs of a dictionary with the items() method. <code>for k, v in calories.items():</code> <code>print(k) if v &gt; 500 else None # 'chocolate'</code> Membership operator Check with the 'in' keyword whether the set, list, or dictionary contains an element. Set containment is faster than list containment. <code>basket = {'apple', 'eggs', 'banana', 'orange'}</code> <code>print('eggs' in basket) # True</code> <code>print('mushroom' in basket) # False</code> List and Set Comprehension List comprehension is the concise Python way to create lists. Use brackets plus an expression, followed by a for clause. Close with zero or more for or if clauses. Set comprehension is similar to list comprehension. <code># List comprehension</code> <code>l = [(i + \" \" + x) for x in ['Alice', 'Bob', 'Pete']]</code> <code>print(l) # ['Hi Alice', 'Hi Bob', 'Hi Pete']</code> <code>l2 = [(x + y) for x in range(3) for y in range(3) if x+y != 0]</code> <code>print(l2) # [0, 2, 2]</code> <code># Set comprehension</code> <code>squares = {x**2 for x in [0,2,4] if x &lt; 4 } # {0, 4}</code> <p>Reference Image: basic datatype complex datatype</p>"},{"location":"python/functions/","title":"Python Cheat Sheet - Function","text":"Function Example Description List <code>l = [1, 2, 3]</code><code>print(len(l)) # 3</code> A container data type that stores a sequence of elements. Unlike strings, lists are mutable: modification possible. Adding elements <code>[1, 2, 3].append(4) # [1, 2, 3, 4]</code><code>[1, 2, 3].insert(2,2) # [1, 2, 2, 3]</code><code>[1, 2, 3] + [4] # [1, 2, 3, 4]</code> Add elements to a list with (i) append, (ii) insert, or (iii) list concatenation. The append operation is very fast. Removal <code>[1, 2, 3, 4].remove(1) # [2, 3, 4]</code> Removing an element can be slower. Reversing <code>[1, 2, 3].reverse() # [3, 2, 1]</code> This reverses the order of list elements. Sorting <code>[2, 4, 3].sort() # [2, 3, 4]</code> Sorts a list. The computational complexity of sorting is O(n log n) for n list elements. Indexing <code>[2, 4, 3].index(2) # index of element 4 is \"0\"</code><code>[2, 4, 3].index(2,1) # index of element 2 after pos 1 is \"1\"</code> Finds the first occurrence of an element in the list &amp; returns its index. Can be slow as the whole list is traversed. Stack <code>stack = [3]</code><code>stack.append(42) # [3, 42]</code><code>stack.pop() # 42 (stack: [3])</code><code>stack.pop() # 3 (stack: [])</code> Python lists can be used intuitively as stack via the two list operations append() and pop(). Set <code>basket = {'apple', 'eggs', 'banana', 'orange'}</code> A set is an unordered collection of elements. Each can exist only once. Dictionary <code>calories = {'apple': 52, 'banana': 89, 'choco': 546}</code> The dictionary is a useful data structure for storing (key, value) pairs. Reading and writing elements <code>print(calories['apple'] &lt; calories['choco']) # True</code><code>calories['capu'] = 74</code><code>print(calories['banana'] &lt; calories['capu']) # False</code><code>print('apple' in calories.keys()) # True</code><code>print(52 in calories.values()) # True</code> Read and write elements by specifying the key within the brackets. Use the keys() and values() functions to access all keys and values of the dictionary. Dictionary Looping <code>for k, v in calories.items():</code><code>print(k) if v &gt; 500 else None # 'chocolate'</code> You can loop over the (key, value) pairs of a dictionary with the items() method. Membership operator <code>basket = {'apple', 'eggs', 'banana', 'orange'}</code><code>print('eggs' in basket) # True</code><code>print('mushroom' in basket) # False</code> Check with the 'in' keyword whether the set, list, or dictionary contains an element. Set containment is faster than list containment. List and Set Comprehension <code># List comprehension</code><code>l = [i * i for i in [1, 2, 3]]</code><code># Set comprehension</code><code>squares = {x**2 for x in [0,2,4] if x &lt; 4}</code> List comprehension is the concise Python way to create lists. Use brackets plus an expression, followed by a for clause. Close with zero or more for or if clauses. Set comprehension is similar to list comprehension. <p>Reference Image: function</p>"},{"location":"python/interview/","title":"Python Cheat Sheet - Interview Question","text":"Question Code Question Code Check if list contains integer x <code>l = [3, 4, 5, 2, 11, 5]</code><code>print(111 in l) # True</code> Get missing number in range [1...100] <code>def get_missing_number(lst):</code><code>return set(range(1, len(lst)+1)) - set(lst)</code><code>l = list(range(1,100))</code><code>l.remove(50)</code><code>print(get_missing_number(l)) # 50</code> Find duplicate number in integer list <code>def find_duplicates(elements):</code><code>duplicates, seen = set(), set()</code><code>for element in elements:</code><code>if element in seen:</code><code>duplicates.add(element)</code><code>seen.add(element)</code><code>return duplicates</code> Compute the intersection of two lists <code>def intersect(lst1, lst2):</code><code>res, lst2_copy = [], lst2[:]</code><code>for el in lst1:</code><code>if el in lst2_copy:</code><code>res.append(el)</code><code>lst2_copy.remove(el)</code><code>return res</code> Check if two strings are anagrams <code>def is_anagram(s1, s2):</code><code>return set(s1) == set(s2)</code><code>print(is_anagram(\"elvis\", \"lives\")) # True</code> Find max and min in unsorted list <code>l = [4, 3, 6, 3, 4, 888, 1, -11, 22, 3]</code><code>print(max(l)) # 888</code><code>print(min(l)) # -11</code> Remove all duplicates from list <code>lst = list(range(10)) + list(range(10))</code><code>lst = list(set(lst))</code><code>print(lst)</code><code># [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code> Reverse string using recursion <code>def reverse(string):</code><code>if len(string) &lt; 1: return string</code><code>return reverse(string[1:]) + string[0]</code><code>print(reverse(\"hello\")) # olleh</code> Find pairs of integers in list so that their sum is equal to integer x <code>def find_pairs(l, x):</code><code>pairs = []</code><code>for (i, el1) in enumerate(l):</code><code>for (j, el2) in enumerate(l[i+1:]):</code><code>if el1 + el2 == x:</code><code>pairs.append((el1, el2))</code><code>return pairs</code> Compute the first n Fibonacci numbers <code>a, b = 0, 1</code><code>n = 10</code><code>for i in range(n):</code><code>print(b)</code><code>a, b = b, a+b</code><code># 1, 1, 2, 3, 5, 8, ...</code> Check if a string is a palindrome <code>def is_palindrome(phrase):</code><code>return phrase == phrase[::-1]</code><code>print(is_palindrome(\"anna\")) # True</code> Sort list with Quicksort algorithm <code>def qsort(L):</code><code>if L == []: return L</code><code>return qsort([x for x in L[1:] if x &lt; L[0]]) + [L[0]] + qsort([x for x in L[1:] if x &gt;=L[0]])</code><code>lst = [44, 33, 22, 55, 77, 999]</code><code>print(qsort(lst))</code><code># [22, 33, 44, 55, 77, 999]</code> Use list as stack, array, and queue <code># as a list ...</code><code>l = [3, 4]</code><code>l += [5, 6] # [3, 4, 5, 6]</code><code># ... as a stack ...</code><code>l.append(10) # [3, 4, 5, 6, 10]</code><code>l.pop() # [3, 4, 5, 6]</code><code># ... and as a queue</code><code>l.insert(0, 5) # [5, 3, 4, 5, 6]</code><code>l.pop(0) # [3, 4, 5, 6]</code> Find all permutation of string <code>def get_permutations(w):</code><code>if len(w)&lt;=1:</code><code>return set(w)</code><code>smaller = get_permutations(w[1:])</code><code>perms = set()</code><code>for x in smaller:</code><code>for pos in range(0,len(x)+1):</code><code>perm = x[:pos] + w[0] + x[pos:]</code><code>perms.add(perm)</code><code>return perms</code><code>print(get_permutations(\"nan\"))</code><code># {'nna', 'ann', 'nan'}</code> <p>Reference Image: Interview</p>"},{"location":"python/keywords/","title":"Python Cheat Sheet - Keywords","text":"Keyword Description Code example <code>False, True</code> Data values from the data type Boolean <code>False == (1 &gt; 2), True == (2 &gt; 1)</code> <code>and, or, not</code> Logical operators:(x and y) -&gt; both x and y must be True  (x and y) -&gt; both x and y must be True (x or y) -&gt; either x or y must be True (not x) -&gt; x must be false  <code>x, y = True, False</code><code>(x or y) == True  # True</code> <code>(x and y) == False  # True</code> <code>(not y) == True  # True</code> <code>break</code> Ends loop prematurely <code>while(True):</code> <code>break # no infinite loop</code><code>print(\"hello world\")</code> <code>continue</code> Finishes current loop iteration <code>while(True):</code> <code>continue</code> <code>print(\"43\") # dead code</code> <code>class</code> Defines a new class + a real-world concept (object oriented programming) <code>class Beer:</code> <code>def __init__(self):</code> <code>self.content = 1.0</code> <code>def drink(self):</code> <code>self.content = 0.0</code> <code>becks = Beer() # constructor - create class</code> <code>becks.drink() # beer empty: b.content == 0</code>&lt;/br <code>def</code> Defines a new function or class method. For latter, first parameter (\"self\") points to the class object  When calling class method, first parameter is implicit. <code>def incrementor(x):</code> <code>return x + 1</code> <code>incrementor(4) # returns 5</code> <code>if, elif, else</code> Conditional program execution: program starts with  \"if\" branch, tries the \"elif\" branches, and finishes with \"else\" branch (until one branch evaluates to True). <code>x = int(input(\"your value: \"))</code> <code>if x &gt; 3: print(\"Big\")</code> <code>elif x == 3: print(\"Medium\")</code> <code>else: print(\"Small\")</code> <code>for, while</code> # For loop declaration <code>for i in [0,1,2]:</code> <code>print(i)</code> # While loop - same semantics <code>j = 0</code> <code>while j &lt; 3:</code> <code>print(j)</code> <code>j = j + 1</code> <code>in</code> Checks whether element is in sequence <code>42 in [2, 39, 42] # True</code> <code>is</code> Checks whether both elements point to the same object <code>y = x = 3</code> <code>x is y # True</code> <code>[3] is [3] # False</code> <code>None</code> Empty value constant <code>def f():</code> <code>x = 2</code> <code>f() is None # True</code> <code>lambda</code> Function with no name (anonymous function) <code>(lambda x: x + 3)(3) # returns 6</code> <code>return</code> Terminates execution of the function and passes the flow of execution to the caller. An optional value after the return keyword specifies the function result. <code>def incrementor(x):</code> <code>return x + 1</code> <code>incrementor(4) # returns 5</code> <p>Reference Image: Keywords</p>"},{"location":"python/numpy/","title":"Python Cheat Sheet - Numpy","text":"Name Description Example <code>a.shape</code> The shape attribute of NumPy array a keeps a tuple of integers. Each integer describes the number of elements of the axis. <code>a = np.array([[1,2],[1,1],[0,0]])</code><code>print(np.shape(a)) # (3, 2)</code> <code>a.ndim</code> The ndim attribute is equal to the length of the shape tuple. <code>print(np.ndim(a)) # 2</code> <code>*</code> The asterisk (star) operator performs the Hadamard product, i.e., multiplies two matrices with equal shape element-wise. <code>a = np.array([[2, 0], [0, 2]])</code><code>b = np.array([[1, 1], [1, 1]])</code><code>print(a*b) # [[2 0] [0 2]]</code> <code>np.matmul(a,b)</code> The standard matrix multiplication operator. Equivalent to the @ operator. <code>print(np.matmul(a,b))</code><code># [[2 2] [1 1]]</code> <code>np.arange([start, ]stop, [step, ])</code> Creates a new 1D numpy array with evenly spaced values <code>print(np.arange(0,10,2))</code><code># [0 2 4 6 8]</code> <code>np.linspace(start, stop, num=50)</code> Creates a new 1D numpy array with evenly spread elements within the given interval <code>print(np.linspace(0,10,3))</code><code># [ 0. 5. 10.]</code> <code>np.average(a)</code> Averages over all the values in the numpy array <code>a = np.array([[2, 0], [0, 2]])</code><code>print(np.average(a)) # 1.0</code> <code>&lt;slice&gt; = &lt;val&gt;</code> Replace the <code>&lt;slice&gt;</code> as selected by the slicing operator with the value <code>&lt;val&gt;</code>. <code>a = np.array([1, 0, 0, 0])</code><code>a[:2] = 2</code><code>print(a) # [2 2 0 0]</code> <code>np.var(a)</code> Calculates the variance of a numpy array. <code>a = np.array([2, 6])</code><code>print(np.var(a)) # 4.0</code> <code>np.std(a)</code> Calculates the standard deviation of a numpy array <code>print(np.std(a)) # 2.0</code> <code>np.diff(a)</code> Calculates the difference between subsequent values in NumPy array a <code>fibs = np.array([0, 1, 1, 2, 3, 5])</code><code>print(np.diff(fibs, n=1))</code> <code>np.cumsum(a)</code> Calculates the cumulative sum of the elements in NumPy array a. <code>print(np.cumsum(np.arange(5)))</code><code># [ 0 1 3 6 10]</code> <code>np.sort(a)</code> Creates a new NumPy array with the values from a (ascending). <code>a = np.array([10,3,7,1,0])</code><code>print(np.sort(a))</code><code># [ 0 1 3 7 10]</code> <code>np.argsort(a)</code> Returns the indices of a NumPy array so that the indexed values would be sorted. <code>print(np.argsort(a))</code><code># [4 3 1 2 0]</code> <code>np.max(a)</code> Returns the maximal value of NumPy array a. <code>print(np.max(a)) # 10</code> <code>np.argmax(a)</code> Returns the index of the element with maximal value in the NumPy array a. <code>print(np.argmax(a)) # 0</code> <code>np.nonzero(a)</code> Returns the indices of the nonzero elements in NumPy array a. <code>print(np.nonzero(a))</code><code># [0 1 2 3]</code> <p>Reference Image: numpy</p>"},{"location":"python/objectorient/","title":"Python Cheat Sheet - Object Orientation","text":"Class Description Example Class A blueprint to create objects. It defines the data (attributes) and functionality (methods) of the objects. You can access both attributes and methods via the dot notation. <code>class Dog:</code><code># class attribute</code><code>is_hairy = True</code><code></code> <code># constructor</code><code>def __init__(self, name):</code><code># instance attribute</code><code>self.name = name</code><code></code> <code># method</code><code>def bark(self):</code><code>print(\"Wuff\")</code> Object (instance) A piece of encapsulated data with functionality in your Python program that is built according to a class definition. Often, an object corresponds to a thing in the real world. An example is the object \"cat\" that is created according to the class definition \"Person\". An object consists of an arbitrary number of attributes and methods, encapsulated within a single unit. <code>bello = Dog(\"bello\")</code><code>paris = Dog(\"paris\")</code><code></code> <code>print(bello.name)</code><code># \"bello\"</code><code>print(paris.name)</code><code># \"paris\"</code> Instantiation The process of creating an object of a class. This is done with the constructor method <code>__init__(self, ...)</code> Method A subset of the overall functionality of an object. The method is defined similarly to a function (using the keyword \"def\") in the class definition. An object can have an arbitrary number of methods. Self The first argument when defining any method is always the <code>self</code> argument. This argument specifies the instance on which you call the method. <code>self</code> gives the Python interpreter the information about the concrete instance. To define a method, you use <code>self</code> to modify the instance attributes. But to call an instance method, you do not need to specify <code>self</code>. Encapsulation Binding together data and functionality that manipulates the data. <code>class Cat:</code><code></code> <code># method overloading</code><code>def miau(self, times=1):</code><code>print(\"miau \" * times)</code><code></code> <code>fifi = Cat()</code><code></code> <code>fifi.miau()</code><code># \"miau \"</code><code>fifi.miau(5)</code><code># \"miau miau miau miau miau \"</code> Attribute A variable defined for a class (class attribute) or for an object (instance attribute). You use attributes to package data into enclosed units (class or instance). <code># Dynamic attribute</code><code>fifi.likes = \"mice\"</code><code>print(fifi.likes)</code><code># \"mice\"</code> Class attribute (=class variable, static variable, static attribute) A variable that is created statically in the class definition and that is shared by all class objects. Instance attribute (=instance variable) A variable that holds data that belongs only to a single instance. Other instances do not share this variable (in contrast to class attributes). In most cases, you create an instance attribute in the constructor when creating the instance itself using the <code>self</code> keywords (e.g. <code>self.x = &lt;val&gt;</code>). Dynamic attribute An instance attribute that is defined dynamically during the execution of the program and that is not defined within any method. For example, you can simply add a new attribute new to any object by calling <code>o.new = &lt;val&gt;</code>. Method overloading You may want to define a method in a way so that there are multiple options to call it. For example, for class X, you define a method <code>f(...)</code> that can be called in three ways: <code>f(a)</code>, <code>f(a,b)</code>, or <code>f(a,b,c)</code>. To this end, you can define the method with default parameters (e.g. <code>f(a, b=None, c=None)</code>). Inheritance Class A can inherit certain characteristics (like attributes or methods) from class B. For example, the class \"Dog\" may inherit the attribute \"number_of_legs\" from the class \"Animal\". In this case, you would define the inherited class \"Dog\" as follows: <code>\"class Dog(Animal): ...\"</code> <code>class Persian_Cat(Cat):</code><code>classification = \"Persian\"</code><code></code> <code>mimi = Persian_Cat()</code><code></code> <code>print(mimi.miau(3))</code><code># \"miau miau miau \"</code><code>print(mimi.classification)</code><code># \"Persian\"</code> <p>Reference Image: objectorientation</p>"},{"location":"regression/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":"<p>Oops! You've caught us in the middle of our creative process.</p> <p>Our team of Generative AI experts are working around the clock to bring you an extraordinary experience in the world of Generative AI.</p> <p>Stay Tuned!</p> <p>We're supercharging our platform with additional examples for JavaScript, broadening your horizons with an array of programming languages and tools. Stay tuned and dive into the mesmerizing universe of Generative AI with even more choices at your fingertips.</p>"},{"location":"statistics/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":"<p>Oops! You've caught us in the middle of our creative process.</p> <p>Our team of Generative AI experts are working around the clock to bring you an extraordinary experience in the world of Generative AI.</p> <p>Stay Tuned!</p> <p>We're supercharging our platform with additional examples for JavaScript, broadening your horizons with an array of programming languages and tools. Stay tuned and dive into the mesmerizing universe of Generative AI with even more choices at your fingertips.</p>"},{"location":"tags/","title":"Tags","text":"<p>This page shows a list of pages indexed by their tags:</p>"}]}